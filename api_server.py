#!/usr/bin/env python3
"""
Flask APIæœåŠ¡å™¨
ä¸ºå‰ç«¯Reactåº”ç”¨æä¾›APIæ¥å£ï¼Œå¯¹æ¥ç°æœ‰çš„toolsæ¨¡å—
"""

from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from werkzeug.utils import secure_filename
import os
from pathlib import Path
import glob
from dotenv import dotenv_values
import traceback
import json
import time
from datetime import datetime
import random
import string
import shutil

# å¯¼å…¥ç°æœ‰çš„å·¥å…·æ¨¡å—
from tools.utils import count_files, log_activity
from tools.email_processing.email_cleaner import EmailCleaner
# from tools.data_cleaning import clean_email_files  # åŒ…å«streamlitä¾èµ–ï¼Œä¸å¯¼å…¥
# from tools.llm_processing import process_with_llm  # åŒ…å«streamlitä¾èµ–ï¼Œä¸å¯¼å…¥
from tools.api_clients.knowledge_base_api import KnowledgeBaseAPI
from tools.api_clients.gptbots_api import GPTBotsAPI
from config import DIRECTORIES, init_directories

app = Flask(__name__)
# é…ç½®CORS - å…è®¸æ‰€æœ‰æ¥æºè®¿é—®æ‰€æœ‰è·¯ç”±
CORS(app, 
     resources={r"/*": {"origins": "*"}},
     allow_headers=["Content-Type", "Authorization"],
     methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
     supports_credentials=False)

# é…ç½®ä¸Šä¼ å¤§å°é™åˆ¶
max_content_mb = int(os.getenv('MAX_CONTENT_LENGTH', '2000'))
app.config['MAX_CONTENT_LENGTH'] = max_content_mb * 1024 * 1024  # è½¬æ¢ä¸ºå­—èŠ‚

# åˆå§‹åŒ–ç›®å½•
init_directories()

# é…ç½®ä¸Šä¼ 
UPLOAD_FOLDER = DIRECTORIES["upload_dir"]
ALLOWED_EXTENSIONS = {'eml'}

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def get_disk_usage():
    """è·å–å½“å‰ç£ç›˜ä½¿ç”¨æƒ…å†µ"""
    try:
        import psutil
        # è·å–å½“å‰å·¥ä½œç›®å½•æ‰€åœ¨çš„ç£ç›˜åˆ†åŒº
        current_path = os.getcwd()
        disk_usage = psutil.disk_usage(current_path)
        
        total_gb = disk_usage.total / (1024 ** 3)
        used_gb = disk_usage.used / (1024 ** 3)
        free_gb = disk_usage.free / (1024 ** 3)
        percent = disk_usage.percent
        
        return {
            'total_gb': round(total_gb, 2),
            'used_gb': round(used_gb, 2),
            'free_gb': round(free_gb, 2),
            'percent': percent,
            'status': 'warning' if percent > 85 else 'critical' if percent > 95 else 'normal'
        }
    except ImportError:
        # psutilæœªå®‰è£…ï¼Œä½¿ç”¨shutilä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
        try:
            import shutil
            current_path = os.getcwd()
            disk_stat = shutil.disk_usage(current_path)
            
            total_gb = disk_stat.total / (1024 ** 3)
            used_gb = disk_stat.used / (1024 ** 3)
            free_gb = disk_stat.free / (1024 ** 3)
            percent = (disk_stat.used / disk_stat.total) * 100
            
            return {
                'total_gb': round(total_gb, 2),
                'used_gb': round(used_gb, 2),
                'free_gb': round(free_gb, 2),
                'percent': round(percent, 2),
                'status': 'warning' if percent > 85 else 'critical' if percent > 95 else 'normal'
            }
        except Exception as e:
            return {
                'error': str(e),
                'status': 'unknown'
            }
    except Exception as e:
        return {
            'error': str(e),
            'status': 'unknown'
        }

def log_disk_usage(prefix=""):
    """è®°å½•ç£ç›˜ä½¿ç”¨æƒ…å†µåˆ°æ—¥å¿—"""
    disk_info = get_disk_usage()
    if 'error' not in disk_info:
        status_emoji = "âš ï¸" if disk_info['status'] == 'warning' else "ğŸ”´" if disk_info['status'] == 'critical' else "ğŸ’¾"
        log_activity(
            f"{prefix}{status_emoji} Disk usage: {disk_info['used_gb']:.2f}GB / {disk_info['total_gb']:.2f}GB "
            f"({disk_info['percent']:.1f}%) | Free: {disk_info['free_gb']:.2f}GB"
        )
        return disk_info
    else:
        log_activity(f"{prefix}âš ï¸ Unable to get disk usage: {disk_info['error']}")
        return None


@app.route('/api/config/env', methods=['GET'])
def get_env_config():
    """è·å–ç¯å¢ƒå˜é‡é…ç½®"""
    try:
        # å°è¯•å¤šä¸ªå¯èƒ½çš„.envæ–‡ä»¶ä½ç½®
        possible_paths = [
            Path(__file__).parent / '.env',  # ä¸api_server.pyåŒç›®å½•
            Path.cwd() / '.env',  # å½“å‰å·¥ä½œç›®å½•
            Path(__file__).parent.parent / '.env',  # çˆ¶ç›®å½•
        ]
        
        env_path = None
        for path in possible_paths:
            log_activity(f"Trying to read .env file: {path.absolute()}")
            if path.exists():
                env_path = path
                log_activity(f"âœ… Found .env file: {path.absolute()}")
                break
        
        if not env_path:
            error_msg = f'.env file not found. Attempted paths:\n' + '\n'.join([f'  - {p.absolute()}' for p in possible_paths])
            log_activity(f"âŒ {error_msg}")
            return jsonify({
                'success': False, 
                'error': '.envæ–‡ä»¶ä¸å­˜åœ¨',
                'details': error_msg,
                'cwd': str(Path.cwd().absolute()),
                'script_dir': str(Path(__file__).parent.absolute())
            }), 404
        
        env_vars = dotenv_values(env_path)
        log_activity(f"Successfully read .env file, {len(env_vars)} variables")
        
        # æå–LLM API Keys
        llm_keys = []
        for i in range(1, 4):
            key = env_vars.get(f'GPTBOTS_LLM_API_KEY_{i}')
            if key:
                llm_keys.append({
                    'id': f'llm_key_{i}',
                    'name': f'LLM API Key {i}',
                    'value': key,
                    'masked': key[:8] + '...' + key[-4:] if len(key) > 12 else key
                })
        
        # æå–çŸ¥è¯†åº“ API Keys
        kb_keys = []
        for i in range(1, 4):
            key = env_vars.get(f'GPTBOTS_KB_API_KEY_{i}')
            if key:
                kb_keys.append({
                    'id': f'kb_key_{i}',
                    'name': f'çŸ¥è¯†åº“ API Key {i}',
                    'value': key,
                    'masked': key[:8] + '...' + key[-4:] if len(key) > 12 else key
                })
        
        config = {
            'llm_api_keys': llm_keys,
            'kb_api_keys': kb_keys,
            'default_llm_key': env_vars.get('GPTBOTS_LLM_API_KEY_1', ''),
            'default_kb_key': env_vars.get('GPTBOTS_KB_API_KEY_1', ''),
        }
        
        log_activity(f"Returning config: {len(llm_keys)} LLM Keys, {len(kb_keys)} KB Keys")
        return jsonify({'success': True, 'config': config})
    except Exception as e:
        error_msg = f'Error reading environment config: {str(e)}'
        log_activity(error_msg)
        return jsonify({
            'success': False, 
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500


@app.route('/api/knowledge-bases', methods=['POST'])
def fetch_knowledge_bases():
    """è·å–çŸ¥è¯†åº“åˆ—è¡¨"""
    try:
        data = request.json
        api_key = data.get('api_key')
        
        if not api_key:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘API Key'}), 400
        
        kb_client = KnowledgeBaseAPI(api_key)
        knowledge_bases = kb_client.list_knowledge_bases()
        
        if knowledge_bases:
            return jsonify({
                'success': True,
                'knowledge_bases': knowledge_bases
            })
        else:
            # è¿”å›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
            return jsonify({
                'success': False, 
                'error': 'æ— æ³•è·å–çŸ¥è¯†åº“åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥API Keyæ˜¯å¦æ­£ç¡®ã€ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸ï¼Œæˆ–çŸ¥è¯†åº“æœåŠ¡æ˜¯å¦å¯è®¿é—®'
            }), 200  # æ”¹ä¸º200çŠ¶æ€ç ï¼Œè®©å‰ç«¯èƒ½æ­£ç¡®å¤„ç†
            
    except Exception as e:
        error_msg = str(e)
        # æ£€æŸ¥æ˜¯å¦æ˜¯è¿æ¥è¶…æ—¶é”™è¯¯
        if 'timeout' in error_msg.lower() or 'Connection' in error_msg:
            error_msg = 'æ— æ³•è¿æ¥åˆ°çŸ¥è¯†åº“æœåŠ¡ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æœåŠ¡åœ°å€é…ç½®'
        elif 'Max retries exceeded' in error_msg:
            error_msg = 'çŸ¥è¯†åº“æœåŠ¡è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ'
        
        log_activity(f"Error getting knowledge base list: {error_msg}")
        return jsonify({
            'success': False, 
            'error': f'è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {error_msg}'
        }), 200  # æ”¹ä¸º200çŠ¶æ€ç ï¼Œè®©å‰ç«¯èƒ½æ­£ç¡®å¤„ç†


@app.route('/api/stats', methods=['GET'])
def get_stats():
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    try:
        # çŸ¥è¯†åº“æ–‡æ¡£æ•° = LLMå¤„ç†å®Œæˆçš„æ–‡ä»¶æ•°ï¼ˆè¿™äº›æ–‡ä»¶åº”è¯¥å·²ä¸Šä¼ åˆ°çŸ¥è¯†åº“ï¼‰
        llm_processed_count = count_files(DIRECTORIES["final_output_dir"], "*.md")
        
        stats = {
            'uploaded': count_files(DIRECTORIES["upload_dir"], "*.eml"),
            'cleaned': count_files(DIRECTORIES["processed_dir"], "*.md"),
            'processed': llm_processed_count,
            'inKnowledgeBase': llm_processed_count  # å·²LLMå¤„ç†çš„æ–‡ä»¶å³ä¸ºä¸Šä¼ åˆ°çŸ¥è¯†åº“çš„æ–‡ä»¶
        }
        return jsonify({'success': True, 'stats': stats})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/delete-uploaded/<path:filename>', methods=['DELETE'])
def delete_uploaded_file(filename):
    """åˆ é™¤å·²ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆæ”¯æŒæ‰¹æ¬¡è·¯å¾„ï¼‰"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ–‡ä»¶å
        filename = unquote(filename)
        
        file_path = Path(UPLOAD_FOLDER) / filename
        if file_path.exists():
            file_path.unlink()
            log_activity(f"Deleted upload file: {filename}")
            return jsonify({'success': True, 'message': 'æ–‡ä»¶åˆ é™¤æˆåŠŸ'})
        else:
            log_activity(f"æ–‡ä»¶ä¸å­˜åœ¨: {filename}, è·¯å¾„: {file_path}")
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
    except Exception as e:
        log_activity(f"Failed to delete file: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/delete-processed/<path:filename>', methods=['DELETE'])
def delete_processed_file(filename):
    """åˆ é™¤å·²å»é‡çš„æ–‡ä»¶ï¼ˆæ”¯æŒæ‰¹æ¬¡è·¯å¾„ï¼‰"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ–‡ä»¶å
        filename = unquote(filename)
        
        processed_dir = Path(DIRECTORIES["processed_dir"])
        file_path = processed_dir / filename
        if file_path.exists():
            file_path.unlink()
            log_activity(f"Deleted deduplicated file: {filename}")
            return jsonify({'success': True, 'message': 'æ–‡ä»¶åˆ é™¤æˆåŠŸ'})
        else:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
    except Exception as e:
        log_activity(f"Failed to delete file: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/delete-llm-processed/<path:filename>', methods=['DELETE'])
def delete_llm_processed_file(filename):
    """åˆ é™¤LLMå¤„ç†çš„æ–‡ä»¶ï¼ˆæ”¯æŒæ‰¹æ¬¡è·¯å¾„ï¼‰"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ–‡ä»¶å
        filename = unquote(filename)
        
        final_dir = Path(DIRECTORIES["final_output_dir"])
        file_path = final_dir / filename
        if file_path.exists():
            file_path.unlink()
            log_activity(f"Deleted LLM processed file: {filename}")
            return jsonify({'success': True, 'message': 'æ–‡ä»¶åˆ é™¤æˆåŠŸ'})
        else:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
    except Exception as e:
        log_activity(f"Failed to delete file: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/check-duplicates', methods=['GET'])
def check_duplicates():
    """æ£€æŸ¥å“ªäº›æ–‡ä»¶æ˜¯é‡å¤çš„ï¼ˆæ£€æŸ¥æ‰€æœ‰å·²ä¸Šä¼ çš„é‚®ä»¶ï¼‰"""
    try:
        filenames_str = request.args.get('filenames', '')
        if not filenames_str:
            return jsonify({'success': True, 'duplicates': []})
        
        filenames = filenames_str.split(',')
        
        # æ”¶é›†æ‰€æœ‰æ‰¹æ¬¡ä¸­å·²ä¸Šä¼ çš„é‚®ä»¶æ–‡ä»¶å
        upload_dir = Path(DIRECTORIES["upload_dir"])
        existing_files = set()
        
        # æ£€æŸ¥æ‰€æœ‰æ‰¹æ¬¡ç›®å½•
        if upload_dir.exists():
            for batch_dir in upload_dir.iterdir():
                if batch_dir.is_dir() and batch_dir.name.startswith('batch_'):
                    # æ”¶é›†è¯¥æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰.emlæ–‡ä»¶
                    for eml_file in batch_dir.glob('*.eml'):
                        existing_files.add(eml_file.name)
        
        # æ‰¾å‡ºé‡å¤çš„æ–‡ä»¶åï¼ˆå·²åœ¨å…¶ä»–æ‰¹æ¬¡ä¸­ä¸Šä¼ è¿‡ï¼‰
        duplicates = [fname for fname in filenames if fname in existing_files]
        
        return jsonify({
            'success': True,
            'duplicates': duplicates
        })
    except Exception as e:
        log_activity(f"Error checking duplicate files: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/upload', methods=['POST'])
def upload_files():
    """ä¸Šä¼ EMLæ–‡ä»¶ - è‡ªåŠ¨æ‰¹æ¬¡ç®¡ç†"""
    try:
        if 'files' not in request.files:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ–‡ä»¶'}), 400
        
        files = request.files.getlist('files')
        
        # ç”Ÿæˆæ‰¹æ¬¡ID (ç²¾ç¡®åˆ°ç§’ + éšæœºåç¼€ï¼Œç¡®ä¿å”¯ä¸€æ€§)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        random_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=4))
        batch_id = f"batch_{timestamp}_{random_suffix}"
        
        # è·å–å¿…å¡«çš„æ‰¹æ¬¡æ ‡ç­¾
        batch_label = request.form.get('label', '').strip()
        if not batch_label:
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡æ ‡ç­¾ä¸ºå¿…å¡«é¡¹'}), 400
        
        # æ£€æŸ¥æ‰€æœ‰æ‰¹æ¬¡ä¸­å·²ä¸Šä¼ çš„æ–‡ä»¶
        upload_dir = Path(UPLOAD_FOLDER)
        existing_files = {}  # {filename: batch_id}
        
        if upload_dir.exists():
            for existing_batch_dir in upload_dir.iterdir():
                if existing_batch_dir.is_dir() and existing_batch_dir.name.startswith('batch_'):
                    # æ”¶é›†æ‰€æœ‰å·²ä¸Šä¼ çš„.emlæ–‡ä»¶
                    for eml_file in existing_batch_dir.glob('*.eml'):
                        existing_files[eml_file.name] = existing_batch_dir.name
        
        # åˆ›å»ºæ‰¹æ¬¡ç›®å½•
        batch_dir = Path(UPLOAD_FOLDER) / batch_id
        batch_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ–‡ä»¶å¹¶æ£€æµ‹é‡å¤
        uploaded_files = []
        file_details = []
        duplicate_files = []  # è®°å½•é‡å¤çš„æ–‡ä»¶
        
        for file in files:
            if file and allowed_file(file.filename):
                filename = secure_filename(file.filename)
                
                # æ£€æŸ¥æ˜¯å¦åœ¨å…¶ä»–æ‰¹æ¬¡ä¸­å·²å­˜åœ¨
                if filename in existing_files:
                    previous_batch = existing_files[filename]
                    duplicate_files.append({
                        'filename': filename,
                        'previous_batch': previous_batch,
                        'previous_time': 'N/A'
                    })
                    log_activity(f"âš ï¸ Skipped duplicate file: {filename} (already uploaded in batch {previous_batch})")
                    continue  # è·³è¿‡è¿™ä¸ªæ–‡ä»¶ï¼Œä¸ä¿å­˜
                
                # ä¿å­˜æ–‡ä»¶
                filepath = batch_dir / filename
                file.save(str(filepath))
                uploaded_files.append(filename)
                
                file_details.append({
                    "filename": filename,
                    "size": filepath.stat().st_size,
                    "upload_time": datetime.now().isoformat()
                })
                log_activity(f"Uploaded file to batch {batch_id}: {filename}")
        
        # åˆ›å»ºæ‰¹æ¬¡å…ƒæ•°æ®
        batch_info = {
            "batch_id": batch_id,
            "upload_time": datetime.now().isoformat(),
            "file_count": len(uploaded_files),
            "custom_label": batch_label,
            "status": {
                "uploaded": True,
                "cleaned": False,
                "llm_processed": False,
                "uploaded_to_kb": False
            },
            "files": file_details,
            "processing_history": {}
        }
        
        # ä¿å­˜æ‰¹æ¬¡å…ƒæ•°æ®
        # batch_id ä¿æŒç®€å•ç¨³å®šï¼Œä¸åŒ…å«æ ‡ç­¾å’Œæ–‡ä»¶æ•°
        # æ ‡ç­¾å’Œæ–‡ä»¶æ•°ä»…å­˜å‚¨åœ¨å…ƒæ•°æ®ä¸­ï¼Œç”¨äºå‰ç«¯å±•ç¤º
        batch_info_file = batch_dir / ".batch_info.json"
        with open(batch_info_file, 'w', encoding='utf-8') as f:
            json.dump(batch_info, f, ensure_ascii=False, indent=2)
        
        # å³ä½¿å…¨éƒ¨é‡å¤ä¹Ÿä¸æŠ¥é”™ï¼Œè¿”å›æˆåŠŸï¼ˆä½†countä¸º0ï¼‰
        if len(uploaded_files) == 0 and len(duplicate_files) > 0:
            log_activity(f"All files are duplicates, batch {batch_id} created but no new files")
        
        log_activity(f"Batch created: {batch_id}, {len(uploaded_files)} files uploaded successfully")
        if duplicate_files:
            log_activity(f"Skipped {len(duplicate_files)} duplicate files")
        
        return jsonify({
            'success': True,
            'batch_id': batch_id,
            'uploaded_files': uploaded_files,
            'count': len(uploaded_files),
            'duplicate_files': duplicate_files,
            'duplicate_count': len(duplicate_files),
            'message': f'æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶' + (f'ï¼Œè·³è¿‡ {len(duplicate_files)} ä¸ªé‡å¤æ–‡ä»¶' if duplicate_files else '')
        })
    except Exception as e:
        log_activity(f"Upload failed: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/uploaded-files', methods=['GET'])
def get_uploaded_files():
    """è·å–å·²ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆæ”¯æŒæ‰¹æ¬¡æ¨¡å¼ï¼‰"""
    try:
        upload_dir = Path(DIRECTORIES["upload_dir"])
        files = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ‰¹æ¬¡æ–‡ä»¶å¤¹
        batch_dirs = [d for d in upload_dir.iterdir() if d.is_dir()]
        
        if batch_dirs:
            # æ‰¹æ¬¡æ¨¡å¼ï¼šä»æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶å¤¹ä¸­æ”¶é›†æ–‡ä»¶
            for batch_dir in batch_dirs:
                batch_files = [f"{batch_dir.name}/{f.name}" for f in batch_dir.glob("*.eml")]
                files.extend(batch_files)
        else:
            # éæ‰¹æ¬¡æ¨¡å¼ï¼šç›´æ¥ä»ä¸Šä¼ ç›®å½•è·å–
            files = [f.name for f in upload_dir.glob("*.eml")]
        
        return jsonify({'success': True, 'files': files})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/processed-files', methods=['GET'])
def get_processed_files():
    """è·å–å·²å»é‡çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆæ”¯æŒæ‰¹æ¬¡æ¨¡å¼ï¼‰"""
    try:
        processed_dir = Path(DIRECTORIES["processed_dir"])
        files = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ‰¹æ¬¡æ–‡ä»¶å¤¹
        batch_dirs = [d for d in processed_dir.iterdir() if d.is_dir()]
        
        if batch_dirs:
            # æ‰¹æ¬¡æ¨¡å¼ï¼šä»æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶å¤¹ä¸­æ”¶é›†æ–‡ä»¶
            for batch_dir in batch_dirs:
                batch_files = [f"{batch_dir.name}/{f.name}" for f in batch_dir.glob("*.md")]
                files.extend(batch_files)
        else:
            # éæ‰¹æ¬¡æ¨¡å¼ï¼šç›´æ¥ä»å¤„ç†ç›®å½•è·å–
            files = [f.name for f in processed_dir.glob("*.md")]
        
        return jsonify({'success': True, 'files': files})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/llm-processed-files', methods=['GET'])
def get_llm_processed_files():
    """è·å–LLMå¤„ç†çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆæ”¯æŒæ‰¹æ¬¡æ¨¡å¼å’Œæ‰¹æ¬¡è¿‡æ»¤ï¼‰"""
    try:
        final_dir = Path(DIRECTORIES["final_output_dir"])
        files = []
        
        # è·å–å¯é€‰çš„batch_idå‚æ•°
        batch_id_filter = request.args.get('batch_id')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ‰¹æ¬¡æ–‡ä»¶å¤¹
        batch_dirs = [d for d in final_dir.iterdir() if d.is_dir()]
        
        if batch_dirs:
            # æ‰¹æ¬¡æ¨¡å¼ï¼šä»æ‰¹æ¬¡æ–‡ä»¶å¤¹ä¸­æ”¶é›†æ–‡ä»¶
            for batch_dir in batch_dirs:
                # å¦‚æœæŒ‡å®šäº†batch_idï¼Œåªå¤„ç†è¯¥æ‰¹æ¬¡
                if batch_id_filter and batch_dir.name != batch_id_filter:
                    continue
                    
                batch_files = [f"{batch_dir.name}/{f.name}" for f in batch_dir.glob("*.md")]
                files.extend(batch_files)
        else:
            # éæ‰¹æ¬¡æ¨¡å¼ï¼šç›´æ¥ä»æœ€ç»ˆè¾“å‡ºç›®å½•è·å–
            files = [f.name for f in final_dir.glob("*.md")]
        
        # æ£€æŸ¥å…¨å±€è¿›åº¦çŠ¶æ€
        global llm_processing_progress
        batch_key = batch_id_filter if batch_id_filter else 'default'
        is_processing = llm_processing_progress.get(batch_key, {}).get('is_processing', False)
        
        return jsonify({
            'success': True, 
            'files': files, 
            'count': len(files),
            'is_processing': is_processing  # æ–°å¢ï¼šæ˜¯å¦æ­£åœ¨å¤„ç†
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/kb-upload-progress', methods=['GET'])
def get_kb_upload_progress():
    """è·å–çŸ¥è¯†åº“ä¸Šä¼ è¿›åº¦ï¼ˆç”¨äºè¿›åº¦ç›‘æ§ï¼Œæ”¯æŒæ‰¹æ¬¡éš”ç¦»ï¼‰"""
    try:
        global kb_upload_progress
        
        # è·å–æ‰¹æ¬¡keyï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
        batch_key = request.args.get('batch_key', 'default')
        
        # å¦‚æœæ²¡æœ‰è¯¥æ‰¹æ¬¡çš„è¿›åº¦ï¼Œè¿”å›é»˜è®¤å€¼
        if batch_key not in kb_upload_progress:
            return jsonify({
                'success': True,
                'total': 0,
                'uploaded': 0,
                'is_uploading': False,
                'batch_key': batch_key
            })
        
        progress = kb_upload_progress[batch_key]
        
        # å¦‚æœä¸Šä¼ å®Œæˆï¼ˆuploaded === total ä¸” total > 0ï¼‰ï¼Œè‡ªåŠ¨æ›´æ–°æ‰¹æ¬¡çŠ¶æ€
        # è¿™ç¡®ä¿å³ä½¿è¯·æ±‚è¿”å›500æˆ– is_uploading æ ‡å¿—æœªåŠæ—¶æ›´æ–°ï¼ŒçŠ¶æ€ä¹Ÿä¼šè¢«æ›´æ–°
        if progress['total'] > 0 and progress['uploaded'] == progress['total']:
            # å°è¯•ä» batch_key æå–æ‰¹æ¬¡IDï¼ˆå¯èƒ½æ˜¯å•ä¸ªæ‰¹æ¬¡IDæˆ–ä¸‹åˆ’çº¿è¿æ¥çš„å¤šä¸ªï¼‰
            # å¦‚æœæ˜¯å•ä¸ªæ‰¹æ¬¡ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦‚æœæ˜¯å¤šä¸ªæ‰¹æ¬¡ï¼Œéœ€è¦æ‹†åˆ†
            batch_ids_from_key = batch_key.split('_') if '_' in batch_key and not batch_key.startswith('batch_') else [batch_key]
            # ä½†é€šå¸¸å•ä¸ªæ‰¹æ¬¡æ—¶ï¼Œbatch_keyå°±æ˜¯batch_idï¼Œå¤šä¸ªæ‰¹æ¬¡æ—¶ç”¨ä¸‹åˆ’çº¿è¿æ¥
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰¹æ¬¡IDæ ¼å¼
            if batch_key.startswith('batch_'):
                # å•ä¸ªæ‰¹æ¬¡ï¼Œç›´æ¥ä½¿ç”¨
                batch_ids_to_update = [batch_key]
            else:
                # å¯èƒ½æ˜¯å¤šä¸ªæ‰¹æ¬¡ç”¨ä¸‹åˆ’çº¿è¿æ¥ï¼Œå°è¯•è§£æ
                # ä½†å®é™…ä½¿ç”¨ä¸­ï¼Œbatch_keyé€šå¸¸æ˜¯å•ä¸ªæ‰¹æ¬¡ID
                batch_ids_to_update = [batch_key] if len(batch_key.split('_')) < 3 else batch_key.split('_')
            
            # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€ï¼ˆåªæ›´æ–°å·²ä¸Šä¼ å®Œæˆçš„æ‰¹æ¬¡ï¼‰
            for batch_id in batch_ids_to_update:
                try:
                    # éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ‰¹æ¬¡IDæ ¼å¼
                    if batch_id.startswith('batch_'):
                        # æ£€æŸ¥æ‰¹æ¬¡æ˜¯å¦å­˜åœ¨ä¸”çŠ¶æ€æœªæ›´æ–°
                        upload_dir = Path(DIRECTORIES["upload_dir"])
                        batch_dir = upload_dir / batch_id
                        if batch_dir.exists():
                            batch_info_file = batch_dir / ".batch_info.json"
                            if batch_info_file.exists():
                                with open(batch_info_file, 'r', encoding='utf-8') as f:
                                    batch_info = json.load(f)
                                if not batch_info.get('status', {}).get('uploaded_to_kb', False):
                                    update_batch_status_file(batch_id, 'uploaded_to_kb', True)
                                    log_activity(f"Auto-updated batch {batch_id} status to 'uploaded_to_kb': True (from progress check)")
                except Exception as e:
                    log_activity(f"Failed to auto-update batch status from progress check: {str(e)}")
        
        return jsonify({
            'success': True,
            'total': progress['total'],
            'uploaded': progress['uploaded'],
            'is_uploading': progress['is_uploading'],
            'batch_key': batch_key
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/file-content/uploaded/<path:filename>', methods=['GET'])
def get_uploaded_file_content(filename):
    """è·å–å·²ä¸Šä¼ æ–‡ä»¶çš„å†…å®¹ï¼ˆæ”¯æŒæ‰¹æ¬¡è·¯å¾„ï¼‰"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ–‡ä»¶å
        filename = unquote(filename)
        
        upload_dir = Path(DIRECTORIES["upload_dir"])
        file_path = upload_dir / filename
        
        if not file_path.exists():
            log_activity(f"æ–‡ä»¶ä¸å­˜åœ¨: {filename}, è·¯å¾„: {file_path}")
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        # è¯»å–EMLæ–‡ä»¶å†…å®¹
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        return jsonify({'success': True, 'content': content})
    except Exception as e:
        log_activity(f"Failed to read file content: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/file-content/processed/<path:filename>', methods=['GET'])
def get_processed_file_content(filename):
    """è·å–å·²å»é‡æ–‡ä»¶çš„å†…å®¹ï¼ˆæ”¯æŒæ‰¹æ¬¡è·¯å¾„ï¼‰"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ–‡ä»¶å
        filename = unquote(filename)
        
        processed_dir = Path(DIRECTORIES["processed_dir"])
        file_path = processed_dir / filename
        
        if not file_path.exists():
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        return jsonify({'success': True, 'content': content})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/file-content/llm-processed/<path:filename>', methods=['GET'])
def get_llm_processed_file_content(filename):
    """è·å–LLMå¤„ç†æ–‡ä»¶çš„å†…å®¹ï¼ˆæ”¯æŒæ‰¹æ¬¡è·¯å¾„ï¼‰"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ–‡ä»¶å
        filename = unquote(filename)
        
        final_dir = Path(DIRECTORIES["final_output_dir"])
        file_path = final_dir / filename
        
        if not file_path.exists():
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        return jsonify({'success': True, 'content': content})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/clean', methods=['POST'])
def clean_files():
    """æ¸…æ´—é‚®ä»¶æ–‡ä»¶"""
    try:
        data = request.json
        files = data.get('files', [])
        
        if not files:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦æ¸…æ´—'}), 400
        
        # ä½¿ç”¨EmailCleaneræ‰¹é‡å¤„ç†
        cleaner = EmailCleaner(
            input_dir=DIRECTORIES["upload_dir"],
            output_dir=DIRECTORIES["processed_dir"]
        )
        
        # æ‰§è¡Œæ¸…æ´—
        result = cleaner.process_all_emails()
        
        if result.get("success"):
            processed_count = len(result.get("generated_files", []))
            log_activity(f"Batch cleaning completed: {processed_count} files")
            return jsonify({
                'success': True,
                'processed_files': [Path(f).name for f in result.get("generated_files", [])],
                'count': processed_count,
                'report': result.get("report")
            })
        else:
            return jsonify({
                'success': False,
                'error': result.get("message", "æ¸…æ´—å¤±è´¥")
            }), 500
            
    except Exception as e:
        log_activity(f"Cleaning process failed: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/cleaned-files', methods=['GET'])
def get_cleaned_files():
    """è·å–å·²æ¸…æ´—çš„æ–‡ä»¶åˆ—è¡¨"""
    try:
        processed_dir = Path(DIRECTORIES["processed_dir"])
        files = [f.name for f in processed_dir.glob("*.md")]
        return jsonify({'success': True, 'files': files})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/llm-process', methods=['POST'])
def llm_process():
    """LLMå¤„ç†æ–‡ä»¶"""
    try:
        data = request.json
        files = data.get('files', [])
        api_key = data.get('api_key')
        endpoint = data.get('endpoint', 'internal')
        delay = data.get('delay', 2)
        
        if not files or not api_key:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
        api_client = GPTBotsAPI(api_key)
        
        processed_files = []
        errors = []
        
        for filename in files:
            input_path = os.path.join(DIRECTORIES["processed_dir"], filename)
            output_path = os.path.join(DIRECTORIES["final_output_dir"], filename)
            
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(input_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # åˆ›å»ºå¯¹è¯
                conversation_id = api_client.create_conversation()
                if not conversation_id:
                    raise Exception("åˆ›å»ºå¯¹è¯å¤±è´¥")
                
                # å‘é€å†…å®¹åˆ°LLMå¤„ç†
                result = api_client.send_message(conversation_id, content)
                
                if result and result.get('answer'):
                    # ä¿å­˜å¤„ç†åçš„å†…å®¹
                    processed_content = result['answer']
                    with open(output_path, 'w', encoding='utf-8') as f:
                        f.write(processed_content)
                    
                    processed_files.append(filename)
                    log_activity(f"LLM processing file: {filename}")
                else:
                    raise Exception("LLMæœªè¿”å›æœ‰æ•ˆå“åº”")
                
                # å»¶è¿Ÿé¿å…é™æµ
                import time
                time.sleep(delay)
                
            except Exception as e:
                error_msg = f"å¤„ç†å¤±è´¥: {filename} - {str(e)}"
                errors.append(error_msg)
                log_activity(f"LLM {error_msg}")
        
        return jsonify({
            'success': len(processed_files) > 0,
            'processed_files': processed_files,
            'count': len(processed_files),
            'errors': errors
        })
    except Exception as e:
        log_activity(f"LLM processing failed: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/results', methods=['GET'])
def get_results():
    """è·å–æ‰€æœ‰å¤„ç†ç»“æœ"""
    try:
        results = []
        
        # è·å–æ‰€æœ‰é˜¶æ®µçš„æ–‡ä»¶
        upload_dir = Path(DIRECTORIES["upload_dir"])
        processed_dir = Path(DIRECTORIES["processed_dir"])
        final_dir = Path(DIRECTORIES["final_output_dir"])
        
        # LLMå¤„ç†å®Œæˆçš„æ–‡ä»¶
        for f in final_dir.glob("*.md"):
            results.append({
                'filename': f.name,
                'stage': 'llm_processed',
                'size': f'{f.stat().st_size / 1024:.2f} KB',
                'processed_time': None
            })
        
        # åªæ¸…æ´—çš„æ–‡ä»¶
        for f in processed_dir.glob("*.md"):
            if not (final_dir / f.name).exists():
                results.append({
                    'filename': f.name,
                    'stage': 'cleaned',
                    'size': f'{f.stat().st_size / 1024:.2f} KB',
                    'processed_time': None
                })
        
        return jsonify({'success': True, 'results': results})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/file-content', methods=['GET'])
def get_file_content():
    """è·å–æ–‡ä»¶å†…å®¹"""
    try:
        filename = request.args.get('file')
        if not filename:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘æ–‡ä»¶å'}), 400
        
        # å°è¯•ä»å„ä¸ªç›®å½•æŸ¥æ‰¾æ–‡ä»¶
        for dir_path in [DIRECTORIES["final_output_dir"], DIRECTORIES["processed_dir"], DIRECTORIES["upload_dir"]]:
            filepath = Path(dir_path) / filename
            if filepath.exists():
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                return jsonify({'success': True, 'content': content})
        
        return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/download', methods=['GET'])
def download_file():
    """ä¸‹è½½æ–‡ä»¶"""
    try:
        filename = request.args.get('file')
        if not filename:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘æ–‡ä»¶å'}), 400
        
        # å°è¯•ä»å„ä¸ªç›®å½•æŸ¥æ‰¾æ–‡ä»¶
        for dir_path in [DIRECTORIES["final_output_dir"], DIRECTORIES["processed_dir"], DIRECTORIES["upload_dir"]]:
            filepath = Path(dir_path) / filename
            if filepath.exists():
                return send_file(filepath, as_attachment=True)
        
        return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500




@app.route('/api/upload-to-kb', methods=['POST'])
def upload_to_knowledge_base():
    """ä¸Šä¼ æ–‡ä»¶åˆ°çŸ¥è¯†åº“"""
    try:
        data = request.json
        files = data.get('files', [])
        api_key = data.get('api_key')
        knowledge_base_id = data.get('knowledge_base_id', '')
        chunk_token = data.get('chunk_token', 600)
        
        if not files or not api_key:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        kb_api = KnowledgeBaseAPI(api_key)
        uploaded_count = 0
        
        for filename in files:
            filepath = os.path.join(DIRECTORIES["final_output_dir"], filename)
            
            if not os.path.exists(filepath):
                log_activity(f"File not found: {filename}")
                continue
            
            try:
                # ä¸Šä¼ åˆ°çŸ¥è¯†åº“
                result = kb_api.upload_file(
                    filepath,
                    knowledge_base_id=knowledge_base_id,
                    chunk_token=chunk_token
                )
                
                if result:
                    uploaded_count += 1
                    log_activity(f"Uploaded to knowledge base: {filename}")
                    
            except Exception as e:
                log_activity(f"Failed to upload to knowledge base: {filename} - {str(e)}")
        
        return jsonify({
            'success': True,
            'uploaded_count': uploaded_count
        })
    except Exception as e:
        log_activity(f"Knowledge base upload process failed: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/qa-chat', methods=['POST'])
def qa_chat():
    """é—®ç­”èŠå¤©"""
    try:
        data = request.json
        message = data.get('message')
        api_key = data.get('api_key')
        conversation_id = data.get('conversation_id', '')
        
        if not message or not api_key:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        # åˆå§‹åŒ–GPTBots API
        api_client = GPTBotsAPI(api_key)
        
        # å¦‚æœæ²¡æœ‰conversation_idï¼Œå…ˆåˆ›å»º
        if not conversation_id:
            conversation_id = api_client.create_conversation()
            if not conversation_id:
                return jsonify({'success': False, 'error': 'åˆ›å»ºå¯¹è¯å¤±è´¥'}), 500
        
        # å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤
        response = api_client.send_message(conversation_id, message)
        
        if response and 'answer' in response:
            return jsonify({
                'success': True,
                'reply': response['answer'],
                'conversation_id': conversation_id
            })
        else:
            return jsonify({'success': False, 'error': 'APIå“åº”å¼‚å¸¸'}), 500
            
    except Exception as e:
        log_activity(f"QA chat failed: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({'status': 'ok'})


@app.errorhandler(500)
def internal_error(error):
    """å…¨å±€500é”™è¯¯å¤„ç†å™¨"""
    import traceback
    error_trace = traceback.format_exc()
    error_msg = str(error)
    
    # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
    log_activity(f"[500 ERROR] {error_msg}")
    log_activity(f"[500 ERROR] Traceback: {error_trace}")
    
    # ä¹Ÿæ‰“å°åˆ°æ§åˆ¶å°
    print(f"[500 ERROR] {error_msg}")
    print(f"[500 ERROR] Traceback:\n{error_trace}")
    
    return jsonify({
        'success': False,
        'error': f'Internal Server Error: {error_msg}',
        'traceback': error_trace
    }), 500


@app.errorhandler(Exception)
def handle_exception(e):
    """å…¨å±€å¼‚å¸¸å¤„ç†å™¨"""
    import traceback
    error_trace = traceback.format_exc()
    error_msg = str(e)
    
    # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
    log_activity(f"[EXCEPTION] {error_msg}")
    log_activity(f"[EXCEPTION] Traceback: {error_trace}")
    
    # ä¹Ÿæ‰“å°åˆ°æ§åˆ¶å°
    print(f"[EXCEPTION] {error_msg}")
    print(f"[EXCEPTION] Traceback:\n{error_trace}")
    
    return jsonify({
        'success': False,
        'error': f'Exception: {error_msg}',
        'traceback': error_trace
    }), 500


# è¾…åŠ©å‡½æ•°ï¼šæ›´æ–°æ‰¹æ¬¡çŠ¶æ€
def update_batch_status_file(batch_id: str, status_key: str, status_value: bool = True):
    """æ›´æ–°æ‰¹æ¬¡çŠ¶æ€åˆ°å…ƒæ•°æ®æ–‡ä»¶"""
    try:
        from datetime import datetime
        import json
        
        upload_dir = Path(DIRECTORIES["upload_dir"])
        batch_dir = upload_dir / batch_id
        
        if not batch_dir.exists():
            log_activity(f"Warning: Batch directory not found: {batch_id}")
            return False
        
        batch_info_file = batch_dir / ".batch_info.json"
        if not batch_info_file.exists():
            log_activity(f"Warning: Batch metadata not found: {batch_id}")
            return False
        
        # è¯»å–å¹¶æ›´æ–°å…ƒæ•°æ®
        with open(batch_info_file, 'r', encoding='utf-8') as f:
            batch_info = json.load(f)
        
        # æ›´æ–°çŠ¶æ€
        if 'status' not in batch_info:
            batch_info['status'] = {}
        batch_info['status'][status_key] = status_value
        
        # è®°å½•å¤„ç†æ—¶é—´
        if 'processing_history' not in batch_info:
            batch_info['processing_history'] = {}
        if status_value:
            batch_info['processing_history'][f"{status_key}_at"] = datetime.now().isoformat()
        
        # ä¿å­˜
        with open(batch_info_file, 'w', encoding='utf-8') as f:
            json.dump(batch_info, f, ensure_ascii=False, indent=2)
        
        log_activity(f"Batch {batch_id} status updated: {status_key} = {status_value}")
        return True
    except Exception as e:
        log_activity(f"Failed to update batch status {batch_id}: {str(e)}")
        return False


# å…¨å±€åœæ­¢æ ‡å¿—ï¼ˆç”¨äºè·¨è¯·æ±‚é€šä¿¡ï¼‰
from threading import Event
global_stop_event = Event()

# å…¨å±€çŸ¥è¯†åº“ä¸Šä¼ è¿›åº¦è·Ÿè¸ªï¼ˆæ‰¹æ¬¡éš”ç¦»ï¼‰
# æ ¼å¼: {batch_key: {'total': int, 'uploaded': int, 'is_uploading': bool}}
kb_upload_progress = {}

# å…¨å±€LLMå¤„ç†è¿›åº¦è·Ÿè¸ªï¼ˆæ‰¹æ¬¡éš”ç¦»ï¼‰
# æ ¼å¼: {batch_key: {'total': int, 'processed': int, 'failed': int, 'is_processing': bool}}
llm_processing_progress = {}

@app.route('/api/auto/stop', methods=['POST'])
def auto_stop():
    """åœæ­¢å½“å‰çš„è‡ªåŠ¨å¤„ç†æµç¨‹"""
    try:
        global_stop_event.set()
        log_activity("Received stop request, stopping processing...")
        return jsonify({'success': True, 'message': 'åœæ­¢ä¿¡å·å·²å‘é€'})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/auto/clear-processing-status', methods=['POST'])
def clear_processing_status():
    """æ¸…é™¤å¤„ç†çŠ¶æ€æ ‡å¿—ï¼ˆç”¨äºè§£å†³å¡ä½çš„çŠ¶æ€ï¼‰"""
    try:
        data = request.json
        batch_id = data.get('batch_id')
        
        global llm_processing_progress, kb_upload_progress
        
        if batch_id:
            # æ¸…é™¤ç‰¹å®šæ‰¹æ¬¡çš„çŠ¶æ€
            if batch_id in llm_processing_progress:
                llm_processing_progress[batch_id]['is_processing'] = False
                log_activity(f"Cleared LLM processing status for batch: {batch_id}")
            
            if batch_id in kb_upload_progress:
                kb_upload_progress[batch_id]['is_uploading'] = False
                log_activity(f"Cleared KB upload status for batch: {batch_id}")
            
            return jsonify({
                'success': True,
                'message': f'å·²æ¸…é™¤æ‰¹æ¬¡ {batch_id} çš„å¤„ç†çŠ¶æ€'
            })
        else:
            # æ¸…é™¤æ‰€æœ‰æ‰¹æ¬¡çš„çŠ¶æ€
            for key in llm_processing_progress:
                llm_processing_progress[key]['is_processing'] = False
            for key in kb_upload_progress:
                kb_upload_progress[key]['is_uploading'] = False
            
            log_activity("Cleared all processing status flags")
            return jsonify({
                'success': True,
                'message': 'å·²æ¸…é™¤æ‰€æœ‰å¤„ç†çŠ¶æ€'
            })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


# å…¨è‡ªåŠ¨å¤„ç†æµç¨‹API
@app.route('/api/auto/clean', methods=['POST'])
def auto_clean():
    """å…¨è‡ªåŠ¨æµç¨‹ - æ­¥éª¤1: é‚®ä»¶æ¸…æ´—"""
    try:
        # å®‰å…¨è·å–è¯·æ±‚æ•°æ®
        try:
            if not request.is_json:
                log_activity("Email cleaning error: Request is not JSON")
                return jsonify({'success': False, 'error': 'è¯·æ±‚å¿…é¡»æ˜¯JSONæ ¼å¼'}), 400
            
            data = request.get_json(silent=True)
            if not data:
                log_activity("Email cleaning error: Request JSON is None or invalid")
                return jsonify({'success': False, 'error': 'è¯·æ±‚æ•°æ®ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯'}), 400
        except Exception as e:
            log_activity(f"Email cleaning error: Failed to parse request JSON: {str(e)}")
            return jsonify({'success': False, 'error': f'è§£æè¯·æ±‚æ•°æ®å¤±è´¥: {str(e)}'}), 400
        
        batch_ids = data.get('batch_ids', [])
        skip_if_exists = data.get('skip_if_exists', True)  # é»˜è®¤å¯ç”¨æ™ºèƒ½è·³è¿‡
        
        log_activity(f"Email cleaning request received: batch_ids={batch_ids}, skip_if_exists={skip_if_exists}")
        
        upload_dir = Path(DIRECTORIES["upload_dir"])
        processed_dir = Path(DIRECTORIES["processed_dir"])
        
        # æ™ºèƒ½è·³è¿‡ï¼šæ£€æŸ¥æ˜¯å¦å·²æœ‰å¤„ç†åçš„æ–‡ä»¶
        if skip_if_exists and batch_ids:
            skipped_batches = []
            batches_to_process = []
            
            for batch_id in batch_ids:
                processed_batch_dir = processed_dir / batch_id
                if processed_batch_dir.exists():
                    md_files = list(processed_batch_dir.glob("*.md"))
                    if md_files:
                        log_activity(f"Batch {batch_id} already has {len(md_files)} processed files, skipping cleaning step")
                        skipped_batches.append(batch_id)
                        continue  # åªæœ‰åœ¨æœ‰MDæ–‡ä»¶æ—¶æ‰è·³è¿‡
                # ç›®å½•ä¸å­˜åœ¨ï¼Œæˆ–è€…å­˜åœ¨ä½†æ²¡æœ‰MDæ–‡ä»¶ï¼Œéƒ½åº”è¯¥å¤„ç†
                batches_to_process.append(batch_id)
            
            if not batches_to_process:
                log_activity(f"All batches already cleaned, skipping this step")
                return jsonify({
                    'success': True,
                    'processed_count': 0,
                    'total_files': 0,
                    'duplicates': 0,
                    'skipped': True,
                    'skipped_batches': skipped_batches,
                    'message': 'æ‰€æœ‰æ‰¹æ¬¡éƒ½å·²å®Œæˆæ¸…æ´—ï¼Œå·²è·³è¿‡'
                })
            
            batch_ids = batches_to_process
            log_activity(f"Will process {len(batch_ids)} batches, skipping {len(skipped_batches)} already processed batches")
        
        if batch_ids:
            # æ‰¹æ¬¡æ¨¡å¼ï¼šåªå¤„ç†é€‰ä¸­çš„æ‰¹æ¬¡
            log_activity(f"Starting email cleaning (selected {len(batch_ids)} batches): {batch_ids}")
            log_disk_usage("[æ¸…æ´—å‰] ")
            cleaner = EmailCleaner(input_dir=str(upload_dir), output_dir=str(processed_dir), batch_mode=True)
            result = cleaner.process_all_emails(selected_batches=batch_ids)
        else:
            # éæ‰¹æ¬¡æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰æ–‡ä»¶
            log_activity(f"Starting email cleaning: {upload_dir}")
            log_disk_usage("[æ¸…æ´—å‰] ")
            cleaner = EmailCleaner(input_dir=str(upload_dir), output_dir=str(processed_dir))
            result = cleaner.process_all_emails()
        
        if result.get('success'):
            # ä»reportä¸­è·å–å¤„ç†æ•°é‡
            report = result.get('report', {})
            processed_count = report.get('unique_emails', len(result.get('generated_files', [])))
            log_activity(f"Email cleaning completed: {processed_count} files")
            log_disk_usage("[æ¸…æ´—å] ")
            
            # è®°å½•å…¨å±€å»é‡ä¿¡æ¯
            global_duplicates = report.get('all_global_duplicates', [])
            if global_duplicates:
                log_activity(f"Detected {len(global_duplicates)} cross-batch duplicate emails, automatically skipped")
                for dup in global_duplicates:
                    log_activity(f"  - {dup['file_name']} (å·²åœ¨æ‰¹æ¬¡ {dup['previous_batch']} ä¸­å¤„ç†)")
            
            return jsonify({
                'success': True,
                'processed_count': processed_count,
                'total_files': report.get('total_input_files', 0),
                'duplicates': report.get('duplicate_emails', 0),
                'global_duplicates': global_duplicates,
                'message': 'é‚®ä»¶æ¸…æ´—å®Œæˆ'
            })
        else:
            error_msg = result.get('message', 'é‚®ä»¶æ¸…æ´—å¤±è´¥')
            log_activity(f"Email cleaning failed: {error_msg}")
            return jsonify({'success': False, 'error': error_msg})
            
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        log_activity(f"Email cleaning error: {str(e)}")
        log_activity(f"é”™è¯¯å †æ ˆ: {error_trace}")
        print(f"é‚®ä»¶æ¸…æ´—å¼‚å¸¸: {str(e)}")
        print(f"é”™è¯¯å †æ ˆ:\n{error_trace}")
        return jsonify({'success': False, 'error': str(e), 'traceback': error_trace}), 500


@app.route('/api/auto/llm-process', methods=['POST'])
def auto_llm_process():
    """å…¨è‡ªåŠ¨æµç¨‹ - æ­¥éª¤2: LLMå¤„ç†ï¼ˆæ”¯æŒå¹¶å‘å’Œåœæ­¢ï¼‰"""
    try:
        # æ¸…é™¤ä¹‹å‰çš„åœæ­¢æ ‡å¿—
        global_stop_event.clear()
        
        # å®‰å…¨è·å–è¯·æ±‚æ•°æ®
        try:
            if not request.is_json:
                log_activity("LLM processing error: Request is not JSON")
                return jsonify({'success': False, 'error': 'è¯·æ±‚å¿…é¡»æ˜¯JSONæ ¼å¼'}), 400
            
            data = request.get_json(silent=True)
            if not data:
                log_activity("LLM processing error: Request JSON is None or invalid")
                return jsonify({'success': False, 'error': 'è¯·æ±‚æ•°æ®ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯'}), 400
        except Exception as e:
            log_activity(f"LLM processing error: Failed to parse request JSON: {str(e)}")
            return jsonify({'success': False, 'error': f'è§£æè¯·æ±‚æ•°æ®å¤±è´¥: {str(e)}'}), 400
        
        api_key = data.get('api_key')
        delay = data.get('delay', 1)  # é»˜è®¤1ç§’é—´éš”
        batch_ids = data.get('batch_ids', [])
        skip_if_exists = data.get('skip_if_exists', True)  # é»˜è®¤å¯ç”¨æ™ºèƒ½è·³è¿‡
        max_workers = data.get('max_workers', 1)  # å¹¶å‘æ•°ï¼Œé»˜è®¤1ä¸ªï¼ˆä¸²è¡Œï¼‰
        
        # ç¡®ä¿ batch_ids æ˜¯åˆ—è¡¨æ ¼å¼
        if batch_ids and not isinstance(batch_ids, list):
            if isinstance(batch_ids, str):
                batch_ids = [batch_ids]
            else:
                log_activity(f"LLM processing error: Invalid batch_ids type: {type(batch_ids)}")
                batch_ids = []
        
        if not api_key:
            log_activity("LLM processing error: Missing API Key")
            return jsonify({'success': False, 'error': 'ç¼ºå°‘API Key'}), 400
        
        # è®°å½•è¯·æ±‚å‚æ•°ï¼ˆéšè—æ•æ„Ÿä¿¡æ¯ï¼‰
        log_activity(f"LLM processing request: batch_ids={batch_ids}, delay={delay}, max_workers={max_workers}, skip_if_exists={skip_if_exists}")
        
        processed_dir = Path(DIRECTORIES["processed_dir"])
        final_dir = Path(DIRECTORIES["final_output_dir"])
        final_dir.mkdir(parents=True, exist_ok=True)
        
        # æ™ºèƒ½è·³è¿‡ï¼šæ£€æŸ¥æ˜¯å¦å·²æœ‰LLMå¤„ç†åçš„æ–‡ä»¶ï¼ˆæ¯”è¾ƒprocessedå’Œfinal_outputçš„æ–‡ä»¶æ•°ï¼‰
        if skip_if_exists and batch_ids:
            skipped_batches = []
            batches_to_process = []
            
            for batch_id in batch_ids:
                final_batch_dir = final_dir / batch_id
                processed_batch_dir = processed_dir / batch_id
                
                if final_batch_dir.exists() and processed_batch_dir.exists():
                    # ç»Ÿè®¡ä¸¤ä¸ªç›®å½•çš„æ–‡ä»¶æ•°
                    llm_files = list(final_batch_dir.glob("*.md"))
                    processed_files = [f for f in processed_batch_dir.glob("*.md") if f.name != "processing_report.json"]
                    
                    # åªæœ‰å½“final_outputçš„æ–‡ä»¶æ•°ç­‰äºprocessedçš„æ–‡ä»¶æ•°æ—¶ï¼Œæ‰è®¤ä¸ºå·²å®Œæˆ
                    if llm_files and len(llm_files) >= len(processed_files):
                        log_activity(f"Batch {batch_id} LLM processing completed: {len(llm_files)}/{len(processed_files)} files, skipping LLM processing step")
                        skipped_batches.append(batch_id)
                        # æ›´æ–°çŠ¶æ€ï¼ˆå¦‚æœå°šæœªæ›´æ–°ï¼‰
                        update_batch_status_file(batch_id, 'llm_processed', True)
                        continue
                    elif llm_files:
                        log_activity(f"Batch {batch_id} LLM processing incomplete: processed {len(llm_files)}/{len(processed_files)} files, continuing with remaining files")
                
                batches_to_process.append(batch_id)
            
            if not batches_to_process:
                log_activity(f"All batches already LLM processed, skipping this step")
                # æ›´æ–°æ‰€æœ‰è·³è¿‡æ‰¹æ¬¡çš„çŠ¶æ€
                for batch_id in skipped_batches:
                    update_batch_status_file(batch_id, 'llm_processed', True)
                
                return jsonify({
                    'success': True,
                    'processed_count': 0,
                    'failed_count': 0,
                    'total_files_after_dedup': 0,
                    'skipped': True,
                    'skipped_batches': skipped_batches,
                    'message': 'æ‰€æœ‰æ‰¹æ¬¡éƒ½å·²å®ŒæˆLLMå¤„ç†ï¼Œå·²è·³è¿‡'
                })
            
            batch_ids = batches_to_process
            log_activity(f"Will process {len(batch_ids)} batches, skipping {len(skipped_batches)} already processed batches")
        
        # è·å–æ‰€æœ‰markdownæ–‡ä»¶ï¼ˆæ”¯æŒæ‰¹æ¬¡æ¨¡å¼ï¼‰
        md_files = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ‰¹æ¬¡æ–‡ä»¶å¤¹
        batch_dirs = [d for d in processed_dir.iterdir() if d.is_dir()]
        
        if batch_dirs:
            # æ‰¹æ¬¡æ¨¡å¼ï¼šä»æŒ‡å®šæ‰¹æ¬¡æ–‡ä»¶å¤¹ä¸­æ”¶é›†æ–‡ä»¶
            if batch_ids:
                # åªå¤„ç†é€‰ä¸­çš„æ‰¹æ¬¡
                log_activity(f"LLM processing: selected {len(batch_ids)} batches")
                for batch_id in batch_ids:
                    batch_dir = processed_dir / batch_id
                    if batch_dir.exists() and batch_dir.is_dir():
                        batch_md_files = list(batch_dir.glob("*.md"))
                        batch_md_files = [f for f in batch_md_files if f.name != "processing_report.json"]
                        md_files.extend(batch_md_files)
                        log_activity(f"Batch {batch_id}: found {len(batch_md_files)} files")
            else:
                # å¤„ç†æ‰€æœ‰æ‰¹æ¬¡
                for batch_dir in batch_dirs:
                    batch_md_files = list(batch_dir.glob("*.md"))
                    batch_md_files = [f for f in batch_md_files if f.name != "processing_report.json"]
                    md_files.extend(batch_md_files)
        else:
            # éæ‰¹æ¬¡æ¨¡å¼ï¼šç›´æ¥ä»å¤„ç†ç›®å½•è·å–
            md_files = list(processed_dir.glob("*.md"))
            md_files = [f for f in md_files if f.name != "processing_report.json"]
        
        if not md_files:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ‰¾åˆ°å¾…å¤„ç†çš„æ–‡ä»¶'}), 404
        
        # è®°å½•å»é‡åçš„å®é™…æ–‡ä»¶æ•°
        total_files_after_dedup = len(md_files)
        log_activity(f"LLM processing: total {total_files_after_dedup} files to process after deduplication")
        
        # æ£€æŸ¥æœ‰å¤šå°‘æ–‡ä»¶å®é™…ä¸Šéœ€è¦å¤„ç†ï¼ˆä¸åŒ…æ‹¬å·²å­˜åœ¨çš„æ–‡ä»¶ï¼‰
        files_needing_processing = []
        for md_file in md_files:
            # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
            if md_file.parent != processed_dir:
                batch_name = md_file.parent.name
                batch_final_dir = final_dir / batch_name
                output_file = batch_final_dir / md_file.name
            else:
                output_file = final_dir / md_file.name
            
            # åªæ·»åŠ ä¸å­˜åœ¨çš„æ–‡ä»¶
            if not output_file.exists():
                files_needing_processing.append(md_file)
        
        # å¦‚æœæ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†è¿‡ï¼Œç›´æ¥è¿”å›æˆåŠŸ
        if not files_needing_processing:
            log_activity(f"All {total_files_after_dedup} files already processed, nothing to do")
            # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€
            if batch_ids:
                for batch_id in batch_ids:
                    update_batch_status_file(batch_id, 'llm_processed', True)
            
            return jsonify({
                'success': True,
                'processed_count': total_files_after_dedup,
                'failed_count': 0,
                'total_files_after_dedup': total_files_after_dedup,
                'message': f'æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆï¼ˆå…± {total_files_after_dedup} ä¸ªæ–‡ä»¶ï¼‰',
                'all_already_processed': True
            })
        
        log_activity(f"Found {len(files_needing_processing)} files that need processing (out of {total_files_after_dedup} total)")
        log_activity(f"LLM processing: concurrency set to {max_workers}")
        log_disk_usage("[LLMå¤„ç†å‰] ")
        
        # åˆå§‹åŒ–å…¨å±€è¿›åº¦è·Ÿè¸ª
        global llm_processing_progress
        batch_key = batch_ids[0] if batch_ids else 'default'
        llm_processing_progress[batch_key] = {
            'total': total_files_after_dedup,
            'processed': 0,
            'failed': 0,
            'is_processing': True
        }
        
        # åˆå§‹åŒ–GPTBots APIå®¢æˆ·ç«¯
        client = GPTBotsAPI(api_key)
        processed_count = 0
        failed_count = 0
        
        # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„è®¡æ•°å™¨å’Œåœæ­¢æ ‡å¿—
        from threading import Lock, Event
        count_lock = Lock()
        # ä½¿ç”¨å…¨å±€åœæ­¢æ ‡å¿—ï¼ˆå¯ä»¥è¢«/api/auto/stopè§¦å‘ï¼‰
        stop_event = global_stop_event
        
        # LLMæç¤ºè¯æ¨¡æ¿
        llm_prompt_template = """ä»¥ä¸‹æ˜¯éœ€è¦å¤„ç†çš„é‚®ä»¶å†…å®¹ï¼Œè¯·å¸®æˆ‘æ•´ç†å’Œä¼˜åŒ–ï¼š

{email_content}"""
        
        def process_single_file(md_file):
            """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆçº¿ç¨‹å®‰å…¨ï¼Œæ”¯æŒåœæ­¢ï¼‰"""
            nonlocal processed_count, failed_count
            
            # æ£€æŸ¥åœæ­¢ä¿¡å·
            if stop_event.is_set():
                log_activity(f"Received stop signal, skipping: {md_file.name}")
                return False
            
            try:
                # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
                if md_file.parent != processed_dir:
                    # æ‰¹æ¬¡æ¨¡å¼ï¼šåœ¨final_dirä¸­åˆ›å»ºå¯¹åº”çš„æ‰¹æ¬¡ç›®å½•
                    batch_name = md_file.parent.name
                    batch_final_dir = final_dir / batch_name
                    batch_final_dir.mkdir(parents=True, exist_ok=True)
                    output_file = batch_final_dir / md_file.name
                else:
                    # éæ‰¹æ¬¡æ¨¡å¼ï¼šç›´æ¥ä¿å­˜åˆ°final_dir
                    output_file = final_dir / md_file.name
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å¤„ç†è¿‡ï¼ˆè·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶ï¼‰
                if output_file.exists():
                    log_activity(f"File already processed, skipped: {md_file.name}")
                    with count_lock:
                        processed_count += 1  # è®¡å…¥å·²å¤„ç†æ•°
                    return True
                
                # å†æ¬¡æ£€æŸ¥åœæ­¢ä¿¡å·ï¼ˆåœ¨å¼€å§‹å¤„ç†å‰ï¼‰
                if stop_event.is_set():
                    log_activity(f"Received stop signal, aborting processing: {md_file.name}")
                    return False
                
                # è®°å½•å½“å‰å¤„ç†è¿›åº¦
                with count_lock:
                    current_progress = processed_count + failed_count + 1
                log_activity(f"[{current_progress}/{total_files_after_dedup}] Processing: {md_file.name}")
                
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # åˆ›å»ºå¯¹è¯
                conversation_id = client.create_conversation()
                if not conversation_id:
                    log_activity(f"Failed to create conversation: {md_file.name}")
                    with count_lock:
                        failed_count += 1
                    return False
                
                # å‘é€æ¶ˆæ¯
                prompt = llm_prompt_template.format(email_content=content)
                response = client.send_message(conversation_id, prompt)
                
                if response and "output" in response:
                    # æå–LLMå¤„ç†ç»“æœ
                    output_list = response.get("output", [])
                    processed_content = ""
                    
                    for output_item in output_list:
                        if "content" in output_item:
                            content_obj = output_item["content"]
                            if "text" in content_obj:
                                processed_content += content_obj["text"] + "\n"
                    
                    if processed_content.strip():
                        # ä¿å­˜å¤„ç†ç»“æœ
                        with open(output_file, 'w', encoding='utf-8') as f:
                            f.write(processed_content.strip())
                        
                        with count_lock:
                            processed_count += 1
                            current_count = processed_count
                        log_activity(f"[{current_count}/{total_files_after_dedup}] Successfully processed: {md_file.name}")
                        return True
                    else:
                        log_activity(f"LLM returned empty content: {md_file.name}")
                        with count_lock:
                            failed_count += 1
                        return False
                else:
                    # è®°å½•å“åº”è¯¦æƒ…ä»¥ä¾¿æ’æŸ¥
                    if response:
                        if isinstance(response, dict):
                            # æ‰“å°å®Œæ•´çš„é”™è¯¯å“åº”
                            error_code = response.get('code', 'N/A')
                            error_msg = response.get('message', 'N/A')
                            log_activity(f"LLM call failed (no output): {md_file.name}")
                            log_activity(f"  Error code: {error_code}, Error message: {error_msg}")
                            log_activity(f"  Full response keys: {list(response.keys())}")
                        else:
                            log_activity(f"LLM call failed (no output): {md_file.name}, response type: {type(response)}")
                    else:
                        log_activity(f"LLM call failed (no response): {md_file.name}")
                    with count_lock:
                        failed_count += 1
                    return False
                
            except Exception as e:
                log_activity(f"File processing error {md_file.name}: {str(e)}")
                with count_lock:
                    failed_count += 1
                return False
        
        # å¹¶å‘å¤„ç†æ–‡ä»¶
        try:
            if max_workers > 1:
                # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
                from concurrent.futures import ThreadPoolExecutor, as_completed
                import threading
                
                log_activity(f"ä½¿ç”¨å¹¶å‘æ¨¡å¼å¤„ç† (workers={max_workers})")
                
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    # æäº¤æ‰€æœ‰ä»»åŠ¡
                    futures = {executor.submit(process_single_file, md_file): md_file for md_file in files_needing_processing}
                    
                    # ç­‰å¾…å®Œæˆï¼Œæ·»åŠ å»¶è¿Ÿé¿å…è¿‡å¿«
                    for future in as_completed(futures):
                        try:
                            future.result()
                            # å¹¶å‘æ—¶çš„å»¶è¿Ÿï¼šæ€»å»¶è¿Ÿ/å¹¶å‘æ•°ï¼Œé¿å…APIé™æµ
                            time.sleep(delay / max_workers)
                        except Exception as e:
                            log_activity(f"å¹¶å‘ä»»åŠ¡å¼‚å¸¸: {str(e)}")
            else:
                # ä¸²è¡Œå¤„ç†ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
                log_activity(f"ä½¿ç”¨ä¸²è¡Œæ¨¡å¼å¤„ç†")
                for md_file in files_needing_processing:
                    try:
                        process_single_file(md_file)
                        time.sleep(delay)  # å»¶è¿Ÿé¿å…APIé™æµ
                    except Exception as e:
                        # å•ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥ä¸åº”è¯¥ä¸­æ–­æ•´ä¸ªæµç¨‹ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                        log_activity(f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª: {md_file.name}, é”™è¯¯: {str(e)}")
                        continue
        except Exception as e:
            # å¤„ç†å¾ªç¯ä¸­çš„å¼‚å¸¸ä¸åº”è¯¥ä¸­æ–­ï¼Œåº”è¯¥è®°å½•å¹¶ç»§ç»­
            log_activity(f"æ–‡ä»¶å¤„ç†å¾ªç¯ä¸­çš„å¼‚å¸¸: {str(e)}")
            import traceback
            log_activity(f"å¤„ç†å¾ªç¯å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        
        # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€
        if batch_ids and processed_count > 0:
            for batch_id in batch_ids:
                update_batch_status_file(batch_id, 'llm_processed', True)
        
        log_activity(f"LLM processing completed: {processed_count} successful, {failed_count} failed")
        log_disk_usage("[LLMå¤„ç†å] ")
        
        # æ ‡è®°å¤„ç†å®Œæˆ
        if batch_key in llm_processing_progress:
            llm_processing_progress[batch_key]['is_processing'] = False
            llm_processing_progress[batch_key]['processed'] = processed_count
            llm_processing_progress[batch_key]['failed'] = failed_count
        
        if processed_count > 0:
            return jsonify({
                'success': True,
                'processed_count': processed_count,
                'failed_count': failed_count,
                'total_files_after_dedup': total_files_after_dedup,
                'message': f'LLMå¤„ç†å®Œæˆ: æˆåŠŸ {processed_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ªï¼ˆå»é‡åå…± {total_files_after_dedup} ä¸ªæ–‡ä»¶ï¼‰'
            })
        else:
            error_msg = f'LLMå¤„ç†å¤±è´¥: æ‰€æœ‰æ–‡ä»¶å¤„ç†å¤±è´¥ã€‚è¯·æ£€æŸ¥API Keyæ˜¯å¦æ­£ç¡®ï¼Œæ˜¯å¦æœ‰å¯¹è¯æƒé™'
            log_activity(error_msg)
            return jsonify({
                'success': False,
                'processed_count': 0,
                'failed_count': failed_count,
                'error': error_msg
            })
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        log_activity(f"LLM processing error: {str(e)}")
        log_activity(f"é”™è¯¯å †æ ˆ: {error_trace}")
        print(f"LLMå¤„ç†å¼‚å¸¸: {str(e)}")
        print(f"é”™è¯¯å †æ ˆ:\n{error_trace}")
        
        # å¼‚å¸¸æ—¶ä¹Ÿæ ‡è®°å¤„ç†å®Œæˆï¼Œå¹¶ä¿å­˜å·²å¤„ç†çš„è¿›åº¦
        if 'batch_key' in locals() and batch_key in llm_processing_progress:
            llm_processing_progress[batch_key]['is_processing'] = False
            # å¦‚æœæœ‰éƒ¨åˆ†å¤„ç†æˆåŠŸï¼Œä¿å­˜è¿›åº¦
            if 'processed_count' in locals():
                llm_processing_progress[batch_key]['processed'] = processed_count
            if 'failed_count' in locals():
                llm_processing_progress[batch_key]['failed'] = failed_count
        
        # å¦‚æœå·²æœ‰éƒ¨åˆ†æ–‡ä»¶å¤„ç†æˆåŠŸï¼Œå³ä½¿å‘ç”Ÿå¼‚å¸¸ä¹Ÿè¿”å›éƒ¨åˆ†æˆåŠŸçš„ç»“æœ
        if 'processed_count' in locals() and processed_count > 0:
            log_activity(f"LLMå¤„ç†éƒ¨åˆ†å®Œæˆ: {processed_count} æˆåŠŸï¼Œå‘ç”Ÿå¼‚å¸¸ä½†å·²å¤„ç†çš„æ–‡ä»¶å·²ä¿å­˜")
            return jsonify({
                'success': True,
                'processed_count': processed_count,
                'failed_count': failed_count if 'failed_count' in locals() else 0,
                'total_files_after_dedup': total_files_after_dedup if 'total_files_after_dedup' in locals() else 0,
                'message': f'LLMå¤„ç†éƒ¨åˆ†å®Œæˆ: æˆåŠŸ {processed_count} ä¸ªï¼ˆå¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}ï¼‰',
                'warning': f'å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}'
            })
        
        return jsonify({'success': False, 'error': str(e), 'traceback': error_trace}), 500


@app.route('/api/auto/upload-kb', methods=['POST'])
def auto_upload_kb():
    """å…¨è‡ªåŠ¨æµç¨‹ - æ­¥éª¤3: ä¸Šä¼ åˆ°çŸ¥è¯†åº“"""
    try:
        data = request.json
        api_key = data.get('api_key')
        kb_id = data.get('knowledge_base_id')
        chunk_token = data.get('chunk_token')
        chunk_separator = data.get('chunk_separator')
        batch_size = data.get('batch_size', 10)
        skip_if_exists = data.get('skip_if_exists', True)  # é»˜è®¤å¯ç”¨æ™ºèƒ½è·³è¿‡
        
        if not api_key or not kb_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…éœ€å‚æ•°'}), 400
        
        # å¿…é¡»æä¾›chunk_tokenæˆ–chunk_separatorä¹‹ä¸€
        if not chunk_token and not chunk_separator:
            return jsonify({'success': False, 'error': 'å¿…é¡»æä¾›chunk_tokenæˆ–chunk_separator'}), 400
        
        batch_ids = data.get('batch_ids', [])
        
        # æ™ºèƒ½è·³è¿‡ï¼šæ£€æŸ¥æ‰¹æ¬¡çŠ¶æ€
        if skip_if_exists and batch_ids:
            import json
            skipped_batches = []
            batches_to_process = []
            upload_dir = Path(DIRECTORIES["upload_dir"])
            
            for batch_id in batch_ids:
                batch_dir = upload_dir / batch_id
                batch_info_file = batch_dir / ".batch_info.json"
                
                if batch_info_file.exists():
                    try:
                        with open(batch_info_file, 'r', encoding='utf-8') as f:
                            batch_info = json.load(f)
                        
                        if batch_info.get('status', {}).get('uploaded_to_kb', False):
                            log_activity(f"Batch {batch_id} already uploaded to knowledge base, skipping upload step")
                            skipped_batches.append(batch_id)
                            continue
                    except Exception as e:
                        log_activity(f"Failed to read batch metadata {batch_id}: {str(e)}")
                
                batches_to_process.append(batch_id)
            
            if not batches_to_process:
                log_activity(f"All batches already uploaded to knowledge base, skipping this step")
                return jsonify({
                    'success': True,
                    'uploaded_count': 0,
                    'skipped': True,
                    'skipped_batches': skipped_batches,
                    'message': 'æ‰€æœ‰æ‰¹æ¬¡éƒ½å·²å®ŒæˆçŸ¥è¯†åº“ä¸Šä¼ ï¼Œå·²è·³è¿‡'
                })
            
            batch_ids = batches_to_process
            log_activity(f"Will upload {len(batch_ids)} batches, skipping {len(skipped_batches)} already uploaded batches")
        
        final_dir = Path(DIRECTORIES["final_output_dir"])
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ‰¹æ¬¡æ–‡ä»¶å¤¹
        batch_dirs = [d for d in final_dir.iterdir() if d.is_dir()]
        
        if batch_dirs:
            # æ‰¹æ¬¡æ¨¡å¼ï¼šæ”¶é›†æŒ‡å®šæ‰¹æ¬¡ä¸­çš„æ–‡ä»¶
            md_files = []
            if batch_ids:
                # åªå¤„ç†é€‰ä¸­çš„æ‰¹æ¬¡
                log_activity(f"çŸ¥è¯†åº“ä¸Šä¼ : é€‰ä¸­ {len(batch_ids)} ä¸ªæ‰¹æ¬¡")
                for batch_id in batch_ids:
                    batch_dir = final_dir / batch_id
                    if batch_dir.exists() and batch_dir.is_dir():
                        batch_md_files = list(batch_dir.glob("*.md"))
                        md_files.extend(batch_md_files)
                        log_activity(f"Batch {batch_id}: found {len(batch_md_files)} files")
            else:
                # å¤„ç†æ‰€æœ‰æ‰¹æ¬¡
                for batch_dir in batch_dirs:
                    batch_md_files = list(batch_dir.glob("*.md"))
                    md_files.extend(batch_md_files)
        else:
            # éæ‰¹æ¬¡æ¨¡å¼ï¼šç›´æ¥ä»final_dirè·å–
            md_files = list(final_dir.glob("*.md"))
        
        if not md_files:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ‰¾åˆ°å¾…ä¸Šä¼ çš„æ–‡ä»¶'}), 404
        
        # åˆå§‹åŒ–ä¸Šä¼ è¿›åº¦ï¼ˆä½¿ç”¨æ‰¹æ¬¡keyéš”ç¦»ï¼‰
        global kb_upload_progress
        batch_key = '_'.join(batch_ids) if batch_ids else 'default'
        kb_upload_progress[batch_key] = {
            'total': len(md_files),
            'uploaded': 0,
            'is_uploading': True
        }
        
        log_activity(f"Starting upload of {len(md_files)} files to knowledge base")
        log_disk_usage("[ä¸Šä¼ å‰] ")
        
        kb_client = KnowledgeBaseAPI(api_key)
        
        # æ‰¹æ¬¡æ¨¡å¼éœ€è¦é€ä¸ªä¸Šä¼ æ–‡ä»¶ï¼Œéæ‰¹æ¬¡æ¨¡å¼å¯ä»¥ä½¿ç”¨ç›®å½•ä¸Šä¼ 
        if batch_dirs:
            # æ‰¹æ¬¡æ¨¡å¼ï¼šä½¿ç”¨å¹¶å‘ä¸Šä¼ æé«˜é€Ÿåº¦
            successful_uploads = 0
            failed_uploads = 0
            
            # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤è®¡æ•°å™¨
            from threading import Lock
            upload_lock = Lock()
            
            # å®šä¹‰å•ä¸ªæ–‡ä»¶ä¸Šä¼ å‡½æ•°
            def upload_single_file(md_file):
                nonlocal successful_uploads, failed_uploads
                try:
                    # è¯»å–æ–‡ä»¶å†…å®¹
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # æ„å»ºä¸Šä¼ å‚æ•°
                    upload_params = {
                        'content': content,
                        'filename': md_file.name,
                        'knowledge_base_id': kb_id
                    }
                    
                    if chunk_token:
                        upload_params['chunk_token'] = chunk_token
                        upload_params['splitter'] = "PARAGRAPH"
                    else:
                        upload_params['chunk_token'] = 600
                        upload_params['splitter'] = "PARAGRAPH"
                    
                    # ä¸Šä¼ å•ä¸ªæ–‡ä»¶
                    result = kb_client.upload_markdown_content(**upload_params)
                    
                    with upload_lock:
                        if result and 'error' not in result:
                            successful_uploads += 1
                            kb_upload_progress[batch_key]['uploaded'] = successful_uploads
                            log_activity(f"âœ“ Uploaded: {md_file.name}")
                        else:
                            failed_uploads += 1
                            log_activity(f"âœ— Upload failed: {md_file.name}, error: {result.get('error', 'Unknown error')}")
                    
                    return True
                    
                except Exception as e:
                    with upload_lock:
                        failed_uploads += 1
                        kb_upload_progress[batch_key]['uploaded'] = successful_uploads
                        log_activity(f"âœ— Upload error {md_file.name}: {str(e)}")
                    return False
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸Šä¼ ï¼ˆ3ä¸ªå¹¶å‘ï¼Œå¹³è¡¡é€Ÿåº¦å’ŒAPIå‹åŠ›ï¼‰
            from concurrent.futures import ThreadPoolExecutor, as_completed
            max_upload_workers = 3  # 3ä¸ªæ–‡ä»¶åŒæ—¶ä¸Šä¼ 
            
            log_activity(f"Starting concurrent upload with {max_upload_workers} workers")
            
            with ThreadPoolExecutor(max_workers=max_upload_workers) as executor:
                futures = {executor.submit(upload_single_file, f): f for f in md_files}
                
                for future in as_completed(futures):
                    try:
                        future.result()
                        # å°å»¶è¿Ÿé¿å…è¿‡å¿«ï¼ˆæ¯ä¸ªworkeréƒ½æœ‰å»¶è¿Ÿï¼‰
                        time.sleep(0.5)
                    except Exception as e:
                        log_activity(f"Upload task exception: {str(e)}")
            
            # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€ - æ ‡è®°ä¸ºå·²ä¸Šä¼ å¹¶æ·»åŠ çŸ¥è¯†åº“åç§°æ ‡ç­¾
            # å³ä½¿åç»­å‘ç”Ÿå¼‚å¸¸ï¼Œä¹Ÿè¦ç¡®ä¿çŠ¶æ€å·²æ›´æ–°
            if batch_ids and successful_uploads > 0:
                # å…ˆæ›´æ–°æ‰¹æ¬¡çŠ¶æ€ï¼ˆæ— è®ºæ˜¯å¦æˆåŠŸè·å–çŸ¥è¯†åº“åç§°ï¼‰
                try:
                    for batch_id in batch_ids:
                        update_batch_status_file(batch_id, 'uploaded_to_kb', True)
                        log_activity(f"Batch {batch_id} status updated to 'uploaded_to_kb': True")
                except Exception as e:
                    log_activity(f"Warning: Failed to update batch status: {str(e)}")
                
                # è·å–çŸ¥è¯†åº“åç§°ï¼ˆå¯é€‰æ“ä½œï¼Œå¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼‰
                kb_name = None
                try:
                    log_activity(f"æ­£åœ¨è·å–çŸ¥è¯†åº“åç§°ï¼ŒKB ID: {kb_id}")
                    kb_response = kb_client.get_knowledge_bases()
                    log_activity(f"çŸ¥è¯†åº“APIå“åº”: {kb_response}")
                    
                    # å°è¯•å¤šç§å¯èƒ½çš„å“åº”ç»“æ„
                    kb_list = None
                    if kb_response:
                        # æ ¼å¼1: {'data': {'list': [...]}}
                        if 'data' in kb_response and isinstance(kb_response['data'], dict) and 'list' in kb_response['data']:
                            kb_list = kb_response['data']['list']
                        # æ ¼å¼2: {'data': [...]}
                        elif 'data' in kb_response and isinstance(kb_response['data'], list):
                            kb_list = kb_response['data']
                        # æ ¼å¼3: {'knowledge_base': [...]}
                        elif 'knowledge_base' in kb_response and isinstance(kb_response['knowledge_base'], list):
                            kb_list = kb_response['knowledge_base']
                        # æ ¼å¼4: ç›´æ¥æ˜¯åˆ—è¡¨
                        elif isinstance(kb_response, list):
                            kb_list = kb_response
                    
                    if kb_list:
                        log_activity(f"Found {len(kb_list)} knowledge bases")
                        for kb in kb_list:
                            if kb.get('id') == kb_id:
                                kb_name = kb.get('name', '')
                                log_activity(f"åŒ¹é…åˆ°çŸ¥è¯†åº“: {kb_name}")
                                break
                    else:
                        log_activity(f"æ— æ³•ä»å“åº”ä¸­è§£æçŸ¥è¯†åº“åˆ—è¡¨")
                        
                except Exception as e:
                    log_activity(f"Failed to get knowledge base name (non-critical): {str(e)}")
                
                # å¦‚æœæˆåŠŸè·å–çŸ¥è¯†åº“åç§°ï¼Œä¿å­˜åˆ°æ‰¹æ¬¡å…ƒæ•°æ®ï¼ˆå¯é€‰æ“ä½œï¼Œå¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼‰
                if kb_name:
                    for batch_id in batch_ids:
                        try:
                            upload_dir = Path(DIRECTORIES["upload_dir"])
                            batch_dir = upload_dir / batch_id
                            batch_info_file = batch_dir / ".batch_info.json"
                            
                            if batch_info_file.exists():
                                with open(batch_info_file, 'r', encoding='utf-8') as f:
                                    batch_info = json.load(f)
                                
                                batch_info['kb_name'] = kb_name
                                
                                with open(batch_info_file, 'w', encoding='utf-8') as f:
                                    json.dump(batch_info, f, ensure_ascii=False, indent=2)
                                
                                log_activity(f"Batch {batch_id} automatically tagged with knowledge base: {kb_name}")
                            else:
                                log_activity(f"Warning: Batch info file not found for {batch_id}, cannot save KB name")
                        except Exception as e:
                            log_activity(f"Failed to save knowledge base name to batch {batch_id} (non-critical): {str(e)}")
                else:
                    log_activity(f"Warning: Unable to get knowledge base name, batches need manual tagging")
            
            log_activity(f"Knowledge base upload completed: {successful_uploads} successful, {failed_uploads} failed")
            log_disk_usage("[ä¸Šä¼ å] ")
            
            if successful_uploads > 0:
                kb_upload_progress[batch_key]['is_uploading'] = False  # æ ‡è®°ä¸Šä¼ å®Œæˆ
                return jsonify({
                    'success': True,
                    'uploaded_count': successful_uploads,
                    'message': f'çŸ¥è¯†åº“ä¸Šä¼ å®Œæˆ: æˆåŠŸ {successful_uploads} ä¸ªï¼Œå¤±è´¥ {failed_uploads} ä¸ª'
                })
            else:
                kb_upload_progress[batch_key]['is_uploading'] = False  # æ ‡è®°ä¸Šä¼ å®Œæˆï¼ˆå³ä½¿å¤±è´¥ï¼‰
                return jsonify({'success': False, 'error': 'æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ å¤±è´¥'}), 500
        
        # éæ‰¹æ¬¡æ¨¡å¼ï¼šä½¿ç”¨ç›®å½•ä¸Šä¼ 
        # æ ¹æ®åˆ†å—æ–¹å¼æ„å»ºå‚æ•°
        upload_params = {
            'directory_path': str(final_dir),
            'knowledge_base_id': kb_id,
            'batch_size': batch_size
        }
        
        if chunk_token:
            upload_params['chunk_token'] = chunk_token
            upload_params['splitter'] = "PARAGRAPH"
        else:
            upload_params['chunk_separator'] = chunk_separator
            upload_params['splitter'] = "CUSTOM"
        
        result = kb_client.upload_markdown_files_from_directory(**upload_params)
        
        if batch_key in kb_upload_progress:
            kb_upload_progress[batch_key]['is_uploading'] = False  # æ ‡è®°ä¸Šä¼ å®Œæˆ
        
        if result and 'error' not in result:
            return jsonify({
                'success': True,
                'uploaded_count': result.get('successful_uploads', 0),
                'message': 'çŸ¥è¯†åº“ä¸Šä¼ å®Œæˆ'
            })
        else:
            return jsonify({'success': False, 'error': result.get('error', 'ä¸Šä¼ å¤±è´¥')}), 500
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        log_activity(f"KB upload error: {str(e)}")
        log_activity(f"é”™è¯¯å †æ ˆ: {error_trace}")
        
        # å³ä½¿å‘ç”Ÿå¼‚å¸¸ï¼Œå¦‚æœæœ‰éƒ¨åˆ†ä¸Šä¼ æˆåŠŸï¼Œä¹Ÿè¦æ›´æ–°æ‰¹æ¬¡çŠ¶æ€
        if 'batch_ids' in locals() and 'successful_uploads' in locals():
            if batch_ids and successful_uploads > 0:
                try:
                    log_activity(f"Exception occurred but {successful_uploads} files uploaded successfully, updating batch status...")
                    for batch_id in batch_ids:
                        update_batch_status_file(batch_id, 'uploaded_to_kb', True)
                        log_activity(f"Batch {batch_id} status updated to 'uploaded_to_kb': True (despite exception)")
                except Exception as status_error:
                    log_activity(f"Failed to update batch status after exception: {str(status_error)}")
        
        # æ ‡è®°ä¸Šä¼ å®Œæˆ
        if 'batch_key' in locals() and batch_key in kb_upload_progress:
            kb_upload_progress[batch_key]['is_uploading'] = False  # å¼‚å¸¸æ—¶ä¹Ÿæ ‡è®°å®Œæˆ
        
        # å¦‚æœæœ‰éƒ¨åˆ†æˆåŠŸï¼Œè¿”å›éƒ¨åˆ†æˆåŠŸçš„ç»“æœè€Œä¸æ˜¯å®Œå…¨å¤±è´¥
        if 'successful_uploads' in locals() and successful_uploads > 0:
            log_activity(f"Returning partial success: {successful_uploads} files uploaded despite exception")
            return jsonify({
                'success': True,
                'uploaded_count': successful_uploads,
                'failed_count': failed_uploads if 'failed_uploads' in locals() else 0,
                'message': f'çŸ¥è¯†åº“ä¸Šä¼ éƒ¨åˆ†å®Œæˆ: æˆåŠŸ {successful_uploads} ä¸ªï¼ˆå¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}ï¼‰',
                'warning': f'å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}'
            })
        
        return jsonify({'success': False, 'error': str(e), 'traceback': error_trace}), 500


# ========== å•æ‰¹æ¬¡å®Œæ•´å¤„ç† APIï¼ˆç”¨äºå¹¶è¡Œå¤„ç†ï¼‰ ==========

@app.route('/api/auto/process-single-batch', methods=['POST'])
def process_single_batch():
    """å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„å®Œæ•´æµç¨‹ï¼ˆæ¸…æ´—â†’LLMâ†’ä¸Šä¼ ï¼‰
    
    ç”¨äºå¤šæ‰¹æ¬¡å¹¶è¡Œå¤„ç†åœºæ™¯ï¼Œæ¯ä¸ªæ‰¹æ¬¡ç‹¬ç«‹è°ƒç”¨æ­¤API
    """
    try:
        data = request.json
        batch_id = data.get('batch_id')
        llm_api_key = data.get('llm_api_key')
        kb_api_key = data.get('kb_api_key')
        kb_id = data.get('knowledge_base_id')
        chunk_token = data.get('chunk_token')
        chunk_separator = data.get('chunk_separator')
        max_workers = data.get('max_workers', 1)  # å•æ‰¹æ¬¡é»˜è®¤1ä¸ªå¹¶å‘ï¼ˆä¸²è¡Œï¼‰
        delay = data.get('delay', 1)  # é»˜è®¤1ç§’é—´éš”
        skip_if_exists = data.get('skip_if_exists', True)
        
        if not all([batch_id, llm_api_key, kb_api_key, kb_id]):
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…éœ€å‚æ•°'}), 400
        
        log_activity(f"[Batch {batch_id}] Starting complete processing flow")
        
        result = {
            'batch_id': batch_id,
            'success': True,
            'steps': {}
        }
        
        # ========== æ­¥éª¤1: é‚®ä»¶æ¸…æ´— ==========
        try:
            log_activity(f"[Batch {batch_id}] Step 1/3: Email cleaning")
            
            upload_dir = Path(DIRECTORIES["upload_dir"])
            processed_dir = Path(DIRECTORIES["processed_dir"])
            
            # æ£€æŸ¥æ˜¯å¦å·²æ¸…æ´—
            should_clean = True
            if skip_if_exists:
                processed_batch_dir = processed_dir / batch_id
                if processed_batch_dir.exists():
                    md_files = list(processed_batch_dir.glob("*.md"))
                    if md_files:
                        log_activity(f"[Batch {batch_id}] Already cleaned ({len(md_files)} files), skipping")
                        should_clean = False
                        result['steps']['clean'] = {
                            'success': True,
                            'processed_count': len(md_files),
                            'skipped': True
                        }
            
            if should_clean:
                cleaner = EmailCleaner(
                    input_dir=str(upload_dir),
                    output_dir=str(processed_dir),
                    batch_mode=True
                )
                clean_result = cleaner.process_all_emails(selected_batches=[batch_id])
                
                result['steps']['clean'] = {
                    'success': clean_result.get('success', False),
                    'processed_count': clean_result.get('unique_files', 0),
                    'duplicates': clean_result.get('duplicates', 0)
                }
                
                if not clean_result.get('success'):
                    raise Exception(f"Cleaning failed: {clean_result.get('message', 'Unknown error')}")
                
                log_activity(f"[Batch {batch_id}] Cleaned: {result['steps']['clean']['processed_count']} files")
        
        except Exception as e:
            log_activity(f"[Batch {batch_id}] Cleaning error: {str(e)}")
            result['steps']['clean'] = {'success': False, 'error': str(e)}
            result['success'] = False
            return jsonify(result), 500
        
        # ========== æ­¥éª¤2: LLMå¤„ç† ==========
        try:
            log_activity(f"[Batch {batch_id}] Step 2/3: LLM processing")
            
            processed_dir = Path(DIRECTORIES["processed_dir"])
            final_dir = Path(DIRECTORIES["final_output_dir"])
            final_dir.mkdir(parents=True, exist_ok=True)
            
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            should_process_llm = True
            if skip_if_exists:
                final_batch_dir = final_dir / batch_id
                processed_batch_dir = processed_dir / batch_id
                
                if final_batch_dir.exists() and processed_batch_dir.exists():
                    llm_files = list(final_batch_dir.glob("*.md"))
                    processed_files = [f for f in processed_batch_dir.glob("*.md") if f.name != "processing_report.json"]
                    
                    if llm_files and len(llm_files) >= len(processed_files):
                        log_activity(f"[Batch {batch_id}] Already LLM processed ({len(llm_files)} files), skipping")
                        should_process_llm = False
                        result['steps']['llm'] = {
                            'success': True,
                            'processed_count': len(llm_files),
                            'skipped': True
                        }
            
            if should_process_llm:
                # æ”¶é›†è¯¥æ‰¹æ¬¡çš„MDæ–‡ä»¶
                batch_dir = processed_dir / batch_id
                if not batch_dir.exists():
                    raise Exception(f"Processed directory not found: {batch_dir}")
                
                md_files = list(batch_dir.glob("*.md"))
                md_files = [f for f in md_files if f.name != "processing_report.json"]
                
                if not md_files:
                    raise Exception(f"No files to process in batch {batch_id}")
                
                log_activity(f"[Batch {batch_id}] Found {len(md_files)} files for LLM processing")
                
                # LLMå¤„ç†ï¼ˆä½¿ç”¨å¹¶å‘ï¼‰
                from concurrent.futures import ThreadPoolExecutor, as_completed
                import threading
                
                llm_api = GPTBotsAPI(llm_api_key)
                processed_count = 0
                failed_count = 0
                count_lock = threading.Lock()
                
                def process_file(md_file):
                    nonlocal processed_count, failed_count
                    try:
                        if global_stop_event.is_set():
                            return False
                        
                        with open(md_file, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        conversation_id = llm_api.create_conversation()
                        if not conversation_id:
                            with count_lock:
                                failed_count += 1
                            return False
                        
                        response = llm_api.send_message(conversation_id, content)
                        if not response:
                            with count_lock:
                                failed_count += 1
                            return False
                        
                        # æå–å®é™…çš„å›å¤å†…å®¹
                        llm_content = response.get('data', {}).get('answer', '') if isinstance(response, dict) else str(response)
                        
                        if not llm_content:
                            log_activity(f"[Batch {batch_id}] Warning: Empty LLM response for {md_file.name}")
                            with count_lock:
                                failed_count += 1
                            return False
                        
                        # ä¿å­˜ç»“æœ
                        output_batch_dir = final_dir / batch_id
                        output_batch_dir.mkdir(parents=True, exist_ok=True)
                        output_file = output_batch_dir / md_file.name
                        
                        with open(output_file, 'w', encoding='utf-8') as f:
                            f.write(llm_content)
                        
                        with count_lock:
                            processed_count += 1
                        
                        return True
                    except Exception as e:
                        log_activity(f"[Batch {batch_id}] File processing error {md_file.name}: {str(e)}")
                        with count_lock:
                            failed_count += 1
                        return False
                
                # å¹¶å‘å¤„ç†
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = {executor.submit(process_file, f): f for f in md_files}
                    
                    for future in as_completed(futures):
                        try:
                            future.result()
                            time.sleep(delay / max_workers)
                        except Exception as e:
                            log_activity(f"[Batch {batch_id}] Task error: {str(e)}")
                
                result['steps']['llm'] = {
                    'success': processed_count > 0,
                    'processed_count': processed_count,
                    'failed_count': failed_count
                }
                
                log_activity(f"[Batch {batch_id}] LLM processed: {processed_count} success, {failed_count} failed")
                
                if processed_count == 0:
                    raise Exception("All LLM processing failed")
        
        except Exception as e:
            log_activity(f"[Batch {batch_id}] LLM processing error: {str(e)}")
            result['steps']['llm'] = {'success': False, 'error': str(e)}
            result['success'] = False
            return jsonify(result), 500
        
        # ========== æ­¥éª¤3: çŸ¥è¯†åº“ä¸Šä¼  ==========
        try:
            log_activity(f"[Batch {batch_id}] Step 3/3: Knowledge base upload")
            
            # æ£€æŸ¥æ˜¯å¦å·²ä¸Šä¼ 
            should_upload = True
            if skip_if_exists:
                upload_dir = Path(DIRECTORIES["upload_dir"])
                batch_dir = upload_dir / batch_id
                batch_info_file = batch_dir / ".batch_info.json"
                
                if batch_info_file.exists():
                    import json
                    with open(batch_info_file, 'r', encoding='utf-8') as f:
                        batch_info = json.load(f)
                    
                    if batch_info.get('status', {}).get('uploaded_to_kb', False):
                        log_activity(f"[Batch {batch_id}] Already uploaded to KB, skipping")
                        should_upload = False
                        result['steps']['upload'] = {
                            'success': True,
                            'uploaded_count': 0,
                            'skipped': True
                        }
            
            if should_upload:
                # æ”¶é›†è¯¥æ‰¹æ¬¡çš„æœ€ç»ˆæ–‡ä»¶
                final_batch_dir = final_dir / batch_id
                if not final_batch_dir.exists():
                    raise Exception(f"Final output directory not found: {final_batch_dir}")
                
                md_files = list(final_batch_dir.glob("*.md"))
                if not md_files:
                    raise Exception(f"No files to upload in batch {batch_id}")
                
                log_activity(f"[Batch {batch_id}] Found {len(md_files)} files for upload")
                
                # åˆå§‹åŒ–è¿›åº¦è¿½è¸ª
                batch_key = batch_id
                global kb_upload_progress
                kb_upload_progress[batch_key] = {
                    'total': len(md_files),
                    'uploaded': 0,
                    'is_uploading': True
                }
                
                # ä¸Šä¼ æ–‡ä»¶
                kb_client = KnowledgeBaseAPI(kb_api_key)
                successful_uploads = 0
                failed_uploads = 0
                
                for i, md_file in enumerate(md_files, 1):
                    try:
                        with open(md_file, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        upload_params = {
                            'content': content,
                            'filename': md_file.name,
                            'knowledge_base_id': kb_id
                        }
                        
                        # æ·»åŠ åˆ†å—å‚æ•°
                        if chunk_token:
                            upload_params['chunk_token'] = chunk_token
                            upload_params['splitter'] = "PARAGRAPH"
                        elif chunk_separator:
                            upload_params['chunk_separator'] = chunk_separator
                            upload_params['splitter'] = "CUSTOM"
                        else:
                            upload_params['chunk_token'] = 600
                            upload_params['splitter'] = "PARAGRAPH"
                        
                        result_upload = kb_client.upload_markdown_content(**upload_params)
                        
                        if result_upload and 'error' not in result_upload:
                            successful_uploads += 1
                            kb_upload_progress[batch_key]['uploaded'] = successful_uploads
                        else:
                            failed_uploads += 1
                        
                        time.sleep(2.0)  # APIé™æµï¼ˆå¢åŠ åˆ°2ç§’ï¼Œé™ä½å¹¶å‘ï¼‰
                    
                    except Exception as e:
                        failed_uploads += 1
                        kb_upload_progress[batch_key]['uploaded'] = successful_uploads
                        log_activity(f"[Batch {batch_id}] Upload error {md_file.name}: {str(e)}")
                
                kb_upload_progress[batch_key]['is_uploading'] = False
                
                # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€
                if successful_uploads > 0:
                    update_batch_status_file(batch_id, 'uploaded_to_kb', True)
                
                result['steps']['upload'] = {
                    'success': successful_uploads > 0,
                    'uploaded_count': successful_uploads,
                    'failed_count': failed_uploads
                }
                
                log_activity(f"[Batch {batch_id}] Uploaded: {successful_uploads} success, {failed_uploads} failed")
                
                if successful_uploads == 0:
                    raise Exception("All uploads failed")
        
        except Exception as e:
            log_activity(f"[Batch {batch_id}] Upload error: {str(e)}")
            if 'batch_key' in locals() and batch_key in kb_upload_progress:
                kb_upload_progress[batch_key]['is_uploading'] = False
            result['steps']['upload'] = {'success': False, 'error': str(e)}
            result['success'] = False
            return jsonify(result), 500
        
        # æ£€æŸ¥æ•´ä½“æˆåŠŸçŠ¶æ€
        result['success'] = all(
            step.get('success', False) 
            for step in result['steps'].values()
        )
        
        log_activity(f"[Batch {batch_id}] Complete! Success: {result['success']}")
        
        return jsonify(result)
    
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        log_activity(f"[Batch {batch_id if 'batch_id' in locals() else 'Unknown'}] Fatal error: {str(e)}")
        print(f"é”™è¯¯å †æ ˆ:\n{error_trace}")
        return jsonify({'success': False, 'error': str(e), 'traceback': error_trace}), 500


# ========== æ‰¹æ¬¡ç®¡ç† API ==========

@app.route('/api/batches', methods=['GET'])
def get_batches():
    """è·å–æ‰€æœ‰æ‰¹æ¬¡åˆ—è¡¨"""
    try:
        upload_dir = Path(DIRECTORIES["upload_dir"])
        import json
        from datetime import datetime
        
        batches = []
        for batch_dir in sorted(upload_dir.iterdir(), key=lambda x: x.name, reverse=True):
            if not batch_dir.is_dir():
                continue
            
            # è¯»å–æ‰¹æ¬¡å…ƒæ•°æ®
            batch_info_file = batch_dir / ".batch_info.json"
            if batch_info_file.exists():
                with open(batch_info_file, 'r', encoding='utf-8') as f:
                    batch_info = json.load(f)
            else:
                # å¦‚æœæ²¡æœ‰å…ƒæ•°æ®ï¼Œç”ŸæˆåŸºæœ¬ä¿¡æ¯ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
                eml_files = list(batch_dir.glob("*.eml"))
                batch_info = {
                    "batch_id": batch_dir.name,
                    "upload_time": datetime.fromtimestamp(batch_dir.stat().st_mtime).isoformat(),
                    "file_count": len(eml_files),
                    "custom_label": "",
                    "status": {
                        "uploaded": True,
                        "cleaned": False,
                        "llm_processed": False,
                        "uploaded_to_kb": False
                    }
                }
            
            batches.append(batch_info)
        
        return jsonify({
            'success': True,
            'batches': batches,
            'total': len(batches)
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/batches/<path:batch_id>', methods=['GET'])
def get_batch_details(batch_id):
    """è·å–ç‰¹å®šæ‰¹æ¬¡çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ‰¹æ¬¡ID
        batch_id = unquote(batch_id)
        
        upload_dir = Path(DIRECTORIES["upload_dir"])
        batch_dir = upload_dir / batch_id
        
        if not batch_dir.exists():
            log_activity(f"æ‰¹æ¬¡ä¸å­˜åœ¨: {batch_id}, è·¯å¾„: {batch_dir}")
            return jsonify({'success': False, 'error': f'æ‰¹æ¬¡ä¸å­˜åœ¨: {batch_id}'}), 404
        
        # è¯»å–æ‰¹æ¬¡å…ƒæ•°æ®
        batch_info_file = batch_dir / ".batch_info.json"
        if batch_info_file.exists():
            with open(batch_info_file, 'r', encoding='utf-8') as f:
                import json
                batch_info = json.load(f)
        else:
            log_activity(f"æ‰¹æ¬¡å…ƒæ•°æ®ä¸å­˜åœ¨: {batch_id}")
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡å…ƒæ•°æ®ä¸å­˜åœ¨'}), 404
        
        # è·å–æ–‡ä»¶åˆ—è¡¨
        files = [f.name for f in batch_dir.glob("*.eml")]
        batch_info['current_files'] = files
        
        return jsonify({
            'success': True,
            'batch': batch_info
        })
    except Exception as e:
        log_activity(f"Failed to get batch details: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/batches/<path:batch_id>/status', methods=['PUT'])
def update_batch_status(batch_id):
    """æ›´æ–°æ‰¹æ¬¡çŠ¶æ€"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ‰¹æ¬¡ID
        batch_id = unquote(batch_id)
        
        data = request.json
        status_key = data.get('status_key')  # cleaned, llm_processed, uploaded_to_kb
        status_value = data.get('status_value', True)
        
        upload_dir = Path(DIRECTORIES["upload_dir"])
        batch_dir = upload_dir / batch_id
        
        if not batch_dir.exists():
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡ä¸å­˜åœ¨'}), 404
        
        batch_info_file = batch_dir / ".batch_info.json"
        if not batch_info_file.exists():
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡å…ƒæ•°æ®ä¸å­˜åœ¨'}), 404
        
        # è¯»å–å¹¶æ›´æ–°å…ƒæ•°æ®
        with open(batch_info_file, 'r', encoding='utf-8') as f:
            import json
            from datetime import datetime
            batch_info = json.load(f)
        
        # æ›´æ–°çŠ¶æ€
        if 'status' not in batch_info:
            batch_info['status'] = {}
        batch_info['status'][status_key] = status_value
        
        # è®°å½•å¤„ç†æ—¶é—´
        if 'processing_history' not in batch_info:
            batch_info['processing_history'] = {}
        if status_value:
            batch_info['processing_history'][f"{status_key}_at"] = datetime.now().isoformat()
        
        # ä¿å­˜
        with open(batch_info_file, 'w', encoding='utf-8') as f:
            json.dump(batch_info, f, ensure_ascii=False, indent=2)
        
        return jsonify({'success': True, 'batch_info': batch_info})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/batches/<path:batch_id>/label', methods=['PUT'])
def update_batch_label(batch_id):
    """æ›´æ–°æ‰¹æ¬¡çš„è‡ªå®šä¹‰æ ‡ç­¾"""
    try:
        from urllib.parse import unquote
        import json
        
        # URL è§£ç æ‰¹æ¬¡ID
        batch_id = unquote(batch_id)
        
        data = request.json
        custom_label = data.get('custom_label', '').strip()
        
        if not custom_label:
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡æ ‡ç­¾ä¸èƒ½ä¸ºç©º'}), 400
        
        upload_dir = Path(DIRECTORIES["upload_dir"])
        batch_dir = upload_dir / batch_id
        
        if not batch_dir.exists():
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡ä¸å­˜åœ¨'}), 404
        
        batch_info_file = batch_dir / ".batch_info.json"
        if not batch_info_file.exists():
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡å…ƒæ•°æ®ä¸å­˜åœ¨'}), 404
        
        # è¯»å–å¹¶æ›´æ–°å…ƒæ•°æ®
        with open(batch_info_file, 'r', encoding='utf-8') as f:
            batch_info = json.load(f)
        
        # æ›´æ–°è‡ªå®šä¹‰æ ‡ç­¾
        batch_info['custom_label'] = custom_label
        
        # ä¿å­˜æ›´æ–°åçš„å…ƒæ•°æ®
        with open(batch_info_file, 'w', encoding='utf-8') as f:
            json.dump(batch_info, f, ensure_ascii=False, indent=2)
        
        log_activity(f"Batch {batch_id} label updated to: {custom_label}")
        
        return jsonify({
            'success': True,
            'message': 'æ‰¹æ¬¡æ ‡ç­¾æ›´æ–°æˆåŠŸ',
            'custom_label': custom_label
        })
        
    except Exception as e:
        log_activity(f"Failed to update batch label: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/batches/<path:batch_id>/kb-label', methods=['PUT'])
def update_batch_kb_label(batch_id):
    """æ›´æ–°æ‰¹æ¬¡çš„çŸ¥è¯†åº“æ ‡ç­¾"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ‰¹æ¬¡ID
        batch_id = unquote(batch_id)
        
        data = request.json
        kb_name = data.get('kb_name', '').strip()
        
        if not kb_name:
            return jsonify({'success': False, 'error': 'çŸ¥è¯†åº“åç§°ä¸èƒ½ä¸ºç©º'}), 400
        
        upload_dir = Path(DIRECTORIES["upload_dir"])
        batch_dir = upload_dir / batch_id
        
        if not batch_dir.exists():
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡ä¸å­˜åœ¨'}), 404
        
        batch_info_file = batch_dir / ".batch_info.json"
        if not batch_info_file.exists():
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡å…ƒæ•°æ®ä¸å­˜åœ¨'}), 404
        
        # è¯»å–å¹¶æ›´æ–°å…ƒæ•°æ®
        with open(batch_info_file, 'r', encoding='utf-8') as f:
            import json
            from datetime import datetime
            batch_info = json.load(f)
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆä¸Šä¼ åˆ°çŸ¥è¯†åº“
        if not batch_info.get('status', {}).get('uploaded_to_kb', False):
            return jsonify({'success': False, 'error': 'è¯¥æ‰¹æ¬¡å°šæœªå®ŒæˆçŸ¥è¯†åº“ä¸Šä¼ '}), 400
        
        # æ›´æ–°çŸ¥è¯†åº“åç§°
        batch_info['kb_name'] = kb_name
        batch_info['kb_labeled_at'] = datetime.now().isoformat()
        
        # ä¿å­˜
        with open(batch_info_file, 'w', encoding='utf-8') as f:
            json.dump(batch_info, f, ensure_ascii=False, indent=2)
        
        log_activity(f"æ‰¹æ¬¡ {batch_id} æ ‡è®°çŸ¥è¯†åº“: {kb_name}")
        
        return jsonify({'success': True, 'batch_info': batch_info})
    except Exception as e:
        log_activity(f"Failed to update knowledge base label: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/batches/<path:batch_id>', methods=['DELETE'])
def delete_batch(batch_id):
    """åˆ é™¤æ•´ä¸ªæ‰¹æ¬¡"""
    try:
        from urllib.parse import unquote
        import shutil
        import json
        
        # URL è§£ç æ‰¹æ¬¡ID
        batch_id = unquote(batch_id)
        
        # åˆ é™¤ä¸Šä¼ ç›®å½•ä¸­çš„æ‰¹æ¬¡
        upload_dir = Path(DIRECTORIES["upload_dir"])
        batch_dir = upload_dir / batch_id
        if batch_dir.exists():
            shutil.rmtree(batch_dir)
        
        # åˆ é™¤å¤„ç†ç›®å½•ä¸­çš„æ‰¹æ¬¡
        processed_dir = Path(DIRECTORIES["processed_dir"])
        processed_batch_dir = processed_dir / batch_id
        if processed_batch_dir.exists():
            shutil.rmtree(processed_batch_dir)
        
        # åˆ é™¤æœ€ç»ˆè¾“å‡ºç›®å½•ä¸­çš„æ‰¹æ¬¡
        final_dir = Path(DIRECTORIES["final_output_dir"])
        final_batch_dir = final_dir / batch_id
        if final_batch_dir.exists():
            shutil.rmtree(final_batch_dir)
        
        # æ¸…ç†å…¨å±€å·²å¤„ç†é‚®ä»¶è®°å½•
        global_file = Path("eml_process/.global_processed_emails.json")
        if global_file.exists():
            try:
                with open(global_file, 'r', encoding='utf-8') as f:
                    global_processed = json.load(f)
                
                # åˆ é™¤è¯¥æ‰¹æ¬¡çš„æ‰€æœ‰è®°å½•
                emails_to_remove = [
                    email_name for email_name, info in global_processed.items()
                    if info.get('batch_id') == batch_id
                ]
                
                for email_name in emails_to_remove:
                    del global_processed[email_name]
                
                # ä¿å­˜æ›´æ–°åçš„å…¨å±€æ–‡ä»¶
                with open(global_file, 'w', encoding='utf-8') as f:
                    json.dump(global_processed, f, ensure_ascii=False, indent=2)
                
                if emails_to_remove:
                    log_activity(f"ä»å…¨å±€è®°å½•ä¸­åˆ é™¤ {len(emails_to_remove)} ä¸ªé‚®ä»¶è®°å½•")
            except Exception as e:
                log_activity(f"Failed to clean global email records: {str(e)}")
        
        log_activity(f"åˆ é™¤æ‰¹æ¬¡: {batch_id}")
        
        return jsonify({'success': True, 'message': f'æ‰¹æ¬¡ {batch_id} å·²åˆ é™¤'})
    except Exception as e:
        log_activity(f"Failed to delete batch: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/batches/<path:batch_id>/reset', methods=['POST'])
def reset_batch(batch_id):
    """é‡ç½®æ‰¹æ¬¡çŠ¶æ€ï¼šé‡ç½®å¤„ç†çŠ¶æ€å¹¶æ¸…ç†å…¨å±€è®°å½•"""
    try:
        from urllib.parse import unquote
        import shutil
        import json
        
        # URL è§£ç æ‰¹æ¬¡ID
        batch_id = unquote(batch_id)
        
        upload_dir = Path(DIRECTORIES["upload_dir"])
        batch_dir = upload_dir / batch_id
        
        if not batch_dir.exists():
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡ä¸å­˜åœ¨'}), 404
        
        # åˆ é™¤å¤„ç†ç›®å½•ä¸­çš„æ‰¹æ¬¡
        processed_dir = Path(DIRECTORIES["processed_dir"])
        processed_batch_dir = processed_dir / batch_id
        if processed_batch_dir.exists():
            file_count = len(list(processed_batch_dir.glob("*.md")))
            shutil.rmtree(processed_batch_dir)
            log_activity(f"Deleted processed directory for batch {batch_id} ({file_count} files)")
        else:
            log_activity(f"Processed directory for batch {batch_id} does not exist, skipping")
        
        # åˆ é™¤æœ€ç»ˆè¾“å‡ºç›®å½•ä¸­çš„æ‰¹æ¬¡
        final_dir = Path(DIRECTORIES["final_output_dir"])
        final_batch_dir = final_dir / batch_id
        if final_batch_dir.exists():
            file_count = len(list(final_batch_dir.glob("*.md")))
            shutil.rmtree(final_batch_dir)
            log_activity(f"Deleted final_output directory for batch {batch_id} ({file_count} files)")
        else:
            log_activity(f"Final_output directory for batch {batch_id} does not exist, skipping")
        
        # æ¸…ç†å…¨å±€å·²å¤„ç†é‚®ä»¶è®°å½•ä¸­è¯¥æ‰¹æ¬¡çš„è®°å½•
        global_file = Path("eml_process/.global_processed_emails.json")
        if global_file.exists():
            try:
                with open(global_file, 'r', encoding='utf-8') as f:
                    global_processed = json.load(f)
                
                # åˆ é™¤è¯¥æ‰¹æ¬¡çš„æ‰€æœ‰è®°å½•
                emails_to_remove = [
                    email_name for email_name, info in global_processed.items()
                    if info.get('batch_id') == batch_id
                ]
                
                for email_name in emails_to_remove:
                    del global_processed[email_name]
                
                # ä¿å­˜æ›´æ–°åçš„å…¨å±€æ–‡ä»¶
                with open(global_file, 'w', encoding='utf-8') as f:
                    json.dump(global_processed, f, ensure_ascii=False, indent=2)
                
                if emails_to_remove:
                    log_activity(f"Removed {len(emails_to_remove)} email records from global tracking for batch {batch_id}")
                else:
                    log_activity(f"No email records found in global tracking for batch {batch_id}")
            except Exception as e:
                log_activity(f"Failed to clean global email records: {str(e)}")
        
        # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€ï¼ˆå®Œå…¨é‡ç½®ï¼Œä¸ä¿ç•™çŸ¥è¯†åº“æ ‡ç­¾ï¼‰
        batch_info_file = batch_dir / ".batch_info.json"
        if batch_info_file.exists():
            with open(batch_info_file, 'r', encoding='utf-8') as f:
                batch_info = json.load(f)
            
            # é‡ç½®çŠ¶æ€ï¼ŒåŒæ—¶æ¸…é™¤çŸ¥è¯†åº“æ ‡ç­¾
            batch_info['status'] = {
                'uploaded': True,
                'cleaned': False,
                'llm_processed': False,
                'uploaded_to_kb': False
            }
            
            # æ¸…é™¤çŸ¥è¯†åº“æ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'kb_name' in batch_info:
                del batch_info['kb_name']
            
            # ä¿å­˜
            with open(batch_info_file, 'w', encoding='utf-8') as f:
                json.dump(batch_info, f, ensure_ascii=False, indent=2)
        
        log_activity(f"Batch {batch_id} status reset to 'uploaded'")
        
        return jsonify({
            'success': True, 
            'message': f'æ‰¹æ¬¡ {batch_id} å·²é‡ç½®ï¼Œå¯é‡æ–°å¤„ç†'
        })
    except Exception as e:
        log_activity(f"Failed to reset batch status: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


if __name__ == '__main__':
    import time
    print("=" * 60)
    print("Email Processing API Server - Starting...")
    print("=" * 60)
    print("Server Address: http://localhost:5001")
    print("Frontend Address: http://localhost:3000")
    print("=" * 60)
    app.run(host='0.0.0.0', port=5001, debug=True)

