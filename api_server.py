#!/usr/bin/env python3
"""
Flask APIæœåŠ¡å™¨
ä¸ºå‰ç«¯Reactåº”ç”¨æä¾›APIæ¥å£ï¼Œå¯¹æ¥ç°æœ‰çš„toolsæ¨¡å—
"""

from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from werkzeug.utils import secure_filename
import os
from pathlib import Path
import glob
from dotenv import dotenv_values
import traceback
import json
import time
from datetime import datetime
import random
import string
import shutil

# å¯¼å…¥ç°æœ‰çš„å·¥å…·æ¨¡å—
from tools.utils import count_files, log_activity
from tools.email_processing.email_cleaner import EmailCleaner
# from tools.data_cleaning import clean_email_files  # åŒ…å«streamlitä¾èµ–ï¼Œä¸å¯¼å…¥
# from tools.llm_processing import process_with_llm  # åŒ…å«streamlitä¾èµ–ï¼Œä¸å¯¼å…¥
from tools.api_clients.knowledge_base_api import KnowledgeBaseAPI
from tools.api_clients.gptbots_api import GPTBotsAPI
from config import DIRECTORIES, init_directories

app = Flask(__name__)
# é…ç½®CORS - å…è®¸æ‰€æœ‰æ¥æºè®¿é—®æ‰€æœ‰è·¯ç”±
CORS(app, 
     resources={r"/*": {"origins": "*"}},
     allow_headers=["Content-Type", "Authorization"],
     methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
     supports_credentials=False)

# é…ç½®ä¸Šä¼ å¤§å°é™åˆ¶
max_content_mb = int(os.getenv('MAX_CONTENT_LENGTH', '2000'))
app.config['MAX_CONTENT_LENGTH'] = max_content_mb * 1024 * 1024  # è½¬æ¢ä¸ºå­—èŠ‚

# åˆå§‹åŒ–ç›®å½•
init_directories()

# é…ç½®ä¸Šä¼ 
UPLOAD_FOLDER = DIRECTORIES["upload_dir"]
ALLOWED_EXTENSIONS = {'eml'}

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def get_disk_usage():
    """è·å–å½“å‰ç£ç›˜ä½¿ç”¨æƒ…å†µ"""
    try:
        import psutil
        # è·å–å½“å‰å·¥ä½œç›®å½•æ‰€åœ¨çš„ç£ç›˜åˆ†åŒº
        current_path = os.getcwd()
        disk_usage = psutil.disk_usage(current_path)
        
        total_gb = disk_usage.total / (1024 ** 3)
        used_gb = disk_usage.used / (1024 ** 3)
        free_gb = disk_usage.free / (1024 ** 3)
        percent = disk_usage.percent
        
        return {
            'total_gb': round(total_gb, 2),
            'used_gb': round(used_gb, 2),
            'free_gb': round(free_gb, 2),
            'percent': percent,
            'status': 'warning' if percent > 85 else 'critical' if percent > 95 else 'normal'
        }
    except ImportError:
        # psutilæœªå®‰è£…ï¼Œä½¿ç”¨shutilä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
        try:
            import shutil
            current_path = os.getcwd()
            disk_stat = shutil.disk_usage(current_path)
            
            total_gb = disk_stat.total / (1024 ** 3)
            used_gb = disk_stat.used / (1024 ** 3)
            free_gb = disk_stat.free / (1024 ** 3)
            percent = (disk_stat.used / disk_stat.total) * 100
            
            return {
                'total_gb': round(total_gb, 2),
                'used_gb': round(used_gb, 2),
                'free_gb': round(free_gb, 2),
                'percent': round(percent, 2),
                'status': 'warning' if percent > 85 else 'critical' if percent > 95 else 'normal'
            }
        except Exception as e:
            return {
                'error': str(e),
                'status': 'unknown'
            }
    except Exception as e:
        return {
            'error': str(e),
            'status': 'unknown'
        }

def log_disk_usage(prefix=""):
    """è®°å½•ç£ç›˜ä½¿ç”¨æƒ…å†µåˆ°æ—¥å¿—"""
    disk_info = get_disk_usage()
    if 'error' not in disk_info:
        status_emoji = "âš ï¸" if disk_info['status'] == 'warning' else "ğŸ”´" if disk_info['status'] == 'critical' else "ğŸ’¾"
        log_activity(
            f"{prefix}{status_emoji} ç£ç›˜ä½¿ç”¨: {disk_info['used_gb']:.2f}GB / {disk_info['total_gb']:.2f}GB "
            f"({disk_info['percent']:.1f}%) | å‰©ä½™: {disk_info['free_gb']:.2f}GB"
        )
        return disk_info
    else:
        log_activity(f"{prefix}âš ï¸ æ— æ³•è·å–ç£ç›˜ä½¿ç”¨æƒ…å†µ: {disk_info['error']}")
        return None


@app.route('/api/config/env', methods=['GET'])
def get_env_config():
    """è·å–ç¯å¢ƒå˜é‡é…ç½®"""
    try:
        # å°è¯•å¤šä¸ªå¯èƒ½çš„.envæ–‡ä»¶ä½ç½®
        possible_paths = [
            Path(__file__).parent / '.env',  # ä¸api_server.pyåŒç›®å½•
            Path.cwd() / '.env',  # å½“å‰å·¥ä½œç›®å½•
            Path(__file__).parent.parent / '.env',  # çˆ¶ç›®å½•
        ]
        
        env_path = None
        for path in possible_paths:
            log_activity(f"å°è¯•è¯»å–.envæ–‡ä»¶: {path.absolute()}")
            if path.exists():
                env_path = path
                log_activity(f"âœ… æ‰¾åˆ°.envæ–‡ä»¶: {path.absolute()}")
                break
        
        if not env_path:
            error_msg = f'.envæ–‡ä»¶ä¸å­˜åœ¨ã€‚å·²å°è¯•çš„è·¯å¾„:\n' + '\n'.join([f'  - {p.absolute()}' for p in possible_paths])
            log_activity(f"âŒ {error_msg}")
            return jsonify({
                'success': False, 
                'error': '.envæ–‡ä»¶ä¸å­˜åœ¨',
                'details': error_msg,
                'cwd': str(Path.cwd().absolute()),
                'script_dir': str(Path(__file__).parent.absolute())
            }), 404
        
        env_vars = dotenv_values(env_path)
        log_activity(f"æˆåŠŸè¯»å–.envæ–‡ä»¶ï¼Œå…±{len(env_vars)}ä¸ªå˜é‡")
        
        # æå–LLM API Keys
        llm_keys = []
        for i in range(1, 4):
            key = env_vars.get(f'GPTBOTS_LLM_API_KEY_{i}')
            if key:
                llm_keys.append({
                    'id': f'llm_key_{i}',
                    'name': f'LLM API Key {i}',
                    'value': key,
                    'masked': key[:8] + '...' + key[-4:] if len(key) > 12 else key
                })
        
        # æå–çŸ¥è¯†åº“ API Keys
        kb_keys = []
        for i in range(1, 4):
            key = env_vars.get(f'GPTBOTS_KB_API_KEY_{i}')
            if key:
                kb_keys.append({
                    'id': f'kb_key_{i}',
                    'name': f'çŸ¥è¯†åº“ API Key {i}',
                    'value': key,
                    'masked': key[:8] + '...' + key[-4:] if len(key) > 12 else key
                })
        
        config = {
            'llm_api_keys': llm_keys,
            'kb_api_keys': kb_keys,
            'default_llm_key': env_vars.get('GPTBOTS_LLM_API_KEY_1', ''),
            'default_kb_key': env_vars.get('GPTBOTS_KB_API_KEY_1', ''),
        }
        
        log_activity(f"è¿”å›é…ç½®: {len(llm_keys)}ä¸ªLLM Keys, {len(kb_keys)}ä¸ªKB Keys")
        return jsonify({'success': True, 'config': config})
    except Exception as e:
        error_msg = f'è¯»å–ç¯å¢ƒé…ç½®å¼‚å¸¸: {str(e)}'
        log_activity(error_msg)
        return jsonify({
            'success': False, 
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500


@app.route('/api/knowledge-bases', methods=['POST'])
def fetch_knowledge_bases():
    """è·å–çŸ¥è¯†åº“åˆ—è¡¨"""
    try:
        data = request.json
        api_key = data.get('api_key')
        
        if not api_key:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘API Key'}), 400
        
        kb_client = KnowledgeBaseAPI(api_key)
        knowledge_bases = kb_client.list_knowledge_bases()
        
        if knowledge_bases:
            return jsonify({
                'success': True,
                'knowledge_bases': knowledge_bases
            })
        else:
            # è¿”å›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
            return jsonify({
                'success': False, 
                'error': 'æ— æ³•è·å–çŸ¥è¯†åº“åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥API Keyæ˜¯å¦æ­£ç¡®æˆ–æ˜¯å¦æœ‰æƒé™è®¿é—®çŸ¥è¯†åº“'
            }), 200  # æ”¹ä¸º200çŠ¶æ€ç ï¼Œè®©å‰ç«¯èƒ½æ­£ç¡®å¤„ç†
            
    except Exception as e:
        log_activity(f"è·å–çŸ¥è¯†åº“åˆ—è¡¨å¼‚å¸¸: {str(e)}")
        return jsonify({
            'success': False, 
            'error': f'è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {str(e)}'
        }), 200  # æ”¹ä¸º200çŠ¶æ€ç 


@app.route('/api/stats', methods=['GET'])
def get_stats():
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    try:
        # çŸ¥è¯†åº“æ–‡æ¡£æ•° = LLMå¤„ç†å®Œæˆçš„æ–‡ä»¶æ•°ï¼ˆè¿™äº›æ–‡ä»¶åº”è¯¥å·²ä¸Šä¼ åˆ°çŸ¥è¯†åº“ï¼‰
        llm_processed_count = count_files(DIRECTORIES["final_output_dir"], "*.md")
        
        stats = {
            'uploaded': count_files(DIRECTORIES["upload_dir"], "*.eml"),
            'cleaned': count_files(DIRECTORIES["processed_dir"], "*.md"),
            'processed': llm_processed_count,
            'inKnowledgeBase': llm_processed_count  # å·²LLMå¤„ç†çš„æ–‡ä»¶å³ä¸ºä¸Šä¼ åˆ°çŸ¥è¯†åº“çš„æ–‡ä»¶
        }
        return jsonify({'success': True, 'stats': stats})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/delete-uploaded/<path:filename>', methods=['DELETE'])
def delete_uploaded_file(filename):
    """åˆ é™¤å·²ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆæ”¯æŒæ‰¹æ¬¡è·¯å¾„ï¼‰"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ–‡ä»¶å
        filename = unquote(filename)
        
        file_path = Path(UPLOAD_FOLDER) / filename
        if file_path.exists():
            file_path.unlink()
            log_activity(f"åˆ é™¤ä¸Šä¼ æ–‡ä»¶: {filename}")
            return jsonify({'success': True, 'message': 'æ–‡ä»¶åˆ é™¤æˆåŠŸ'})
        else:
            log_activity(f"æ–‡ä»¶ä¸å­˜åœ¨: {filename}, è·¯å¾„: {file_path}")
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
    except Exception as e:
        log_activity(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/delete-processed/<path:filename>', methods=['DELETE'])
def delete_processed_file(filename):
    """åˆ é™¤å·²å»é‡çš„æ–‡ä»¶ï¼ˆæ”¯æŒæ‰¹æ¬¡è·¯å¾„ï¼‰"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ–‡ä»¶å
        filename = unquote(filename)
        
        processed_dir = Path(DIRECTORIES["processed_dir"])
        file_path = processed_dir / filename
        if file_path.exists():
            file_path.unlink()
            log_activity(f"åˆ é™¤å»é‡æ–‡ä»¶: {filename}")
            return jsonify({'success': True, 'message': 'æ–‡ä»¶åˆ é™¤æˆåŠŸ'})
        else:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
    except Exception as e:
        log_activity(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/delete-llm-processed/<path:filename>', methods=['DELETE'])
def delete_llm_processed_file(filename):
    """åˆ é™¤LLMå¤„ç†çš„æ–‡ä»¶ï¼ˆæ”¯æŒæ‰¹æ¬¡è·¯å¾„ï¼‰"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ–‡ä»¶å
        filename = unquote(filename)
        
        final_dir = Path(DIRECTORIES["final_output_dir"])
        file_path = final_dir / filename
        if file_path.exists():
            file_path.unlink()
            log_activity(f"åˆ é™¤LLMå¤„ç†æ–‡ä»¶: {filename}")
            return jsonify({'success': True, 'message': 'æ–‡ä»¶åˆ é™¤æˆåŠŸ'})
        else:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
    except Exception as e:
        log_activity(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/check-duplicates', methods=['GET'])
def check_duplicates():
    """æ£€æŸ¥å“ªäº›æ–‡ä»¶æ˜¯é‡å¤çš„ï¼ˆæ£€æŸ¥æ‰€æœ‰å·²ä¸Šä¼ çš„é‚®ä»¶ï¼‰"""
    try:
        filenames_str = request.args.get('filenames', '')
        if not filenames_str:
            return jsonify({'success': True, 'duplicates': []})
        
        filenames = filenames_str.split(',')
        
        # æ”¶é›†æ‰€æœ‰æ‰¹æ¬¡ä¸­å·²ä¸Šä¼ çš„é‚®ä»¶æ–‡ä»¶å
        upload_dir = Path(DIRECTORIES["upload_dir"])
        existing_files = set()
        
        # æ£€æŸ¥æ‰€æœ‰æ‰¹æ¬¡ç›®å½•
        if upload_dir.exists():
            for batch_dir in upload_dir.iterdir():
                if batch_dir.is_dir() and batch_dir.name.startswith('batch_'):
                    # æ”¶é›†è¯¥æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰.emlæ–‡ä»¶
                    for eml_file in batch_dir.glob('*.eml'):
                        existing_files.add(eml_file.name)
        
        # æ‰¾å‡ºé‡å¤çš„æ–‡ä»¶åï¼ˆå·²åœ¨å…¶ä»–æ‰¹æ¬¡ä¸­ä¸Šä¼ è¿‡ï¼‰
        duplicates = [fname for fname in filenames if fname in existing_files]
        
        return jsonify({
            'success': True,
            'duplicates': duplicates
        })
    except Exception as e:
        log_activity(f"æ£€æŸ¥é‡å¤æ–‡ä»¶å¼‚å¸¸: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/upload', methods=['POST'])
def upload_files():
    """ä¸Šä¼ EMLæ–‡ä»¶ - è‡ªåŠ¨æ‰¹æ¬¡ç®¡ç†"""
    try:
        if 'files' not in request.files:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ–‡ä»¶'}), 400
        
        files = request.files.getlist('files')
        
        # ç”Ÿæˆæ‰¹æ¬¡ID (ç²¾ç¡®åˆ°ç§’ + éšæœºåç¼€ï¼Œç¡®ä¿å”¯ä¸€æ€§)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        random_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=4))
        batch_id = f"batch_{timestamp}_{random_suffix}"
        
        # è·å–å¿…å¡«çš„æ‰¹æ¬¡æ ‡ç­¾
        batch_label = request.form.get('label', '').strip()
        if not batch_label:
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡æ ‡ç­¾ä¸ºå¿…å¡«é¡¹'}), 400
        
        # æ£€æŸ¥æ‰€æœ‰æ‰¹æ¬¡ä¸­å·²ä¸Šä¼ çš„æ–‡ä»¶
        upload_dir = Path(UPLOAD_FOLDER)
        existing_files = {}  # {filename: batch_id}
        
        if upload_dir.exists():
            for existing_batch_dir in upload_dir.iterdir():
                if existing_batch_dir.is_dir() and existing_batch_dir.name.startswith('batch_'):
                    # æ”¶é›†æ‰€æœ‰å·²ä¸Šä¼ çš„.emlæ–‡ä»¶
                    for eml_file in existing_batch_dir.glob('*.eml'):
                        existing_files[eml_file.name] = existing_batch_dir.name
        
        # åˆ›å»ºæ‰¹æ¬¡ç›®å½•
        batch_dir = Path(UPLOAD_FOLDER) / batch_id
        batch_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ–‡ä»¶å¹¶æ£€æµ‹é‡å¤
        uploaded_files = []
        file_details = []
        duplicate_files = []  # è®°å½•é‡å¤çš„æ–‡ä»¶
        
        for file in files:
            if file and allowed_file(file.filename):
                filename = secure_filename(file.filename)
                
                # æ£€æŸ¥æ˜¯å¦åœ¨å…¶ä»–æ‰¹æ¬¡ä¸­å·²å­˜åœ¨
                if filename in existing_files:
                    previous_batch = existing_files[filename]
                    duplicate_files.append({
                        'filename': filename,
                        'previous_batch': previous_batch,
                        'previous_time': 'N/A'
                    })
                    log_activity(f"âš ï¸ è·³è¿‡é‡å¤æ–‡ä»¶: {filename} (å·²åœ¨æ‰¹æ¬¡ {previous_batch} ä¸­ä¸Šä¼ )")
                    continue  # è·³è¿‡è¿™ä¸ªæ–‡ä»¶ï¼Œä¸ä¿å­˜
                
                # ä¿å­˜æ–‡ä»¶
                filepath = batch_dir / filename
                file.save(str(filepath))
                uploaded_files.append(filename)
                
                file_details.append({
                    "filename": filename,
                    "size": filepath.stat().st_size,
                    "upload_time": datetime.now().isoformat()
                })
                log_activity(f"ä¸Šä¼ æ–‡ä»¶åˆ°æ‰¹æ¬¡ {batch_id}: {filename}")
        
        # åˆ›å»ºæ‰¹æ¬¡å…ƒæ•°æ®
        batch_info = {
            "batch_id": batch_id,
            "upload_time": datetime.now().isoformat(),
            "file_count": len(uploaded_files),
            "custom_label": batch_label,
            "status": {
                "uploaded": True,
                "cleaned": False,
                "llm_processed": False,
                "uploaded_to_kb": False
            },
            "files": file_details,
            "processing_history": {}
        }
        
        # ä¿å­˜æ‰¹æ¬¡å…ƒæ•°æ®
        # batch_id ä¿æŒç®€å•ç¨³å®šï¼Œä¸åŒ…å«æ ‡ç­¾å’Œæ–‡ä»¶æ•°
        # æ ‡ç­¾å’Œæ–‡ä»¶æ•°ä»…å­˜å‚¨åœ¨å…ƒæ•°æ®ä¸­ï¼Œç”¨äºå‰ç«¯å±•ç¤º
        batch_info_file = batch_dir / ".batch_info.json"
        with open(batch_info_file, 'w', encoding='utf-8') as f:
            json.dump(batch_info, f, ensure_ascii=False, indent=2)
        
        # å³ä½¿å…¨éƒ¨é‡å¤ä¹Ÿä¸æŠ¥é”™ï¼Œè¿”å›æˆåŠŸï¼ˆä½†countä¸º0ï¼‰
        if len(uploaded_files) == 0 and len(duplicate_files) > 0:
            log_activity(f"æ‰€æœ‰æ–‡ä»¶éƒ½æ˜¯é‡å¤çš„ï¼Œæ‰¹æ¬¡ {batch_id} å·²åˆ›å»ºä½†æ— æ–°æ–‡ä»¶")
        
        log_activity(f"æ‰¹æ¬¡åˆ›å»ºå®Œæˆ: {batch_id}, {len(uploaded_files)} ä¸ªæ–‡ä»¶ä¸Šä¼ æˆåŠŸ")
        if duplicate_files:
            log_activity(f"è·³è¿‡ {len(duplicate_files)} ä¸ªé‡å¤æ–‡ä»¶")
        
        return jsonify({
            'success': True,
            'batch_id': batch_id,
            'uploaded_files': uploaded_files,
            'count': len(uploaded_files),
            'duplicate_files': duplicate_files,
            'duplicate_count': len(duplicate_files),
            'message': f'æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶' + (f'ï¼Œè·³è¿‡ {len(duplicate_files)} ä¸ªé‡å¤æ–‡ä»¶' if duplicate_files else '')
        })
    except Exception as e:
        log_activity(f"ä¸Šä¼ å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/uploaded-files', methods=['GET'])
def get_uploaded_files():
    """è·å–å·²ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆæ”¯æŒæ‰¹æ¬¡æ¨¡å¼ï¼‰"""
    try:
        upload_dir = Path(DIRECTORIES["upload_dir"])
        files = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ‰¹æ¬¡æ–‡ä»¶å¤¹
        batch_dirs = [d for d in upload_dir.iterdir() if d.is_dir()]
        
        if batch_dirs:
            # æ‰¹æ¬¡æ¨¡å¼ï¼šä»æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶å¤¹ä¸­æ”¶é›†æ–‡ä»¶
            for batch_dir in batch_dirs:
                batch_files = [f"{batch_dir.name}/{f.name}" for f in batch_dir.glob("*.eml")]
                files.extend(batch_files)
        else:
            # éæ‰¹æ¬¡æ¨¡å¼ï¼šç›´æ¥ä»ä¸Šä¼ ç›®å½•è·å–
            files = [f.name for f in upload_dir.glob("*.eml")]
        
        return jsonify({'success': True, 'files': files})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/processed-files', methods=['GET'])
def get_processed_files():
    """è·å–å·²å»é‡çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆæ”¯æŒæ‰¹æ¬¡æ¨¡å¼ï¼‰"""
    try:
        processed_dir = Path(DIRECTORIES["processed_dir"])
        files = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ‰¹æ¬¡æ–‡ä»¶å¤¹
        batch_dirs = [d for d in processed_dir.iterdir() if d.is_dir()]
        
        if batch_dirs:
            # æ‰¹æ¬¡æ¨¡å¼ï¼šä»æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶å¤¹ä¸­æ”¶é›†æ–‡ä»¶
            for batch_dir in batch_dirs:
                batch_files = [f"{batch_dir.name}/{f.name}" for f in batch_dir.glob("*.md")]
                files.extend(batch_files)
        else:
            # éæ‰¹æ¬¡æ¨¡å¼ï¼šç›´æ¥ä»å¤„ç†ç›®å½•è·å–
            files = [f.name for f in processed_dir.glob("*.md")]
        
        return jsonify({'success': True, 'files': files})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/llm-processed-files', methods=['GET'])
def get_llm_processed_files():
    """è·å–LLMå¤„ç†çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆæ”¯æŒæ‰¹æ¬¡æ¨¡å¼ï¼‰"""
    try:
        final_dir = Path(DIRECTORIES["final_output_dir"])
        files = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ‰¹æ¬¡æ–‡ä»¶å¤¹
        batch_dirs = [d for d in final_dir.iterdir() if d.is_dir()]
        
        if batch_dirs:
            # æ‰¹æ¬¡æ¨¡å¼ï¼šä»æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶å¤¹ä¸­æ”¶é›†æ–‡ä»¶
            for batch_dir in batch_dirs:
                batch_files = [f"{batch_dir.name}/{f.name}" for f in batch_dir.glob("*.md")]
                files.extend(batch_files)
        else:
            # éæ‰¹æ¬¡æ¨¡å¼ï¼šç›´æ¥ä»æœ€ç»ˆè¾“å‡ºç›®å½•è·å–
            files = [f.name for f in final_dir.glob("*.md")]
        
        return jsonify({'success': True, 'files': files})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/file-content/uploaded/<path:filename>', methods=['GET'])
def get_uploaded_file_content(filename):
    """è·å–å·²ä¸Šä¼ æ–‡ä»¶çš„å†…å®¹ï¼ˆæ”¯æŒæ‰¹æ¬¡è·¯å¾„ï¼‰"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ–‡ä»¶å
        filename = unquote(filename)
        
        upload_dir = Path(DIRECTORIES["upload_dir"])
        file_path = upload_dir / filename
        
        if not file_path.exists():
            log_activity(f"æ–‡ä»¶ä¸å­˜åœ¨: {filename}, è·¯å¾„: {file_path}")
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        # è¯»å–EMLæ–‡ä»¶å†…å®¹
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        return jsonify({'success': True, 'content': content})
    except Exception as e:
        log_activity(f"è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/file-content/processed/<path:filename>', methods=['GET'])
def get_processed_file_content(filename):
    """è·å–å·²å»é‡æ–‡ä»¶çš„å†…å®¹ï¼ˆæ”¯æŒæ‰¹æ¬¡è·¯å¾„ï¼‰"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ–‡ä»¶å
        filename = unquote(filename)
        
        processed_dir = Path(DIRECTORIES["processed_dir"])
        file_path = processed_dir / filename
        
        if not file_path.exists():
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        return jsonify({'success': True, 'content': content})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/file-content/llm-processed/<path:filename>', methods=['GET'])
def get_llm_processed_file_content(filename):
    """è·å–LLMå¤„ç†æ–‡ä»¶çš„å†…å®¹ï¼ˆæ”¯æŒæ‰¹æ¬¡è·¯å¾„ï¼‰"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ–‡ä»¶å
        filename = unquote(filename)
        
        final_dir = Path(DIRECTORIES["final_output_dir"])
        file_path = final_dir / filename
        
        if not file_path.exists():
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        return jsonify({'success': True, 'content': content})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/clean', methods=['POST'])
def clean_files():
    """æ¸…æ´—é‚®ä»¶æ–‡ä»¶"""
    try:
        data = request.json
        files = data.get('files', [])
        
        if not files:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦æ¸…æ´—'}), 400
        
        # ä½¿ç”¨EmailCleaneræ‰¹é‡å¤„ç†
        cleaner = EmailCleaner(
            input_dir=DIRECTORIES["upload_dir"],
            output_dir=DIRECTORIES["processed_dir"]
        )
        
        # æ‰§è¡Œæ¸…æ´—
        result = cleaner.process_all_emails()
        
        if result.get("success"):
            processed_count = len(result.get("generated_files", []))
            log_activity(f"æ‰¹é‡æ¸…æ´—å®Œæˆ: {processed_count} ä¸ªæ–‡ä»¶")
            return jsonify({
                'success': True,
                'processed_files': [Path(f).name for f in result.get("generated_files", [])],
                'count': processed_count,
                'report': result.get("report")
            })
        else:
            return jsonify({
                'success': False,
                'error': result.get("message", "æ¸…æ´—å¤±è´¥")
            }), 500
            
    except Exception as e:
        log_activity(f"æ¸…æ´—è¿‡ç¨‹å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/cleaned-files', methods=['GET'])
def get_cleaned_files():
    """è·å–å·²æ¸…æ´—çš„æ–‡ä»¶åˆ—è¡¨"""
    try:
        processed_dir = Path(DIRECTORIES["processed_dir"])
        files = [f.name for f in processed_dir.glob("*.md")]
        return jsonify({'success': True, 'files': files})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/llm-process', methods=['POST'])
def llm_process():
    """LLMå¤„ç†æ–‡ä»¶"""
    try:
        data = request.json
        files = data.get('files', [])
        api_key = data.get('api_key')
        endpoint = data.get('endpoint', 'internal')
        delay = data.get('delay', 2)
        
        if not files or not api_key:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
        api_client = GPTBotsAPI(api_key)
        
        processed_files = []
        errors = []
        
        for filename in files:
            input_path = os.path.join(DIRECTORIES["processed_dir"], filename)
            output_path = os.path.join(DIRECTORIES["final_output_dir"], filename)
            
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(input_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # åˆ›å»ºå¯¹è¯
                conversation_id = api_client.create_conversation()
                if not conversation_id:
                    raise Exception("åˆ›å»ºå¯¹è¯å¤±è´¥")
                
                # å‘é€å†…å®¹åˆ°LLMå¤„ç†
                result = api_client.send_message(conversation_id, content)
                
                if result and result.get('answer'):
                    # ä¿å­˜å¤„ç†åçš„å†…å®¹
                    processed_content = result['answer']
                    with open(output_path, 'w', encoding='utf-8') as f:
                        f.write(processed_content)
                    
                    processed_files.append(filename)
                    log_activity(f"LLMå¤„ç†æ–‡ä»¶: {filename}")
                else:
                    raise Exception("LLMæœªè¿”å›æœ‰æ•ˆå“åº”")
                
                # å»¶è¿Ÿé¿å…é™æµ
                import time
                time.sleep(delay)
                
            except Exception as e:
                error_msg = f"å¤„ç†å¤±è´¥: {filename} - {str(e)}"
                errors.append(error_msg)
                log_activity(f"LLM{error_msg}")
        
        return jsonify({
            'success': len(processed_files) > 0,
            'processed_files': processed_files,
            'count': len(processed_files),
            'errors': errors
        })
    except Exception as e:
        log_activity(f"LLMå¤„ç†è¿‡ç¨‹å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/results', methods=['GET'])
def get_results():
    """è·å–æ‰€æœ‰å¤„ç†ç»“æœ"""
    try:
        results = []
        
        # è·å–æ‰€æœ‰é˜¶æ®µçš„æ–‡ä»¶
        upload_dir = Path(DIRECTORIES["upload_dir"])
        processed_dir = Path(DIRECTORIES["processed_dir"])
        final_dir = Path(DIRECTORIES["final_output_dir"])
        
        # LLMå¤„ç†å®Œæˆçš„æ–‡ä»¶
        for f in final_dir.glob("*.md"):
            results.append({
                'filename': f.name,
                'stage': 'llm_processed',
                'size': f'{f.stat().st_size / 1024:.2f} KB',
                'processed_time': None
            })
        
        # åªæ¸…æ´—çš„æ–‡ä»¶
        for f in processed_dir.glob("*.md"):
            if not (final_dir / f.name).exists():
                results.append({
                    'filename': f.name,
                    'stage': 'cleaned',
                    'size': f'{f.stat().st_size / 1024:.2f} KB',
                    'processed_time': None
                })
        
        return jsonify({'success': True, 'results': results})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/file-content', methods=['GET'])
def get_file_content():
    """è·å–æ–‡ä»¶å†…å®¹"""
    try:
        filename = request.args.get('file')
        if not filename:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘æ–‡ä»¶å'}), 400
        
        # å°è¯•ä»å„ä¸ªç›®å½•æŸ¥æ‰¾æ–‡ä»¶
        for dir_path in [DIRECTORIES["final_output_dir"], DIRECTORIES["processed_dir"], DIRECTORIES["upload_dir"]]:
            filepath = Path(dir_path) / filename
            if filepath.exists():
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                return jsonify({'success': True, 'content': content})
        
        return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/download', methods=['GET'])
def download_file():
    """ä¸‹è½½æ–‡ä»¶"""
    try:
        filename = request.args.get('file')
        if not filename:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘æ–‡ä»¶å'}), 400
        
        # å°è¯•ä»å„ä¸ªç›®å½•æŸ¥æ‰¾æ–‡ä»¶
        for dir_path in [DIRECTORIES["final_output_dir"], DIRECTORIES["processed_dir"], DIRECTORIES["upload_dir"]]:
            filepath = Path(dir_path) / filename
            if filepath.exists():
                return send_file(filepath, as_attachment=True)
        
        return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/knowledge-bases', methods=['POST'])
def get_knowledge_bases():
    """è·å–çŸ¥è¯†åº“åˆ—è¡¨"""
    try:
        data = request.json
        api_key = data.get('api_key')
        
        if not api_key:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘API Key'}), 400
        
        kb_api = KnowledgeBaseAPI(api_key)
        response = kb_api.get_knowledge_bases()
        
        if response and 'data' in response and 'list' in response['data']:
            knowledge_bases = response['data']['list']
            return jsonify({
                'success': True,
                'knowledge_bases': knowledge_bases
            })
        else:
            return jsonify({'success': False, 'error': 'è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥'}), 500
            
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/upload-to-kb', methods=['POST'])
def upload_to_knowledge_base():
    """ä¸Šä¼ æ–‡ä»¶åˆ°çŸ¥è¯†åº“"""
    try:
        data = request.json
        files = data.get('files', [])
        api_key = data.get('api_key')
        knowledge_base_id = data.get('knowledge_base_id', '')
        chunk_token = data.get('chunk_token', 600)
        
        if not files or not api_key:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        kb_api = KnowledgeBaseAPI(api_key)
        uploaded_count = 0
        
        for filename in files:
            filepath = os.path.join(DIRECTORIES["final_output_dir"], filename)
            
            if not os.path.exists(filepath):
                log_activity(f"æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
                continue
            
            try:
                # ä¸Šä¼ åˆ°çŸ¥è¯†åº“
                result = kb_api.upload_file(
                    filepath,
                    knowledge_base_id=knowledge_base_id,
                    chunk_token=chunk_token
                )
                
                if result:
                    uploaded_count += 1
                    log_activity(f"ä¸Šä¼ åˆ°çŸ¥è¯†åº“: {filename}")
                    
            except Exception as e:
                log_activity(f"ä¸Šä¼ çŸ¥è¯†åº“å¤±è´¥: {filename} - {str(e)}")
        
        return jsonify({
            'success': True,
            'uploaded_count': uploaded_count
        })
    except Exception as e:
        log_activity(f"çŸ¥è¯†åº“ä¸Šä¼ è¿‡ç¨‹å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/qa-chat', methods=['POST'])
def qa_chat():
    """é—®ç­”èŠå¤©"""
    try:
        data = request.json
        message = data.get('message')
        api_key = data.get('api_key')
        conversation_id = data.get('conversation_id', '')
        
        if not message or not api_key:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        # åˆå§‹åŒ–GPTBots API
        api_client = GPTBotsAPI(api_key)
        
        # å¦‚æœæ²¡æœ‰conversation_idï¼Œå…ˆåˆ›å»º
        if not conversation_id:
            conversation_id = api_client.create_conversation()
            if not conversation_id:
                return jsonify({'success': False, 'error': 'åˆ›å»ºå¯¹è¯å¤±è´¥'}), 500
        
        # å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤
        response = api_client.send_message(conversation_id, message)
        
        if response and 'answer' in response:
            return jsonify({
                'success': True,
                'reply': response['answer'],
                'conversation_id': conversation_id
            })
        else:
            return jsonify({'success': False, 'error': 'APIå“åº”å¼‚å¸¸'}), 500
            
    except Exception as e:
        log_activity(f"QAèŠå¤©å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({'status': 'ok'})


# è¾…åŠ©å‡½æ•°ï¼šæ›´æ–°æ‰¹æ¬¡çŠ¶æ€
def update_batch_status_file(batch_id: str, status_key: str, status_value: bool = True):
    """æ›´æ–°æ‰¹æ¬¡çŠ¶æ€åˆ°å…ƒæ•°æ®æ–‡ä»¶"""
    try:
        from datetime import datetime
        import json
        
        upload_dir = Path(DIRECTORIES["upload_dir"])
        batch_dir = upload_dir / batch_id
        
        if not batch_dir.exists():
            log_activity(f"è­¦å‘Š: æ‰¹æ¬¡ç›®å½•ä¸å­˜åœ¨: {batch_id}")
            return False
        
        batch_info_file = batch_dir / ".batch_info.json"
        if not batch_info_file.exists():
            log_activity(f"è­¦å‘Š: æ‰¹æ¬¡å…ƒæ•°æ®ä¸å­˜åœ¨: {batch_id}")
            return False
        
        # è¯»å–å¹¶æ›´æ–°å…ƒæ•°æ®
        with open(batch_info_file, 'r', encoding='utf-8') as f:
            batch_info = json.load(f)
        
        # æ›´æ–°çŠ¶æ€
        if 'status' not in batch_info:
            batch_info['status'] = {}
        batch_info['status'][status_key] = status_value
        
        # è®°å½•å¤„ç†æ—¶é—´
        if 'processing_history' not in batch_info:
            batch_info['processing_history'] = {}
        if status_value:
            batch_info['processing_history'][f"{status_key}_at"] = datetime.now().isoformat()
        
        # ä¿å­˜
        with open(batch_info_file, 'w', encoding='utf-8') as f:
            json.dump(batch_info, f, ensure_ascii=False, indent=2)
        
        log_activity(f"æ‰¹æ¬¡ {batch_id} çŠ¶æ€å·²æ›´æ–°: {status_key} = {status_value}")
        return True
    except Exception as e:
        log_activity(f"æ›´æ–°æ‰¹æ¬¡çŠ¶æ€å¤±è´¥ {batch_id}: {str(e)}")
        return False


# å…¨è‡ªåŠ¨å¤„ç†æµç¨‹API
@app.route('/api/auto/clean', methods=['POST'])
def auto_clean():
    """å…¨è‡ªåŠ¨æµç¨‹ - æ­¥éª¤1: é‚®ä»¶æ¸…æ´—"""
    try:
        data = request.json
        batch_ids = data.get('batch_ids', [])
        skip_if_exists = data.get('skip_if_exists', True)  # é»˜è®¤å¯ç”¨æ™ºèƒ½è·³è¿‡
        
        upload_dir = Path(DIRECTORIES["upload_dir"])
        processed_dir = Path(DIRECTORIES["processed_dir"])
        
        # æ™ºèƒ½è·³è¿‡ï¼šæ£€æŸ¥æ˜¯å¦å·²æœ‰å¤„ç†åçš„æ–‡ä»¶
        if skip_if_exists and batch_ids:
            skipped_batches = []
            batches_to_process = []
            
            for batch_id in batch_ids:
                processed_batch_dir = processed_dir / batch_id
                if processed_batch_dir.exists():
                    md_files = list(processed_batch_dir.glob("*.md"))
                    if md_files:
                        log_activity(f"æ‰¹æ¬¡ {batch_id} å·²æœ‰ {len(md_files)} ä¸ªå¤„ç†åçš„æ–‡ä»¶ï¼Œè·³è¿‡æ¸…æ´—æ­¥éª¤")
                        skipped_batches.append(batch_id)
                        continue  # åªæœ‰åœ¨æœ‰MDæ–‡ä»¶æ—¶æ‰è·³è¿‡
                # ç›®å½•ä¸å­˜åœ¨ï¼Œæˆ–è€…å­˜åœ¨ä½†æ²¡æœ‰MDæ–‡ä»¶ï¼Œéƒ½åº”è¯¥å¤„ç†
                batches_to_process.append(batch_id)
            
            if not batches_to_process:
                log_activity(f"æ‰€æœ‰æ‰¹æ¬¡éƒ½å·²å®Œæˆæ¸…æ´—ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
                return jsonify({
                    'success': True,
                    'processed_count': 0,
                    'total_files': 0,
                    'duplicates': 0,
                    'skipped': True,
                    'skipped_batches': skipped_batches,
                    'message': 'æ‰€æœ‰æ‰¹æ¬¡éƒ½å·²å®Œæˆæ¸…æ´—ï¼Œå·²è·³è¿‡'
                })
            
            batch_ids = batches_to_process
            log_activity(f"å°†å¤„ç† {len(batch_ids)} ä¸ªæ‰¹æ¬¡ï¼Œè·³è¿‡ {len(skipped_batches)} ä¸ªå·²å¤„ç†æ‰¹æ¬¡")
        
        if batch_ids:
            # æ‰¹æ¬¡æ¨¡å¼ï¼šåªå¤„ç†é€‰ä¸­çš„æ‰¹æ¬¡
            log_activity(f"å¼€å§‹é‚®ä»¶æ¸…æ´— (é€‰ä¸­ {len(batch_ids)} ä¸ªæ‰¹æ¬¡): {batch_ids}")
            log_disk_usage("[æ¸…æ´—å‰] ")
            cleaner = EmailCleaner(input_dir=str(upload_dir), output_dir=str(processed_dir), batch_mode=True)
            result = cleaner.process_all_emails(selected_batches=batch_ids)
        else:
            # éæ‰¹æ¬¡æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰æ–‡ä»¶
            log_activity(f"å¼€å§‹é‚®ä»¶æ¸…æ´—: {upload_dir}")
            log_disk_usage("[æ¸…æ´—å‰] ")
            cleaner = EmailCleaner(input_dir=str(upload_dir), output_dir=str(processed_dir))
            result = cleaner.process_all_emails()
        
        if result.get('success'):
            # ä»reportä¸­è·å–å¤„ç†æ•°é‡
            report = result.get('report', {})
            processed_count = report.get('unique_emails', len(result.get('generated_files', [])))
            log_activity(f"é‚®ä»¶æ¸…æ´—å®Œæˆ: {processed_count} ä¸ªæ–‡ä»¶")
            log_disk_usage("[æ¸…æ´—å] ")
            
            # è®°å½•å…¨å±€å»é‡ä¿¡æ¯
            global_duplicates = report.get('all_global_duplicates', [])
            if global_duplicates:
                log_activity(f"æ£€æµ‹åˆ° {len(global_duplicates)} ä¸ªè·¨æ‰¹æ¬¡é‡å¤é‚®ä»¶ï¼Œå·²è‡ªåŠ¨è·³è¿‡")
                for dup in global_duplicates:
                    log_activity(f"  - {dup['file_name']} (å·²åœ¨æ‰¹æ¬¡ {dup['previous_batch']} ä¸­å¤„ç†)")
            
            return jsonify({
                'success': True,
                'processed_count': processed_count,
                'total_files': report.get('total_input_files', 0),
                'duplicates': report.get('duplicate_emails', 0),
                'global_duplicates': global_duplicates,
                'message': 'é‚®ä»¶æ¸…æ´—å®Œæˆ'
            })
        else:
            error_msg = result.get('message', 'é‚®ä»¶æ¸…æ´—å¤±è´¥')
            log_activity(f"é‚®ä»¶æ¸…æ´—å¤±è´¥: {error_msg}")
            return jsonify({'success': False, 'error': error_msg})
            
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        log_activity(f"é‚®ä»¶æ¸…æ´—å¼‚å¸¸: {str(e)}")
        log_activity(f"é”™è¯¯å †æ ˆ: {error_trace}")
        print(f"é‚®ä»¶æ¸…æ´—å¼‚å¸¸: {str(e)}")
        print(f"é”™è¯¯å †æ ˆ:\n{error_trace}")
        return jsonify({'success': False, 'error': str(e), 'traceback': error_trace}), 500


@app.route('/api/auto/llm-process', methods=['POST'])
def auto_llm_process():
    """å…¨è‡ªåŠ¨æµç¨‹ - æ­¥éª¤2: LLMå¤„ç†"""
    try:
        data = request.json
        api_key = data.get('api_key')
        delay = data.get('delay', 2)
        batch_ids = data.get('batch_ids', [])
        skip_if_exists = data.get('skip_if_exists', True)  # é»˜è®¤å¯ç”¨æ™ºèƒ½è·³è¿‡
        
        if not api_key:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘API Key'}), 400
        
        processed_dir = Path(DIRECTORIES["processed_dir"])
        final_dir = Path(DIRECTORIES["final_output_dir"])
        final_dir.mkdir(parents=True, exist_ok=True)
        
        # æ™ºèƒ½è·³è¿‡ï¼šæ£€æŸ¥æ˜¯å¦å·²æœ‰LLMå¤„ç†åçš„æ–‡ä»¶ï¼ˆæ¯”è¾ƒprocessedå’Œfinal_outputçš„æ–‡ä»¶æ•°ï¼‰
        if skip_if_exists and batch_ids:
            skipped_batches = []
            batches_to_process = []
            
            for batch_id in batch_ids:
                final_batch_dir = final_dir / batch_id
                processed_batch_dir = processed_dir / batch_id
                
                if final_batch_dir.exists() and processed_batch_dir.exists():
                    # ç»Ÿè®¡ä¸¤ä¸ªç›®å½•çš„æ–‡ä»¶æ•°
                    llm_files = list(final_batch_dir.glob("*.md"))
                    processed_files = [f for f in processed_batch_dir.glob("*.md") if f.name != "processing_report.json"]
                    
                    # åªæœ‰å½“final_outputçš„æ–‡ä»¶æ•°ç­‰äºprocessedçš„æ–‡ä»¶æ•°æ—¶ï¼Œæ‰è®¤ä¸ºå·²å®Œæˆ
                    if llm_files and len(llm_files) >= len(processed_files):
                        log_activity(f"æ‰¹æ¬¡ {batch_id} å·²å®ŒæˆLLMå¤„ç†: {len(llm_files)}/{len(processed_files)} ä¸ªæ–‡ä»¶ï¼Œè·³è¿‡LLMå¤„ç†æ­¥éª¤")
                        skipped_batches.append(batch_id)
                        # æ›´æ–°çŠ¶æ€ï¼ˆå¦‚æœå°šæœªæ›´æ–°ï¼‰
                        update_batch_status_file(batch_id, 'llm_processed', True)
                        continue
                    elif llm_files:
                        log_activity(f"æ‰¹æ¬¡ {batch_id} LLMå¤„ç†æœªå®Œæˆ: å·²å¤„ç† {len(llm_files)}/{len(processed_files)} ä¸ªæ–‡ä»¶ï¼Œç»§ç»­å¤„ç†å‰©ä½™æ–‡ä»¶")
                
                batches_to_process.append(batch_id)
            
            if not batches_to_process:
                log_activity(f"æ‰€æœ‰æ‰¹æ¬¡éƒ½å·²å®ŒæˆLLMå¤„ç†ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
                # æ›´æ–°æ‰€æœ‰è·³è¿‡æ‰¹æ¬¡çš„çŠ¶æ€
                for batch_id in skipped_batches:
                    update_batch_status_file(batch_id, 'llm_processed', True)
                
                return jsonify({
                    'success': True,
                    'processed_count': 0,
                    'failed_count': 0,
                    'total_files_after_dedup': 0,
                    'skipped': True,
                    'skipped_batches': skipped_batches,
                    'message': 'æ‰€æœ‰æ‰¹æ¬¡éƒ½å·²å®ŒæˆLLMå¤„ç†ï¼Œå·²è·³è¿‡'
                })
            
            batch_ids = batches_to_process
            log_activity(f"å°†å¤„ç† {len(batch_ids)} ä¸ªæ‰¹æ¬¡ï¼Œè·³è¿‡ {len(skipped_batches)} ä¸ªå·²å¤„ç†æ‰¹æ¬¡")
        
        # è·å–æ‰€æœ‰markdownæ–‡ä»¶ï¼ˆæ”¯æŒæ‰¹æ¬¡æ¨¡å¼ï¼‰
        md_files = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ‰¹æ¬¡æ–‡ä»¶å¤¹
        batch_dirs = [d for d in processed_dir.iterdir() if d.is_dir()]
        
        if batch_dirs:
            # æ‰¹æ¬¡æ¨¡å¼ï¼šä»æŒ‡å®šæ‰¹æ¬¡æ–‡ä»¶å¤¹ä¸­æ”¶é›†æ–‡ä»¶
            if batch_ids:
                # åªå¤„ç†é€‰ä¸­çš„æ‰¹æ¬¡
                log_activity(f"LLMå¤„ç†: é€‰ä¸­ {len(batch_ids)} ä¸ªæ‰¹æ¬¡")
                for batch_id in batch_ids:
                    batch_dir = processed_dir / batch_id
                    if batch_dir.exists() and batch_dir.is_dir():
                        batch_md_files = list(batch_dir.glob("*.md"))
                        batch_md_files = [f for f in batch_md_files if f.name != "processing_report.json"]
                        md_files.extend(batch_md_files)
                        log_activity(f"æ‰¹æ¬¡ {batch_id}: æ‰¾åˆ° {len(batch_md_files)} ä¸ªæ–‡ä»¶")
            else:
                # å¤„ç†æ‰€æœ‰æ‰¹æ¬¡
                for batch_dir in batch_dirs:
                    batch_md_files = list(batch_dir.glob("*.md"))
                    batch_md_files = [f for f in batch_md_files if f.name != "processing_report.json"]
                    md_files.extend(batch_md_files)
        else:
            # éæ‰¹æ¬¡æ¨¡å¼ï¼šç›´æ¥ä»å¤„ç†ç›®å½•è·å–
            md_files = list(processed_dir.glob("*.md"))
            md_files = [f for f in md_files if f.name != "processing_report.json"]
        
        if not md_files:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ‰¾åˆ°å¾…å¤„ç†çš„æ–‡ä»¶'}), 404
        
        # è®°å½•å»é‡åçš„å®é™…æ–‡ä»¶æ•°
        total_files_after_dedup = len(md_files)
        log_activity(f"LLMå¤„ç†: å»é‡åå…± {total_files_after_dedup} ä¸ªæ–‡ä»¶å¾…å¤„ç†")
        log_disk_usage("[LLMå¤„ç†å‰] ")
        
        # åˆå§‹åŒ–GPTBots APIå®¢æˆ·ç«¯
        client = GPTBotsAPI(api_key)
        processed_count = 0
        failed_count = 0
        
        # LLMæç¤ºè¯æ¨¡æ¿
        llm_prompt_template = """ä»¥ä¸‹æ˜¯éœ€è¦å¤„ç†çš„é‚®ä»¶å†…å®¹ï¼Œè¯·å¸®æˆ‘æ•´ç†å’Œä¼˜åŒ–ï¼š

{email_content}"""
        
        for md_file in md_files:
            try:
                # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
                if md_file.parent != processed_dir:
                    # æ‰¹æ¬¡æ¨¡å¼ï¼šåœ¨final_dirä¸­åˆ›å»ºå¯¹åº”çš„æ‰¹æ¬¡ç›®å½•
                    batch_name = md_file.parent.name
                    batch_final_dir = final_dir / batch_name
                    batch_final_dir.mkdir(parents=True, exist_ok=True)
                    output_file = batch_final_dir / md_file.name
                else:
                    # éæ‰¹æ¬¡æ¨¡å¼ï¼šç›´æ¥ä¿å­˜åˆ°final_dir
                    output_file = final_dir / md_file.name
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å¤„ç†è¿‡ï¼ˆè·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶ï¼‰
                if output_file.exists():
                    log_activity(f"æ–‡ä»¶å·²å¤„ç†ï¼Œè·³è¿‡: {md_file.name}")
                    processed_count += 1  # è®¡å…¥å·²å¤„ç†æ•°
                    continue
                
                log_activity(f"å¤„ç†æ–‡ä»¶: {md_file.name}")
                
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # åˆ›å»ºå¯¹è¯
                conversation_id = client.create_conversation()
                if not conversation_id:
                    log_activity(f"åˆ›å»ºå¯¹è¯å¤±è´¥: {md_file.name}")
                    failed_count += 1
                    continue
                
                # å‘é€æ¶ˆæ¯
                prompt = llm_prompt_template.format(email_content=content)
                response = client.send_message(conversation_id, prompt)
                
                if response and "output" in response:
                    # æå–LLMå¤„ç†ç»“æœ
                    output_list = response.get("output", [])
                    processed_content = ""
                    
                    for output_item in output_list:
                        if "content" in output_item:
                            content_obj = output_item["content"]
                            if "text" in content_obj:
                                processed_content += content_obj["text"] + "\n"
                    
                    if processed_content.strip():
                        # ä¿å­˜å¤„ç†ç»“æœ
                        with open(output_file, 'w', encoding='utf-8') as f:
                            f.write(processed_content.strip())
                        
                        processed_count += 1
                        log_activity(f"æˆåŠŸå¤„ç†: {md_file.name}")
                    else:
                        log_activity(f"LLMè¿”å›ç©ºå†…å®¹: {md_file.name}")
                        failed_count += 1
                else:
                    log_activity(f"LLMè°ƒç”¨å¤±è´¥: {md_file.name}")
                    failed_count += 1
                
                # å»¶è¿Ÿé¿å…APIé™æµ
                time.sleep(delay)
                
            except Exception as e:
                log_activity(f"å¤„ç†æ–‡ä»¶å¼‚å¸¸ {md_file.name}: {str(e)}")
                failed_count += 1
                continue
        
        # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€
        if batch_ids and processed_count > 0:
            for batch_id in batch_ids:
                update_batch_status_file(batch_id, 'llm_processed', True)
        
        log_activity(f"LLMå¤„ç†å®Œæˆ: æˆåŠŸ {processed_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª")
        log_disk_usage("[LLMå¤„ç†å] ")
        
        if processed_count > 0:
            return jsonify({
                'success': True,
                'processed_count': processed_count,
                'failed_count': failed_count,
                'total_files_after_dedup': total_files_after_dedup,
                'message': f'LLMå¤„ç†å®Œæˆ: æˆåŠŸ {processed_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ªï¼ˆå»é‡åå…± {total_files_after_dedup} ä¸ªæ–‡ä»¶ï¼‰'
            })
        else:
            error_msg = f'LLMå¤„ç†å¤±è´¥: æ‰€æœ‰æ–‡ä»¶å¤„ç†å¤±è´¥ã€‚è¯·æ£€æŸ¥API Keyæ˜¯å¦æ­£ç¡®ï¼Œæ˜¯å¦æœ‰å¯¹è¯æƒé™'
            log_activity(error_msg)
            return jsonify({
                'success': False,
                'processed_count': 0,
                'failed_count': failed_count,
                'error': error_msg
            })
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        log_activity(f"LLMå¤„ç†å¼‚å¸¸: {str(e)}")
        log_activity(f"é”™è¯¯å †æ ˆ: {error_trace}")
        print(f"LLMå¤„ç†å¼‚å¸¸: {str(e)}")
        print(f"é”™è¯¯å †æ ˆ:\n{error_trace}")
        return jsonify({'success': False, 'error': str(e), 'traceback': error_trace}), 500


@app.route('/api/auto/upload-kb', methods=['POST'])
def auto_upload_kb():
    """å…¨è‡ªåŠ¨æµç¨‹ - æ­¥éª¤3: ä¸Šä¼ åˆ°çŸ¥è¯†åº“"""
    try:
        data = request.json
        api_key = data.get('api_key')
        kb_id = data.get('knowledge_base_id')
        chunk_token = data.get('chunk_token')
        chunk_separator = data.get('chunk_separator')
        batch_size = data.get('batch_size', 10)
        skip_if_exists = data.get('skip_if_exists', True)  # é»˜è®¤å¯ç”¨æ™ºèƒ½è·³è¿‡
        
        if not api_key or not kb_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…éœ€å‚æ•°'}), 400
        
        # å¿…é¡»æä¾›chunk_tokenæˆ–chunk_separatorä¹‹ä¸€
        if not chunk_token and not chunk_separator:
            return jsonify({'success': False, 'error': 'å¿…é¡»æä¾›chunk_tokenæˆ–chunk_separator'}), 400
        
        batch_ids = data.get('batch_ids', [])
        
        # æ™ºèƒ½è·³è¿‡ï¼šæ£€æŸ¥æ‰¹æ¬¡çŠ¶æ€
        if skip_if_exists and batch_ids:
            import json
            skipped_batches = []
            batches_to_process = []
            upload_dir = Path(DIRECTORIES["upload_dir"])
            
            for batch_id in batch_ids:
                batch_dir = upload_dir / batch_id
                batch_info_file = batch_dir / ".batch_info.json"
                
                if batch_info_file.exists():
                    try:
                        with open(batch_info_file, 'r', encoding='utf-8') as f:
                            batch_info = json.load(f)
                        
                        if batch_info.get('status', {}).get('uploaded_to_kb', False):
                            log_activity(f"æ‰¹æ¬¡ {batch_id} å·²å®ŒæˆçŸ¥è¯†åº“ä¸Šä¼ ï¼Œè·³è¿‡ä¸Šä¼ æ­¥éª¤")
                            skipped_batches.append(batch_id)
                            continue
                    except Exception as e:
                        log_activity(f"è¯»å–æ‰¹æ¬¡å…ƒæ•°æ®å¤±è´¥ {batch_id}: {str(e)}")
                
                batches_to_process.append(batch_id)
            
            if not batches_to_process:
                log_activity(f"æ‰€æœ‰æ‰¹æ¬¡éƒ½å·²å®ŒæˆçŸ¥è¯†åº“ä¸Šä¼ ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
                return jsonify({
                    'success': True,
                    'uploaded_count': 0,
                    'skipped': True,
                    'skipped_batches': skipped_batches,
                    'message': 'æ‰€æœ‰æ‰¹æ¬¡éƒ½å·²å®ŒæˆçŸ¥è¯†åº“ä¸Šä¼ ï¼Œå·²è·³è¿‡'
                })
            
            batch_ids = batches_to_process
            log_activity(f"å°†ä¸Šä¼  {len(batch_ids)} ä¸ªæ‰¹æ¬¡ï¼Œè·³è¿‡ {len(skipped_batches)} ä¸ªå·²ä¸Šä¼ æ‰¹æ¬¡")
        
        final_dir = Path(DIRECTORIES["final_output_dir"])
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ‰¹æ¬¡æ–‡ä»¶å¤¹
        batch_dirs = [d for d in final_dir.iterdir() if d.is_dir()]
        
        if batch_dirs:
            # æ‰¹æ¬¡æ¨¡å¼ï¼šæ”¶é›†æŒ‡å®šæ‰¹æ¬¡ä¸­çš„æ–‡ä»¶
            md_files = []
            if batch_ids:
                # åªå¤„ç†é€‰ä¸­çš„æ‰¹æ¬¡
                log_activity(f"çŸ¥è¯†åº“ä¸Šä¼ : é€‰ä¸­ {len(batch_ids)} ä¸ªæ‰¹æ¬¡")
                for batch_id in batch_ids:
                    batch_dir = final_dir / batch_id
                    if batch_dir.exists() and batch_dir.is_dir():
                        batch_md_files = list(batch_dir.glob("*.md"))
                        md_files.extend(batch_md_files)
                        log_activity(f"æ‰¹æ¬¡ {batch_id}: æ‰¾åˆ° {len(batch_md_files)} ä¸ªæ–‡ä»¶")
            else:
                # å¤„ç†æ‰€æœ‰æ‰¹æ¬¡
                for batch_dir in batch_dirs:
                    batch_md_files = list(batch_dir.glob("*.md"))
                    md_files.extend(batch_md_files)
        else:
            # éæ‰¹æ¬¡æ¨¡å¼ï¼šç›´æ¥ä»final_dirè·å–
            md_files = list(final_dir.glob("*.md"))
        
        if not md_files:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ‰¾åˆ°å¾…ä¸Šä¼ çš„æ–‡ä»¶'}), 404
        
        log_activity(f"å¼€å§‹ä¸Šä¼  {len(md_files)} ä¸ªæ–‡ä»¶åˆ°çŸ¥è¯†åº“")
        log_disk_usage("[ä¸Šä¼ å‰] ")
        
        kb_client = KnowledgeBaseAPI(api_key)
        
        # æ‰¹æ¬¡æ¨¡å¼éœ€è¦é€ä¸ªä¸Šä¼ æ–‡ä»¶ï¼Œéæ‰¹æ¬¡æ¨¡å¼å¯ä»¥ä½¿ç”¨ç›®å½•ä¸Šä¼ 
        if batch_dirs:
            # æ‰¹æ¬¡æ¨¡å¼ï¼šé€ä¸ªæ–‡ä»¶ä¸Šä¼ 
            successful_uploads = 0
            failed_uploads = 0
            
            for md_file in md_files:
                try:
                    log_activity(f"ä¸Šä¼ æ–‡ä»¶åˆ°çŸ¥è¯†åº“: {md_file.name}")
                    
                    # è¯»å–æ–‡ä»¶å†…å®¹
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # æ„å»ºä¸Šä¼ å‚æ•°
                    upload_params = {
                        'knowledge_base_id': kb_id,
                        'filename': md_file.name,  # ä¿®æ­£å‚æ•°å
                        'content': content
                    }
                    
                    if chunk_token:
                        upload_params['chunk_token'] = chunk_token
                        upload_params['splitter'] = "PARAGRAPH"
                    else:
                        # upload_markdown_content ä¸æ”¯æŒ chunk_separatorï¼Œä½¿ç”¨é»˜è®¤çš„ chunk_token
                        upload_params['chunk_token'] = 600
                        upload_params['splitter'] = "PARAGRAPH"
                        log_activity(f"æ³¨æ„: æ‰¹æ¬¡æ¨¡å¼æš‚ä¸æ”¯æŒè‡ªå®šä¹‰åˆ†éš”ç¬¦ï¼Œä½¿ç”¨é»˜è®¤Tokenåˆ†å—(600)")
                    
                    # ä¸Šä¼ å•ä¸ªæ–‡ä»¶
                    result = kb_client.upload_markdown_content(**upload_params)
                    
                    if result and 'error' not in result:
                        successful_uploads += 1
                        log_activity(f"æˆåŠŸä¸Šä¼ : {md_file.name}")
                    else:
                        failed_uploads += 1
                        log_activity(f"ä¸Šä¼ å¤±è´¥: {md_file.name}, é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    
                    # å»¶è¿Ÿé¿å…APIé™æµ
                    time.sleep(0.5)
                    
                except Exception as e:
                    failed_uploads += 1
                    log_activity(f"ä¸Šä¼ å¼‚å¸¸ {md_file.name}: {str(e)}")
            
            # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€ - æ ‡è®°ä¸ºå·²ä¸Šä¼ å¹¶æ·»åŠ çŸ¥è¯†åº“åç§°æ ‡ç­¾
            if batch_ids and successful_uploads > 0:
                # è·å–çŸ¥è¯†åº“åç§°
                kb_name = None
                try:
                    log_activity(f"æ­£åœ¨è·å–çŸ¥è¯†åº“åç§°ï¼ŒKB ID: {kb_id}")
                    kb_response = kb_client.get_knowledge_bases()
                    log_activity(f"çŸ¥è¯†åº“APIå“åº”: {kb_response}")
                    
                    # å°è¯•å¤šç§å¯èƒ½çš„å“åº”ç»“æ„
                    kb_list = None
                    if kb_response:
                        # æ ¼å¼1: {'data': {'list': [...]}}
                        if 'data' in kb_response and isinstance(kb_response['data'], dict) and 'list' in kb_response['data']:
                            kb_list = kb_response['data']['list']
                        # æ ¼å¼2: {'data': [...]}
                        elif 'data' in kb_response and isinstance(kb_response['data'], list):
                            kb_list = kb_response['data']
                        # æ ¼å¼3: {'knowledge_base': [...]}
                        elif 'knowledge_base' in kb_response and isinstance(kb_response['knowledge_base'], list):
                            kb_list = kb_response['knowledge_base']
                        # æ ¼å¼4: ç›´æ¥æ˜¯åˆ—è¡¨
                        elif isinstance(kb_response, list):
                            kb_list = kb_response
                    
                    if kb_list:
                        log_activity(f"æ‰¾åˆ° {len(kb_list)} ä¸ªçŸ¥è¯†åº“")
                        for kb in kb_list:
                            if kb.get('id') == kb_id:
                                kb_name = kb.get('name', '')
                                log_activity(f"åŒ¹é…åˆ°çŸ¥è¯†åº“: {kb_name}")
                                break
                    else:
                        log_activity(f"æ— æ³•ä»å“åº”ä¸­è§£æçŸ¥è¯†åº“åˆ—è¡¨")
                        
                except Exception as e:
                    log_activity(f"è·å–çŸ¥è¯†åº“åç§°å¤±è´¥: {str(e)}")
                
                log_activity(f"çŸ¥è¯†åº“ä¸Šä¼ å®Œæˆ: æˆåŠŸ {successful_uploads} ä¸ªï¼Œå¤±è´¥ {failed_uploads} ä¸ª")
                log_disk_usage("[ä¸Šä¼ å] ")
                
                for batch_id in batch_ids:
                    update_batch_status_file(batch_id, 'uploaded_to_kb', True)
                    # å¦‚æœæˆåŠŸè·å–çŸ¥è¯†åº“åç§°ï¼Œä¿å­˜åˆ°æ‰¹æ¬¡å…ƒæ•°æ®
                    if kb_name:
                        upload_dir = Path(DIRECTORIES["upload_dir"])
                        batch_dir = upload_dir / batch_id
                        batch_info_file = batch_dir / ".batch_info.json"
                        
                        if batch_info_file.exists():
                            try:
                                with open(batch_info_file, 'r', encoding='utf-8') as f:
                                    batch_info = json.load(f)
                                
                                batch_info['kb_name'] = kb_name
                                
                                with open(batch_info_file, 'w', encoding='utf-8') as f:
                                    json.dump(batch_info, f, ensure_ascii=False, indent=2)
                                
                                log_activity(f"æ‰¹æ¬¡ {batch_id} å·²è‡ªåŠ¨æ ‡è®°çŸ¥è¯†åº“: {kb_name}")
                            except Exception as e:
                                log_activity(f"ä¿å­˜çŸ¥è¯†åº“åç§°åˆ°æ‰¹æ¬¡ {batch_id} å¤±è´¥: {str(e)}")
                    else:
                        log_activity(f"è­¦å‘Š: æœªèƒ½è·å–çŸ¥è¯†åº“åç§°ï¼Œæ‰¹æ¬¡ {batch_id} éœ€è¦æ‰‹åŠ¨æ·»åŠ æ ‡ç­¾")
            
            if successful_uploads > 0:
                return jsonify({
                    'success': True,
                    'uploaded_count': successful_uploads,
                    'message': f'çŸ¥è¯†åº“ä¸Šä¼ å®Œæˆ: æˆåŠŸ {successful_uploads} ä¸ªï¼Œå¤±è´¥ {failed_uploads} ä¸ª'
                })
            else:
                return jsonify({'success': False, 'error': 'æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ å¤±è´¥'}), 500
        
        # éæ‰¹æ¬¡æ¨¡å¼ï¼šä½¿ç”¨ç›®å½•ä¸Šä¼ 
        # æ ¹æ®åˆ†å—æ–¹å¼æ„å»ºå‚æ•°
        upload_params = {
            'directory_path': str(final_dir),
            'knowledge_base_id': kb_id,
            'batch_size': batch_size
        }
        
        if chunk_token:
            upload_params['chunk_token'] = chunk_token
            upload_params['splitter'] = "PARAGRAPH"
        else:
            upload_params['chunk_separator'] = chunk_separator
            upload_params['splitter'] = "CUSTOM"
        
        result = kb_client.upload_markdown_files_from_directory(**upload_params)
        
        if result and 'error' not in result:
            return jsonify({
                'success': True,
                'uploaded_count': result.get('successful_uploads', 0),
                'message': 'çŸ¥è¯†åº“ä¸Šä¼ å®Œæˆ'
            })
        else:
            return jsonify({'success': False, 'error': result.get('error', 'ä¸Šä¼ å¤±è´¥')}), 500
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


# ========== æ‰¹æ¬¡ç®¡ç† API ==========

@app.route('/api/batches', methods=['GET'])
def get_batches():
    """è·å–æ‰€æœ‰æ‰¹æ¬¡åˆ—è¡¨"""
    try:
        upload_dir = Path(DIRECTORIES["upload_dir"])
        import json
        from datetime import datetime
        
        batches = []
        for batch_dir in sorted(upload_dir.iterdir(), key=lambda x: x.name, reverse=True):
            if not batch_dir.is_dir():
                continue
            
            # è¯»å–æ‰¹æ¬¡å…ƒæ•°æ®
            batch_info_file = batch_dir / ".batch_info.json"
            if batch_info_file.exists():
                with open(batch_info_file, 'r', encoding='utf-8') as f:
                    batch_info = json.load(f)
            else:
                # å¦‚æœæ²¡æœ‰å…ƒæ•°æ®ï¼Œç”ŸæˆåŸºæœ¬ä¿¡æ¯ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
                eml_files = list(batch_dir.glob("*.eml"))
                batch_info = {
                    "batch_id": batch_dir.name,
                    "upload_time": datetime.fromtimestamp(batch_dir.stat().st_mtime).isoformat(),
                    "file_count": len(eml_files),
                    "custom_label": "",
                    "status": {
                        "uploaded": True,
                        "cleaned": False,
                        "llm_processed": False,
                        "uploaded_to_kb": False
                    }
                }
            
            batches.append(batch_info)
        
        return jsonify({
            'success': True,
            'batches': batches,
            'total': len(batches)
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/batches/<path:batch_id>', methods=['GET'])
def get_batch_details(batch_id):
    """è·å–ç‰¹å®šæ‰¹æ¬¡çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ‰¹æ¬¡ID
        batch_id = unquote(batch_id)
        
        upload_dir = Path(DIRECTORIES["upload_dir"])
        batch_dir = upload_dir / batch_id
        
        if not batch_dir.exists():
            log_activity(f"æ‰¹æ¬¡ä¸å­˜åœ¨: {batch_id}, è·¯å¾„: {batch_dir}")
            return jsonify({'success': False, 'error': f'æ‰¹æ¬¡ä¸å­˜åœ¨: {batch_id}'}), 404
        
        # è¯»å–æ‰¹æ¬¡å…ƒæ•°æ®
        batch_info_file = batch_dir / ".batch_info.json"
        if batch_info_file.exists():
            with open(batch_info_file, 'r', encoding='utf-8') as f:
                import json
                batch_info = json.load(f)
        else:
            log_activity(f"æ‰¹æ¬¡å…ƒæ•°æ®ä¸å­˜åœ¨: {batch_id}")
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡å…ƒæ•°æ®ä¸å­˜åœ¨'}), 404
        
        # è·å–æ–‡ä»¶åˆ—è¡¨
        files = [f.name for f in batch_dir.glob("*.eml")]
        batch_info['current_files'] = files
        
        return jsonify({
            'success': True,
            'batch': batch_info
        })
    except Exception as e:
        log_activity(f"è·å–æ‰¹æ¬¡è¯¦æƒ…å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/batches/<path:batch_id>/status', methods=['PUT'])
def update_batch_status(batch_id):
    """æ›´æ–°æ‰¹æ¬¡çŠ¶æ€"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ‰¹æ¬¡ID
        batch_id = unquote(batch_id)
        
        data = request.json
        status_key = data.get('status_key')  # cleaned, llm_processed, uploaded_to_kb
        status_value = data.get('status_value', True)
        
        upload_dir = Path(DIRECTORIES["upload_dir"])
        batch_dir = upload_dir / batch_id
        
        if not batch_dir.exists():
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡ä¸å­˜åœ¨'}), 404
        
        batch_info_file = batch_dir / ".batch_info.json"
        if not batch_info_file.exists():
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡å…ƒæ•°æ®ä¸å­˜åœ¨'}), 404
        
        # è¯»å–å¹¶æ›´æ–°å…ƒæ•°æ®
        with open(batch_info_file, 'r', encoding='utf-8') as f:
            import json
            from datetime import datetime
            batch_info = json.load(f)
        
        # æ›´æ–°çŠ¶æ€
        if 'status' not in batch_info:
            batch_info['status'] = {}
        batch_info['status'][status_key] = status_value
        
        # è®°å½•å¤„ç†æ—¶é—´
        if 'processing_history' not in batch_info:
            batch_info['processing_history'] = {}
        if status_value:
            batch_info['processing_history'][f"{status_key}_at"] = datetime.now().isoformat()
        
        # ä¿å­˜
        with open(batch_info_file, 'w', encoding='utf-8') as f:
            json.dump(batch_info, f, ensure_ascii=False, indent=2)
        
        return jsonify({'success': True, 'batch_info': batch_info})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/batches/<path:batch_id>/label', methods=['PUT'])
def update_batch_label(batch_id):
    """æ›´æ–°æ‰¹æ¬¡çš„è‡ªå®šä¹‰æ ‡ç­¾"""
    try:
        from urllib.parse import unquote
        import json
        
        # URL è§£ç æ‰¹æ¬¡ID
        batch_id = unquote(batch_id)
        
        data = request.json
        custom_label = data.get('custom_label', '').strip()
        
        if not custom_label:
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡æ ‡ç­¾ä¸èƒ½ä¸ºç©º'}), 400
        
        upload_dir = Path(DIRECTORIES["upload_dir"])
        batch_dir = upload_dir / batch_id
        
        if not batch_dir.exists():
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡ä¸å­˜åœ¨'}), 404
        
        batch_info_file = batch_dir / ".batch_info.json"
        if not batch_info_file.exists():
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡å…ƒæ•°æ®ä¸å­˜åœ¨'}), 404
        
        # è¯»å–å¹¶æ›´æ–°å…ƒæ•°æ®
        with open(batch_info_file, 'r', encoding='utf-8') as f:
            batch_info = json.load(f)
        
        # æ›´æ–°è‡ªå®šä¹‰æ ‡ç­¾
        batch_info['custom_label'] = custom_label
        
        # ä¿å­˜æ›´æ–°åçš„å…ƒæ•°æ®
        with open(batch_info_file, 'w', encoding='utf-8') as f:
            json.dump(batch_info, f, ensure_ascii=False, indent=2)
        
        log_activity(f"æ‰¹æ¬¡ {batch_id} æ ‡ç­¾å·²æ›´æ–°ä¸º: {custom_label}")
        
        return jsonify({
            'success': True,
            'message': 'æ‰¹æ¬¡æ ‡ç­¾æ›´æ–°æˆåŠŸ',
            'custom_label': custom_label
        })
        
    except Exception as e:
        log_activity(f"æ›´æ–°æ‰¹æ¬¡æ ‡ç­¾å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/batches/<path:batch_id>/kb-label', methods=['PUT'])
def update_batch_kb_label(batch_id):
    """æ›´æ–°æ‰¹æ¬¡çš„çŸ¥è¯†åº“æ ‡ç­¾"""
    try:
        from urllib.parse import unquote
        
        # URL è§£ç æ‰¹æ¬¡ID
        batch_id = unquote(batch_id)
        
        data = request.json
        kb_name = data.get('kb_name', '').strip()
        
        if not kb_name:
            return jsonify({'success': False, 'error': 'çŸ¥è¯†åº“åç§°ä¸èƒ½ä¸ºç©º'}), 400
        
        upload_dir = Path(DIRECTORIES["upload_dir"])
        batch_dir = upload_dir / batch_id
        
        if not batch_dir.exists():
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡ä¸å­˜åœ¨'}), 404
        
        batch_info_file = batch_dir / ".batch_info.json"
        if not batch_info_file.exists():
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡å…ƒæ•°æ®ä¸å­˜åœ¨'}), 404
        
        # è¯»å–å¹¶æ›´æ–°å…ƒæ•°æ®
        with open(batch_info_file, 'r', encoding='utf-8') as f:
            import json
            from datetime import datetime
            batch_info = json.load(f)
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆä¸Šä¼ åˆ°çŸ¥è¯†åº“
        if not batch_info.get('status', {}).get('uploaded_to_kb', False):
            return jsonify({'success': False, 'error': 'è¯¥æ‰¹æ¬¡å°šæœªå®ŒæˆçŸ¥è¯†åº“ä¸Šä¼ '}), 400
        
        # æ›´æ–°çŸ¥è¯†åº“åç§°
        batch_info['kb_name'] = kb_name
        batch_info['kb_labeled_at'] = datetime.now().isoformat()
        
        # ä¿å­˜
        with open(batch_info_file, 'w', encoding='utf-8') as f:
            json.dump(batch_info, f, ensure_ascii=False, indent=2)
        
        log_activity(f"æ‰¹æ¬¡ {batch_id} æ ‡è®°çŸ¥è¯†åº“: {kb_name}")
        
        return jsonify({'success': True, 'batch_info': batch_info})
    except Exception as e:
        log_activity(f"æ›´æ–°çŸ¥è¯†åº“æ ‡ç­¾å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/batches/<path:batch_id>', methods=['DELETE'])
def delete_batch(batch_id):
    """åˆ é™¤æ•´ä¸ªæ‰¹æ¬¡"""
    try:
        from urllib.parse import unquote
        import shutil
        import json
        
        # URL è§£ç æ‰¹æ¬¡ID
        batch_id = unquote(batch_id)
        
        # åˆ é™¤ä¸Šä¼ ç›®å½•ä¸­çš„æ‰¹æ¬¡
        upload_dir = Path(DIRECTORIES["upload_dir"])
        batch_dir = upload_dir / batch_id
        if batch_dir.exists():
            shutil.rmtree(batch_dir)
        
        # åˆ é™¤å¤„ç†ç›®å½•ä¸­çš„æ‰¹æ¬¡
        processed_dir = Path(DIRECTORIES["processed_dir"])
        processed_batch_dir = processed_dir / batch_id
        if processed_batch_dir.exists():
            shutil.rmtree(processed_batch_dir)
        
        # åˆ é™¤æœ€ç»ˆè¾“å‡ºç›®å½•ä¸­çš„æ‰¹æ¬¡
        final_dir = Path(DIRECTORIES["final_output_dir"])
        final_batch_dir = final_dir / batch_id
        if final_batch_dir.exists():
            shutil.rmtree(final_batch_dir)
        
        # æ¸…ç†å…¨å±€å·²å¤„ç†é‚®ä»¶è®°å½•
        global_file = Path("eml_process/.global_processed_emails.json")
        if global_file.exists():
            try:
                with open(global_file, 'r', encoding='utf-8') as f:
                    global_processed = json.load(f)
                
                # åˆ é™¤è¯¥æ‰¹æ¬¡çš„æ‰€æœ‰è®°å½•
                emails_to_remove = [
                    email_name for email_name, info in global_processed.items()
                    if info.get('batch_id') == batch_id
                ]
                
                for email_name in emails_to_remove:
                    del global_processed[email_name]
                
                # ä¿å­˜æ›´æ–°åçš„å…¨å±€æ–‡ä»¶
                with open(global_file, 'w', encoding='utf-8') as f:
                    json.dump(global_processed, f, ensure_ascii=False, indent=2)
                
                if emails_to_remove:
                    log_activity(f"ä»å…¨å±€è®°å½•ä¸­åˆ é™¤ {len(emails_to_remove)} ä¸ªé‚®ä»¶è®°å½•")
            except Exception as e:
                log_activity(f"æ¸…ç†å…¨å±€é‚®ä»¶è®°å½•å¤±è´¥: {str(e)}")
        
        log_activity(f"åˆ é™¤æ‰¹æ¬¡: {batch_id}")
        
        return jsonify({'success': True, 'message': f'æ‰¹æ¬¡ {batch_id} å·²åˆ é™¤'})
    except Exception as e:
        log_activity(f"åˆ é™¤æ‰¹æ¬¡å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/batches/<path:batch_id>/reset', methods=['POST'])
def reset_batch(batch_id):
    """é‡ç½®æ‰¹æ¬¡çŠ¶æ€ï¼šé‡ç½®å¤„ç†çŠ¶æ€å¹¶æ¸…ç†å…¨å±€è®°å½•"""
    try:
        from urllib.parse import unquote
        import shutil
        import json
        
        # URL è§£ç æ‰¹æ¬¡ID
        batch_id = unquote(batch_id)
        
        upload_dir = Path(DIRECTORIES["upload_dir"])
        batch_dir = upload_dir / batch_id
        
        if not batch_dir.exists():
            return jsonify({'success': False, 'error': 'æ‰¹æ¬¡ä¸å­˜åœ¨'}), 404
        
        # åˆ é™¤å¤„ç†ç›®å½•ä¸­çš„æ‰¹æ¬¡
        processed_dir = Path(DIRECTORIES["processed_dir"])
        processed_batch_dir = processed_dir / batch_id
        if processed_batch_dir.exists():
            shutil.rmtree(processed_batch_dir)
            log_activity(f"å·²åˆ é™¤æ‰¹æ¬¡ {batch_id} çš„processedç›®å½•")
        
        # åˆ é™¤æœ€ç»ˆè¾“å‡ºç›®å½•ä¸­çš„æ‰¹æ¬¡
        final_dir = Path(DIRECTORIES["final_output_dir"])
        final_batch_dir = final_dir / batch_id
        if final_batch_dir.exists():
            shutil.rmtree(final_batch_dir)
            log_activity(f"å·²åˆ é™¤æ‰¹æ¬¡ {batch_id} çš„final_outputç›®å½•")
        
        # æ¸…ç†å…¨å±€å·²å¤„ç†é‚®ä»¶è®°å½•ä¸­è¯¥æ‰¹æ¬¡çš„è®°å½•
        global_file = Path("eml_process/.global_processed_emails.json")
        if global_file.exists():
            try:
                with open(global_file, 'r', encoding='utf-8') as f:
                    global_processed = json.load(f)
                
                # åˆ é™¤è¯¥æ‰¹æ¬¡çš„æ‰€æœ‰è®°å½•
                emails_to_remove = [
                    email_name for email_name, info in global_processed.items()
                    if info.get('batch_id') == batch_id
                ]
                
                for email_name in emails_to_remove:
                    del global_processed[email_name]
                
                # ä¿å­˜æ›´æ–°åçš„å…¨å±€æ–‡ä»¶
                with open(global_file, 'w', encoding='utf-8') as f:
                    json.dump(global_processed, f, ensure_ascii=False, indent=2)
                
                if emails_to_remove:
                    log_activity(f"ä»å…¨å±€è®°å½•ä¸­åˆ é™¤ {len(emails_to_remove)} ä¸ªé‚®ä»¶è®°å½•")
            except Exception as e:
                log_activity(f"æ¸…ç†å…¨å±€é‚®ä»¶è®°å½•å¤±è´¥: {str(e)}")
        
        # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€ï¼ˆå®Œå…¨é‡ç½®ï¼Œä¸ä¿ç•™çŸ¥è¯†åº“æ ‡ç­¾ï¼‰
        batch_info_file = batch_dir / ".batch_info.json"
        if batch_info_file.exists():
            with open(batch_info_file, 'r', encoding='utf-8') as f:
                batch_info = json.load(f)
            
            # é‡ç½®çŠ¶æ€ï¼ŒåŒæ—¶æ¸…é™¤çŸ¥è¯†åº“æ ‡ç­¾
            batch_info['status'] = {
                'uploaded': True,
                'cleaned': False,
                'llm_processed': False,
                'uploaded_to_kb': False
            }
            
            # æ¸…é™¤çŸ¥è¯†åº“æ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'kb_name' in batch_info:
                del batch_info['kb_name']
            
            # ä¿å­˜
            with open(batch_info_file, 'w', encoding='utf-8') as f:
                json.dump(batch_info, f, ensure_ascii=False, indent=2)
        
        log_activity(f"æ‰¹æ¬¡ {batch_id} çŠ¶æ€å·²é‡ç½®ä¸º'å·²ä¸Šä¼ '")
        
        return jsonify({
            'success': True, 
            'message': f'æ‰¹æ¬¡ {batch_id} å·²é‡ç½®ï¼Œå¯é‡æ–°å¤„ç†'
        })
    except Exception as e:
        log_activity(f"é‡ç½®æ‰¹æ¬¡çŠ¶æ€å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


if __name__ == '__main__':
    import time
    print("=" * 60)
    print("Email Processing API Server - Starting...")
    print("=" * 60)
    print("Server Address: http://localhost:5001")
    print("Frontend Address: http://localhost:3000")
    print("=" * 60)
    app.run(host='0.0.0.0', port=5001, debug=True)

