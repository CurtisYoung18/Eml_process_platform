#!/usr/bin/env python3
"""
é‚®ä»¶æ¸…æ´—è„šæœ¬ - ç¬¬ä¸€è½®æ¸…æ´—
åŠŸèƒ½ï¼šè§£æEMLæ–‡ä»¶ï¼Œå»é‡ï¼Œç”ŸæˆMarkdownæ ¼å¼æ–‡ä»¶
"""

import os
import re
import email
import json
from pathlib import Path
from email.header import decode_header
from email.utils import parsedate_to_datetime
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import hashlib

class EmailCleaner:
    def __init__(self, input_dir: str = "Eml", output_dir: str = "eml_process/processed", batch_mode: bool = True):
        """
        åˆå§‹åŒ–é‚®ä»¶æ¸…æ´—å™¨
        
        Args:
            input_dir: è¾“å…¥EMLæ–‡ä»¶ç›®å½•
            output_dir: è¾“å‡ºMarkdownæ–‡ä»¶ç›®å½•
            batch_mode: æ˜¯å¦ä½¿ç”¨æ‰¹æ¬¡æ¨¡å¼ï¼ˆè‡ªåŠ¨æ£€æµ‹æ‰¹æ¬¡æ–‡ä»¶å¤¹ï¼‰
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.batch_mode = batch_mode
        
        # å­˜å‚¨å¤„ç†è¿‡çš„é‚®ä»¶ä¿¡æ¯
        self.processed_emails = []
        self.duplicate_info = []
        
        # å…¨å±€é‚®ä»¶è·Ÿè¸ªæ–‡ä»¶
        self.global_processed_file = Path("eml_process/.global_processed_emails.json")
        self.global_processed_emails = self._load_global_processed()
        
    def _load_global_processed(self) -> Dict:
        """åŠ è½½å…¨å±€å·²å¤„ç†é‚®ä»¶è®°å½•"""
        if self.global_processed_file.exists():
            try:
                with open(self.global_processed_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"[WARNING] Failed to load global processed emails: {e}")
        return {}
    
    def _save_global_processed(self):
        """ä¿å­˜å…¨å±€å·²å¤„ç†é‚®ä»¶è®°å½•"""
        try:
            self.global_processed_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.global_processed_file, 'w', encoding='utf-8') as f:
                json.dump(self.global_processed_emails, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[WARNING] Failed to save global processed emails: {e}")
        
    def decode_email_header(self, header_value: str) -> str:
        """è§£ç é‚®ä»¶å¤´éƒ¨ä¿¡æ¯"""
        if not header_value:
            return ""
        
        try:
            decoded_parts = decode_header(header_value)
            result = ""
            for part, encoding in decoded_parts:
                if isinstance(part, bytes):
                    if encoding:
                        result += part.decode(encoding)
                    else:
                        result += part.decode('utf-8', errors='ignore')
                else:
                    result += str(part)
            return result.strip()
        except Exception as e:
            print(f"[WARNING] Header decode failed: {e}")
            return str(header_value)
    
    def extract_email_content(self, msg) -> str:
        """æå–é‚®ä»¶æ­£æ–‡å†…å®¹"""
        content = ""
        
        if msg.is_multipart():
            for part in msg.walk():
                content_type = part.get_content_type()
                content_disposition = str(part.get("Content-Disposition"))
                
                # åªå¤„ç†æ–‡æœ¬å†…å®¹ï¼Œå¿½ç•¥é™„ä»¶
                if content_type == "text/plain" and "attachment" not in content_disposition:
                    try:
                        payload = part.get_payload(decode=True)
                        if payload:
                            charset = part.get_content_charset() or 'utf-8'
                            content += payload.decode(charset, errors='ignore') + "\n"
                    except Exception as e:
                        print(f"[WARNING] Content decode failed: {e}")
                        
                elif content_type == "text/html" and "attachment" not in content_disposition and not content:
                    # å¦‚æœæ²¡æœ‰çº¯æ–‡æœ¬ï¼Œå°è¯•HTMLï¼ˆç®€å•å¤„ç†ï¼‰
                    try:
                        payload = part.get_payload(decode=True)
                        if payload:
                            charset = part.get_content_charset() or 'utf-8'
                            html_content = payload.decode(charset, errors='ignore')
                            # ç®€å•å»é™¤HTMLæ ‡ç­¾
                            clean_content = re.sub(r'<[^>]+>', '', html_content)
                            content += clean_content + "\n"
                    except Exception as e:
                        print(f"[WARNING] HTML content decode failed: {e}")
        else:
            # éå¤šéƒ¨åˆ†é‚®ä»¶
            try:
                payload = msg.get_payload(decode=True)
                if payload:
                    charset = msg.get_content_charset() or 'utf-8'
                    content = payload.decode(charset, errors='ignore')
            except Exception as e:
                print(f"[WARNING] Email content decode failed: {e}")
        
        return content.strip()
    
    def clean_content(self, content: str) -> str:
        """æ¸…ç†é‚®ä»¶å†…å®¹"""
        if not content:
            return ""
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½è¡Œ
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
        
        # ç§»é™¤é‚®ä»¶å¤´éƒ¨çš„æŠ€æœ¯ä¿¡æ¯ï¼ˆReceived, Message-IDç­‰ï¼‰
        lines = content.split('\n')
        cleaned_lines = []
        skip_technical = False
        
        for line in lines:
            line = line.strip()
            
            # è·³è¿‡æŠ€æœ¯å¤´éƒ¨ä¿¡æ¯
            if line.startswith(('Received:', 'Message-ID:', 'Return-Path:', 'X-')):
                skip_technical = True
                continue
            elif skip_technical and line and not line.startswith(' '):
                skip_technical = False
            
            if not skip_technical:
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines).strip()
    
    def parse_eml_file(self, file_path: Path) -> Optional[Dict]:
        """è§£æå•ä¸ªEMLæ–‡ä»¶"""
        try:
            with open(file_path, 'rb') as f:
                msg = email.message_from_bytes(f.read())
            
            # æå–åŸºæœ¬ä¿¡æ¯
            email_info = {
                'filename': file_path.name,
                'from': self.decode_email_header(msg.get('From', '')),
                'to': self.decode_email_header(msg.get('To', '')),
                'cc': self.decode_email_header(msg.get('Cc', '')),
                'subject': self.decode_email_header(msg.get('Subject', '')),
                'date': msg.get('Date', ''),
                'content': self.extract_email_content(msg)
            }
            
            # è§£ææ—¥æœŸ
            try:
                if email_info['date']:
                    parsed_date = parsedate_to_datetime(email_info['date'])
                    email_info['parsed_date'] = parsed_date
                    email_info['date_str'] = parsed_date.strftime('%Y-%m-%d %H:%M:%S')
                else:
                    email_info['parsed_date'] = None
                    email_info['date_str'] = "æœªçŸ¥æ—¶é—´"
            except:
                email_info['parsed_date'] = None
                email_info['date_str'] = "æœªçŸ¥æ—¶é—´"
            
            # æ¸…ç†å†…å®¹
            email_info['cleaned_content'] = self.clean_content(email_info['content'])
            
            # ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºå»é‡
            content_for_hash = email_info['cleaned_content'].lower().replace(' ', '').replace('\n', '')
            email_info['content_hash'] = hashlib.md5(content_for_hash.encode()).hexdigest()
            
            return email_info
            
        except Exception as e:
            print(f"âŒ è§£ææ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
            return None
    
    def find_duplicates(self, emails: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """æŸ¥æ‰¾å¹¶å¤„ç†é‡å¤é‚®ä»¶ - ä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨hashåŠ é€Ÿï¼Œå‡å°‘å†…å­˜"""
        print(f"[DEDUP] Starting optimized deduplication for {len(emails)} emails...")
        # æŒ‰å†…å®¹é•¿åº¦æ’åºï¼ˆé•¿çš„åœ¨å‰ï¼‰
        emails_sorted = sorted(emails, key=lambda x: len(x['cleaned_content']), reverse=True)
        
        unique_emails = []
        duplicates = []
        
        # ã€å†…å­˜ä¼˜åŒ–ã€‘ä½¿ç”¨hashå­—å…¸åŠ é€ŸæŸ¥æ‰¾ï¼Œé¿å…é‡å¤çš„æ­£åˆ™åŒ–è®¡ç®—
        unique_normalized = {}  # {email_index: normalized_content}
        unique_hashes = set()  # å¿«é€ŸhashæŸ¥æ‰¾é›†åˆ
        
        for idx, email_info in enumerate(emails_sorted):
            # æ ‡å‡†åŒ–å†…å®¹ç”¨äºæ¯”è¾ƒ - ç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦å’Œæ¢è¡Œ
            current_content = re.sub(r'\s+', '', email_info['cleaned_content'].lower())
            current_hash = hashlib.md5(current_content.encode('utf-8')).hexdigest()
            
            is_duplicate = False
            container_email = None
            
            # ã€ä¼˜åŒ–1ã€‘å¿«é€Ÿhashæ£€æŸ¥ï¼šå®Œå…¨ç›¸åŒçš„å†…å®¹
            if current_hash in unique_hashes:
                # æ‰¾åˆ°hashåŒ¹é…çš„é‚®ä»¶
                for i, u_email in enumerate(unique_emails):
                    u_content = unique_normalized.get(i, "")
                    u_hash = hashlib.md5(u_content.encode('utf-8')).hexdigest() if u_content else ""
                    if u_hash == current_hash:
                        is_duplicate = True
                        container_email = u_email
                        break
            
            # ã€ä¼˜åŒ–2ã€‘å¦‚æœhashä¸åŒï¼Œæ£€æŸ¥å†…å®¹åŒ…å«ï¼ˆé™åˆ¶æ£€æŸ¥èŒƒå›´ä»¥èŠ‚çœå†…å­˜ï¼‰
            if not is_duplicate:
                # åªä¸æœ€è¿‘çš„100ä¸ªå”¯ä¸€é‚®ä»¶æ¯”è¾ƒï¼Œé¿å…O(nÂ²)çˆ†ç‚¸
                check_range = min(len(unique_emails), 100)
                for i in range(check_range):
                    unique_email = unique_emails[-(i+1)]  # ä»æœ€æ–°çš„å¼€å§‹æ£€æŸ¥
                    
                    # è·å–æˆ–è®¡ç®—å½’ä¸€åŒ–å†…å®¹
                    if len(unique_emails) - (i+1) not in unique_normalized:
                        unique_content = re.sub(r'\s+', '', unique_email['cleaned_content'].lower())
                        unique_normalized[len(unique_emails) - (i+1)] = unique_content
                    else:
                        unique_content = unique_normalized[len(unique_emails) - (i+1)]
                    
                    # 100%åŒ…å«æ£€æµ‹ï¼šè¾ƒçŸ­å†…å®¹å¿…é¡»å®Œå…¨åœ¨è¾ƒé•¿å†…å®¹ä¸­
                    if (len(current_content) > 0 and 
                        len(current_content) <= len(unique_content) and 
                        current_content in unique_content):
                        is_duplicate = True
                        container_email = unique_email
                        break
            
            if is_duplicate and container_email:
                # è®°å½•é‡å¤ä¿¡æ¯
                duplicates.append({
                    'duplicate_file': email_info['filename'],
                    'duplicate_subject': email_info['subject'],
                    'contained_by_file': container_email['filename'],
                    'contained_by_subject': container_email['subject'],
                    'content_length_ratio': f"{len(current_content)}/?"
                })
                
                # åœ¨å®¹å™¨é‚®ä»¶ä¸­è®°å½•åŒ…å«çš„æºæ–‡ä»¶
                if 'contained_files' not in container_email:
                    container_email['contained_files'] = []
                container_email['contained_files'].append(email_info['filename'])
                
                print(f"[DUPLICATE] Found 100% duplicate: {email_info['filename']} contained in {container_email['filename']}")
                
            else:
                # ä¸æ˜¯é‡å¤é‚®ä»¶ï¼Œæ·»åŠ åˆ°å”¯ä¸€åˆ—è¡¨
                unique_emails.append(email_info)
                unique_normalized[len(unique_emails) - 1] = current_content
                unique_hashes.add(current_hash)
                print(f"[UNIQUE] Unique email: {email_info['filename']} (length: {len(current_content)})")
            
            # ã€å†…å­˜ä¼˜åŒ–ã€‘å®šæœŸæ¸…ç†æ—§çš„å½’ä¸€åŒ–å†…å®¹ç¼“å­˜ï¼Œåªä¿ç•™æœ€è¿‘100ä¸ª
            if len(unique_normalized) > 150:
                # åˆ é™¤æœ€æ—§çš„50ä¸ªç¼“å­˜é¡¹
                keys_to_remove = sorted(unique_normalized.keys())[:50]
                for key in keys_to_remove:
                    del unique_normalized[key]
            
            # è¿›åº¦æ˜¾ç¤º
            if (idx + 1) % 100 == 0:
                print(f"[PROGRESS] Processed {idx + 1}/{len(emails_sorted)} emails, {len(unique_emails)} unique, {len(duplicates)} duplicates")
        
        print(f"[DEDUP] Completed: {len(unique_emails)} unique, {len(duplicates)} duplicates")
        return unique_emails, duplicates
    
    def generate_markdown(self, email_info: Dict) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„é‚®ä»¶å†…å®¹"""
        md_content = []
        
        # æ ‡é¢˜
        md_content.append(f"# é‚®ä»¶å†…å®¹ - {email_info['filename']}")
        md_content.append("")
        
        # åŸºæœ¬ä¿¡æ¯
        md_content.append("## Email Information")
        md_content.append("")
        md_content.append(f"- **æºæ–‡ä»¶å**: `{email_info['filename']}`")
        md_content.append(f"- **å‘ä»¶äºº**: {email_info['from']}")
        md_content.append(f"- **æ”¶ä»¶äºº**: {email_info['to']}")
        
        if email_info['cc']:
            md_content.append(f"- **æŠ„é€**: {email_info['cc']}")
        
        md_content.append(f"- **ä¸»é¢˜**: {email_info['subject']}")
        md_content.append(f"- **æ—¶é—´**: {email_info['date_str']}")
        
        # å¦‚æœåŒ…å«å…¶ä»–æ–‡ä»¶ï¼Œåˆ—å‡ºæ¥
        if 'contained_files' in email_info:
            md_content.append(f"- **åŒ…å«çš„å…¶ä»–é‚®ä»¶**: {len(email_info['contained_files'])} å°")
            md_content.append("")
            md_content.append("### Source Files List")
            md_content.append("")
            for contained_file in email_info['contained_files']:
                md_content.append(f"- `{contained_file}`")
        
        md_content.append("")
        
        # é‚®ä»¶å†…å®¹
        md_content.append("## ğŸ“„ é‚®ä»¶å†…å®¹")
        md_content.append("")
        
        if email_info['cleaned_content']:
            # å°†é‚®ä»¶å†…å®¹æŒ‰æ®µè½åˆ†å‰²ï¼Œä¿æŒæ ¼å¼
            content_lines = email_info['cleaned_content'].split('\n')
            for line in content_lines:
                if line.strip():
                    md_content.append(line)
                else:
                    md_content.append("")
        else:
            md_content.append("*ï¼ˆé‚®ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è§£æï¼‰*")
        
        md_content.append("")
        md_content.append("---")
        md_content.append(f"*å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*")
        
        return '\n'.join(md_content)
    
    def save_markdown_file(self, email_info: Dict, batch_id: Optional[str] = None) -> str:
        """ä¿å­˜Markdownæ–‡ä»¶"""
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆå»é™¤.emlæ‰©å±•åï¼Œæ·»åŠ .mdï¼‰
        base_name = email_info['filename'].replace('.eml', '')
        md_filename = f"{base_name}.md"
        
        # å¦‚æœæœ‰æ‰¹æ¬¡IDï¼Œåˆ›å»ºæ‰¹æ¬¡å­ç›®å½•
        if batch_id:
            batch_output_dir = self.output_dir / batch_id
            batch_output_dir.mkdir(parents=True, exist_ok=True)
            md_path = batch_output_dir / md_filename
        else:
            md_path = self.output_dir / md_filename
        
        # ç”ŸæˆMarkdownå†…å®¹
        md_content = self.generate_markdown(email_info)
        
        # ä¿å­˜æ–‡ä»¶
        try:
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(md_content)
            return str(md_path)
        except Exception as e:
            print(f"[ERROR] Failed to save Markdown file {md_filename}: {e}")
            return ""
    
    def process_batch(self, batch_dir: Path, batch_id: str) -> Dict:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        print(f"[INFO] Processing batch: {batch_id}")
        
        # è¯»å–æ‰¹æ¬¡å…ƒæ•°æ®
        batch_info_file = batch_dir / ".batch_info.json"
        batch_info = None
        
        if batch_info_file.exists():
            with open(batch_info_file, 'r', encoding='utf-8') as f:
                batch_info = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            if batch_info.get('status', {}).get('cleaned', False):
                print(f"[INFO] Batch {batch_id} already cleaned, skipping...")
                return {
                    "success": True,
                    "batch_id": batch_id,
                    "skipped": True,
                    "message": "æ‰¹æ¬¡å·²å¤„ç†"
                }
        
        # è·å–æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰EMLæ–‡ä»¶
        eml_files = list(batch_dir.glob("*.eml"))
        
        if not eml_files:
            print(f"[WARNING] No EML files found in batch {batch_id}")
            return {"success": False, "batch_id": batch_id, "message": "æœªæ‰¾åˆ°EMLæ–‡ä»¶"}
        
        print(f"[FOUND] Found {len(eml_files)} EML files in batch {batch_id}")
        
        # è§£ææ‰€æœ‰é‚®ä»¶
        emails = []
        failed_files = []
        global_duplicates = []  # å…¨å±€é‡å¤çš„é‚®ä»¶
        
        for eml_file in eml_files:
            print(f"[PARSE] Parsing: {eml_file.name}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å…¨å±€é‡å¤
            file_name = eml_file.name
            if file_name in self.global_processed_emails:
                previous_batch = self.global_processed_emails[file_name].get('batch_id', 'unknown')
                previous_time = self.global_processed_emails[file_name].get('processed_at', 'unknown')
                print(f"[GLOBAL DUPLICATE] {file_name} already processed in batch {previous_batch} at {previous_time}")
                global_duplicates.append({
                    'file_name': file_name,
                    'previous_batch': previous_batch,
                    'previous_time': previous_time
                })
                continue
            
            email_info = self.parse_eml_file(eml_file)
            
            if email_info:
                emails.append(email_info)
                # è®°å½•åˆ°å…¨å±€å·²å¤„ç†ï¼ˆä»…å†…å­˜ï¼Œç¨åæ‰¹é‡ä¿å­˜ï¼‰
                self.global_processed_emails[file_name] = {
                    'batch_id': batch_id,
                    'processed_at': datetime.now().isoformat(),
                    'subject': email_info.get('subject', '')
                }
            else:
                failed_files.append(eml_file.name)
        
        if not emails:
            return {"success": False, "batch_id": batch_id, "message": "æ‰€æœ‰é‚®ä»¶è§£æå¤±è´¥"}
        
        print(f"[SUCCESS] Successfully parsed {len(emails)} emails")
        
        # ã€æ€§èƒ½ä¼˜åŒ–ã€‘æ‰¹é‡ä¿å­˜å…¨å±€å·²å¤„ç†è®°å½•ï¼ˆç§»å‡ºå¾ªç¯ï¼Œåªä¿å­˜ä¸€æ¬¡ï¼‰
        print(f"[SAVE] Saving global processed emails to {self.global_processed_file}...")
        self._save_global_processed()
        print(f"[SAVE] Global processed emails saved successfully")
        
        # å»é‡å¤„ç†ï¼ˆåªåœ¨æ‰¹æ¬¡å†…å»é‡ï¼‰
        print("[DEDUP] Starting deduplication...")
        unique_emails, duplicates = self.find_duplicates(emails)
        
        print(f"[RESULT] Deduplication result: {len(emails)} -> {len(unique_emails)} emails")
        print(f"[DUPLICATE] Duplicate emails: {len(duplicates)}")
        
        # ä¿å­˜å»é‡åçš„é‚®ä»¶ä¸ºMarkdownï¼ˆå¸¦æ‰¹æ¬¡IDï¼‰
        saved_files = []
        for email_info in unique_emails:
            md_path = self.save_markdown_file(email_info, batch_id=batch_id)
            if md_path:
                saved_files.append(md_path)
        
        # æ›´æ–°æ‰¹æ¬¡å…ƒæ•°æ®
        if batch_info:
            batch_info['status']['cleaned'] = True
            batch_info['processing_history']['cleaned_at'] = datetime.now().isoformat()
            batch_info['dedup_stats'] = {
                'total_emails': len(eml_files),
                'unique_emails': len(unique_emails),
                'duplicates': len(duplicates),
                'global_duplicates': len(global_duplicates)
            }
            
            with open(batch_info_file, 'w', encoding='utf-8') as f:
                json.dump(batch_info, f, ensure_ascii=False, indent=2)
        
        return {
            "success": True,
            "batch_id": batch_id,
            "total_files": len(eml_files),
            "parsed_files": len(emails),
            "unique_files": len(unique_emails),
            "duplicate_files": len(duplicates),
            "global_duplicate_files": len(global_duplicates),
            "global_duplicates": global_duplicates,
            "failed_files": failed_files,
            "saved_files": saved_files
        }
    
    def process_all_emails(self, selected_batches=None) -> Dict:
        """å¤„ç†æ‰€æœ‰é‚®ä»¶æ–‡ä»¶ - æ”¯æŒæ‰¹æ¬¡æ¨¡å¼
        
        Args:
            selected_batches: å¯é€‰ï¼ŒæŒ‡å®šè¦å¤„ç†çš„æ‰¹æ¬¡IDåˆ—è¡¨ã€‚å¦‚æœä¸ºNoneï¼Œå¤„ç†æ‰€æœ‰æ‰¹æ¬¡ã€‚
        """
        print(f"[SCAN] Scanning directory: {self.input_dir}")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ‰¹æ¬¡æ¨¡å¼
        if self.batch_mode:
            # æŸ¥æ‰¾æ‰€æœ‰æ‰¹æ¬¡ç›®å½•
            all_batch_dirs = [d for d in self.input_dir.iterdir() if d.is_dir()]
            
            if not all_batch_dirs:
                print(f"[WARNING] No batch directories found in {self.input_dir}")
                # é™çº§åˆ°éæ‰¹æ¬¡æ¨¡å¼
                self.batch_mode = False
            else:
                # å¦‚æœæŒ‡å®šäº†æ‰¹æ¬¡åˆ—è¡¨ï¼Œåªå¤„ç†è¿™äº›æ‰¹æ¬¡
                if selected_batches:
                    batch_dirs = [d for d in all_batch_dirs if d.name in selected_batches]
                    print(f"[INFO] Processing {len(batch_dirs)} selected batch(es) out of {len(all_batch_dirs)} total")
                else:
                    batch_dirs = all_batch_dirs
                    print(f"[INFO] Found {len(batch_dirs)} batch(es)")
                
                if not batch_dirs:
                    return {
                        "success": False,
                        "message": "æœªæ‰¾åˆ°æŒ‡å®šçš„æ‰¹æ¬¡"
                    }
                
                results = []
                
                for batch_dir in sorted(batch_dirs, key=lambda x: x.name):
                    batch_id = batch_dir.name
                    result = self.process_batch(batch_dir, batch_id)
                    results.append(result)
                
                # æ±‡æ€»ç»Ÿè®¡
                total_unique = sum(r.get('unique_files', 0) for r in results if r.get('success'))
                total_duplicates = sum(r.get('duplicate_files', 0) for r in results if r.get('success'))
                total_global_duplicates = sum(r.get('global_duplicate_files', 0) for r in results if r.get('success'))
                total_failed = sum(len(r.get('failed_files', [])) for r in results if r.get('success'))
                all_global_duplicates = []
                for r in results:
                    if r.get('success') and r.get('global_duplicates'):
                        all_global_duplicates.extend(r['global_duplicates'])
                
                print(f"[STATS] All batches processed:")
                print(f"  - Total unique emails: {total_unique}")
                print(f"  - Total duplicates (in batch): {total_duplicates}")
                print(f"  - Total global duplicates (cross-batch): {total_global_duplicates}")
                print(f"  - Total failed: {total_failed}")
                
                return {
                    "success": True,
                    "mode": "batch",
                    "batches": results,
                    "summary": {
                        "total_batches": len(results),
                        "total_unique": total_unique,
                        "total_duplicates": total_duplicates,
                        "total_global_duplicates": total_global_duplicates,
                        "total_failed": total_failed
                    },
                    "global_duplicates": all_global_duplicates,
                    "report": {
                        "unique_emails": total_unique,
                        "duplicate_emails": total_duplicates,
                        "total_input_files": total_unique + total_duplicates
                    }
                }
        
        # éæ‰¹æ¬¡æ¨¡å¼ï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰
        # è·å–æ‰€æœ‰EMLæ–‡ä»¶
        eml_files = list(self.input_dir.glob("*.eml"))
        
        if not eml_files:
            print(f"[ERROR] No EML files found in {self.input_dir}")
            return {"success": False, "message": "æœªæ‰¾åˆ°EMLæ–‡ä»¶"}
        
        print(f"[FOUND] Found {len(eml_files)} EML files")
        
        # è§£ææ‰€æœ‰é‚®ä»¶
        emails = []
        failed_files = []
        
        for eml_file in eml_files:
            print(f"[PARSE] Parsing: {eml_file.name}")
            email_info = self.parse_eml_file(eml_file)
            
            if email_info:
                emails.append(email_info)
            else:
                failed_files.append(eml_file.name)
        
        if not emails:
            return {"success": False, "message": "æ‰€æœ‰é‚®ä»¶è§£æå¤±è´¥"}
        
        print(f"[SUCCESS] Successfully parsed {len(emails)} emails")
        
        # å»é‡å¤„ç†
        print("[DEDUP] Starting deduplication...")
        unique_emails, duplicates = self.find_duplicates(emails)
        
        print(f"[RESULT] Deduplication result: {len(emails)} -> {len(unique_emails)} emails")
        print(f"[DUPLICATE] Duplicate emails: {len(duplicates)}")
        
        # ç”ŸæˆMarkdownæ–‡ä»¶
        print("[GENERATE] Generating Markdown files...")
        generated_files = []
        
        for email_info in unique_emails:
            md_path = self.save_markdown_file(email_info)
            if md_path:
                generated_files.append(md_path)
                print(f"[SUCCESS] Generated: {Path(md_path).name}")
        
        # ä¿å­˜å¤„ç†æŠ¥å‘Š
        report = {
            "processing_time": datetime.now().isoformat(),
            "input_directory": str(self.input_dir),
            "output_directory": str(self.output_dir),
            "total_input_files": len(eml_files),
            "successfully_parsed": len(emails),
            "failed_to_parse": len(failed_files),
            "failed_files": failed_files,
            "unique_emails": len(unique_emails),
            "duplicate_emails": len(duplicates),
            "duplicate_details": duplicates,
            "generated_markdown_files": [Path(f).name for f in generated_files],
            "compression_ratio": f"{len(duplicates) / len(emails) * 100:.1f}%" if emails else "0%"
        }
        
        report_path = self.output_dir / "processing_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\n[REPORT] Processing report saved: {report_path}")
        print(f"[STATS] Compression ratio: {report['compression_ratio']}")
        
        return {
            "success": True,
            "report": report,
            "generated_files": generated_files
        }

def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œä½¿ç”¨"""
    print("=" * 60)
    print("Email Cleaner Script - Starting...")
    print("=" * 60)
    print("=" * 50)
    
    # åˆ›å»ºæ¸…æ´—å™¨å®ä¾‹
    cleaner = EmailCleaner()
    
    # å¤„ç†é‚®ä»¶
    result = cleaner.process_all_emails()
    
    if result["success"]:
        print("\nğŸ‰ é‚®ä»¶æ¸…æ´—å®Œæˆ!")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {cleaner.output_dir}")
        print(f"ğŸ“„ ç”Ÿæˆæ–‡ä»¶æ•°: {len(result['generated_files'])}")
    else:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {result['message']}")

if __name__ == "__main__":
    main()
